\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{../notation}
\usepackage{pgfplots}
\usepgfplotslibrary{fillbetween}


%\beamerdefaultoverlayspecification{<+->}
% \newcommand{\data}{\mathcal{D}}
% \newcommand\Item[1][]{%
%   \ifx\relax#1\relax  \item \else \item[#1] \fi
%   \abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\newcommand*{\Comb}[2]{{}^{#1}C_{#2}}%

\graphicspath{ {imgs/} }

\usetheme{metropolis}           % Use metropolis theme


\title{Ensemble Learning}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
\maketitle

\begin{frame}{Ensemble Methods}
  \only<1->{
    Use multiple models for prediction.\\
    Most winning entries of Kaggle competition using ensemble learning.\\
    \vspace{1cm}
  }
  \only<2>{
    \textbf{ Example:}\\
    Classifier 1 - Good\\
    Classifier 2 - Good\\
    Classifier 3 - Bad\\
    \vspace{0.5cm}
    Using Majority Voting, we predict Good.\\
  }
  \only<3>{
    \textbf{ Example:}\\
    Regressor 1 - 20\\
    Regressor 2 - 30\\
    Regressor 3 - 30\\
    \vspace{0.5cm}
    Using Average, we predict $\dfrac{80}{3}$
  }
\end{frame}


\begin{frame}{Intuition}

  Three reasons why ensembles make sense:

  \pause 1) Statistical: Sometimes if data is less, many competing hypothesis can be learnt.

  \pause Eg. Depending on criteria and initialisation, we can learn many decision trees for the same data.

  \pause 2) Computational: Some classifiers/regressors can get stuck in local optima. Computationally learning the ``best'' hypothesis can be non-trivial.

  \pause Eg. Decision Trees employ greedy critera

  \pause 3) Representational: Some classifiers/regressors can not learn the true form/representation.

\end{frame}

\begin{frame}{Representation of Limited Depth DTs vs RFs}
  \begin{figure}
    \includegraphics[scale=0.7]{1-representation.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Representation of Limited Depth DTs vs RFs}
  \begin{figure}
    \includegraphics[scale=0.7]{2-representation.pdf}
  \end{figure}
\end{frame}




\begin{frame}{Necessary and Sufficient Conditions}
  \pause 1)A necessary and sufficient condition for an ensemble of classiers to be more
  accurate than any of its individual members is if the classiers are accurate and
  diverse.

  \pause 2) An accurate classier: \pause  is one that has an
  error rate of better than random guessing on new x values.

  \pause 3) Two classifiers are diverse: \pause  if they make different errors on new data points

\end{frame}

\begin{frame}{Necessary and Sufficient Conditions}
  Imagine that we have an ensemble of three classifiers ($h_1, h_2, h_3$) and consider a new case x.


  \pause If the three classifiers are identical, i.e.
  not diverse, then when $h_1(x)$ is wrong $h_2(x)$ and $h_3(x)$ will also be wrong.


  \pause However, if the errors made by the classifiers are uncorrelated, then when $h_1(x)$
  is wrong, $h_2(x)$ and $h_3(x)$ may be correct, so that a majority vote will correctly
  class.

\end{frame}
\begin{frame}{Intuition for Ensemble Methods from Quantitative Perspective}
  \pause Error Probability of each model = $\varepsilon$ = 0.3\\
  \vspace{1cm}
  $Pr(\text{ensemble being wrong}) = \Comb{3}{2}(\varepsilon^2)(1-\varepsilon)^{3-2} + \Comb{3}{3}(\varepsilon^3)(1-\varepsilon)^{3-3}$\\
  \vspace{0.5cm}
  \hspace{4.4cm}$ = 0.19 \leq 0.3$
\end{frame}

\begin{frame}{Some calculations}
  \input{test-0.3.tex}
\end{frame}

\begin{frame}{Some calculations}
  \input{test-0.6.tex}
\end{frame}
\begin{frame}{Ensemble Methods}
  \only<1->{
    Where does ensemble learning not work well?
  }
  \only<2->{
    \begin{itemize}
      \item The base model is bad.
      \item All models give similar prediction or the models are highly correlated.
    \end{itemize}
  }
\end{frame}

\begin{frame}{Bagging}
  \only<1->{
    Also known as $Bootstrap$ $Aggregation$.\\
  }
  \only<2->{
    $Key$ $idea$ : Reduce Variance\\
    \vspace{1cm}
  }
  \only<3->{
    How to learn different classifiers while feeding in the same data?\\
    \vspace{0.5cm}
  }
  \only<4->{
    Think about cross-validation!\\
    \vspace{0.5cm}
  }
  \only<5->{
    We will create multiple datasets from our single dataset using ``$sampling$ $with$ $replacement$''.
  }
\end{frame}

\begin{frame}{Bagging}
  \only<1->{
    Consider our dataset has $n$ samples, $D_1, D_2, D_3, \dots, D_n$.\\
    For each model in the ensemble, we create a new dataset of size $n$ by sampling uniformly with replacement.\\
    \vspace{1cm}
  }
  \only<2->{
    Round 1 : $D_1, D_3, D_6, D_1, \dots, D_n$\\
    Round 2 : $D_2, D_4, D_1, D_{80}, \dots, D_3$\\
    \vdots
  }
  \only<3->{
    Repetition of samples is possible.\\
  }
  \only<4->{
    We can train the same classifier/models on each of these different ``Bagging Rounds''.
  }
\end{frame}

\begin{frame}{Bagging : Classification Example}
  Consider the dataset below. Points (3,3) and (5,8) are anomalies.\\
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.6\textwidth]{dataset}
\end{frame}


\begin{frame}{Bagging : Classification Example}
  Decision Boundary for decision tree with depth 6.\\
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.6\textwidth]{strong-tree}
\end{frame}

\begin{frame}{Bagging : Classification Example}
  Lets use bagging with ensemble of 5 trees.\\
  \vspace{0.5cm}
  \begin{columns}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 1\\

      \includegraphics[width = 0.9\textwidth]{dataset-rnd-0}

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 2\\

      \includegraphics[width = 0.9\textwidth]{dataset-rnd-1}

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 3\\

      \includegraphics[width = 0.9\textwidth]{dataset-rnd-2}

    \end{column}

  \end{columns}
  \vspace{0.5cm}
  \begin{columns}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 4\\

      \includegraphics[width = 0.9\textwidth]{dataset-rnd-3}

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 5\\

      \includegraphics[width = 0.9\textwidth]{dataset-rnd-4}

    \end{column}

  \end{columns}
\end{frame}


\begin{frame}{Bagging : Classification Example}
  \vspace{0.3cm}
  \begin{columns}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 1\\

      \includegraphics[width = 0.9\textwidth]{decision-boundary-0}
      Tree Depth = 4

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 2\\

      \includegraphics[width = 0.9\textwidth]{decision-boundary-1}
      Tree Depth = 5

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 3\\

      \includegraphics[width = 0.9\textwidth]{decision-boundary-2}
      Tree Depth = 5

    \end{column}

  \end{columns}
  \vspace{0.5cm}
  \pause  \begin{columns}
    \begin{column}{0.3\textwidth}
      \centering
      Round - 4\\

      \includegraphics[width = 0.9\textwidth]{decision-boundary-3}
      Tree Depth = 2

    \end{column}
    \pause  \begin{column}{0.3\textwidth}
      \centering
      Round - 5\\

      \includegraphics[width = 0.9\textwidth]{decision-boundary-4}
      Tree Depth = 4

    \end{column}

  \end{columns}
\end{frame}

\begin{frame}{Bagging : Classification Example}
  Using majority voting to combine all predictions, we get the decision boundary below.\\
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.6\textwidth]{decision-boundary-ensemble}
\end{frame}

\begin{frame}{Bagging}
  \textbf{Summary}
  \begin{itemize}
    \item We take ``strong'' learners and combine them to reduce variance.
    \item All learners are independent of each other.
  \end{itemize}
\end{frame}

\begin{frame}{Boosting}
  \begin{itemize}
    \item We take ``weak'' learners and combine them to reduce bias.
          \pause \item All learners are incrementally built.
          \pause \item Incremental building: Incrementally try to classify ``harder'' samples correctly.
  \end{itemize}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.6\textwidth]{ada_data}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
  \end{enumerate}
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.6\textwidth]{ada_data_init_weights}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
          \end{enumerate}
  \end{enumerate}
  \centering
  \includegraphics[width = 0.6\textwidth]{ada_iter1}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
          \end{enumerate}
  \end{enumerate}
  \centering
  \includegraphics[width = 0.85\textwidth]{ada_iter1_misclassify}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
          \end{enumerate}
  \end{enumerate}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width = 1.1\textwidth]{ada_iter1_misclassify}
    \end{column}
    \begin{column}{0.5\textwidth}
      $err_1 = \dfrac{0.3}{1}$
    \end{column}
  \end{columns}
\end{frame}


\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
          \end{enumerate}
  \end{enumerate}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width = \textwidth]{ada_iter1_misclassify}
    \end{column}
    \begin{column}{0.5\textwidth}
      $err_1 = \dfrac{0.3}{1}$\\
      $\alpha_1 = \dfrac{1}{2}log\left(\dfrac{1-0.3}{0.3}\right) = 0.42$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
            \item For samples which were predicted correctly, $w_i = w_ie^{-\alpha_m}$
            \item For samples which were predicted incorrectly, $w_i = w_ie^{\alpha_m}$
          \end{enumerate}
  \end{enumerate}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.8\textwidth]{ada_iter1_new_weights_exp}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.8\textwidth]{ada_iter1_new_weights}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
            \item For samples which were predicted correctly, $w_i = w_ie^{-\alpha_m}$
            \item For samples which were predicted incorrectly, $w_i = w_ie^{\alpha_m}$
            \item Normalize $w_i's$ to sum up to 1.
          \end{enumerate}
  \end{enumerate}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.8\textwidth]{ada_iter1_new_weights_normalized}
\end{frame}


\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
          \end{enumerate}
  \end{enumerate}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width = \textwidth]{ada_iter2_misclassify}
    \end{column}
    \begin{column}{0.5\textwidth}
      $err_2 = \dfrac{0.21}{1}$\\
      $\alpha_2 = \dfrac{1}{2}log\left(\dfrac{1-0.21}{0.21}\right) = 0.66$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
            \item For samples which were predicted correctly, $w_i = w_ie^{-\alpha_m}$
            \item For samples which were predicted incorrectly, $w_i = w_ie^{\alpha_m}$
          \end{enumerate}
  \end{enumerate}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.8\textwidth]{ada_iter2_new_weights}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
            \item For samples which were predicted correctly, $w_i = w_ie^{-\alpha_m}$
            \item For samples which were predicted incorrectly, $w_i = w_ie^{\alpha_m}$
            \item Normalize $w_i's$ to sum up to 1.
          \end{enumerate}
  \end{enumerate}
\end{frame}

\begin{frame}{Boosting : AdaBoost }
  \vspace{0.5cm}
  \centering
  \includegraphics[width = 0.8\textwidth]{ada_iter2_new_weights_normalized}
\end{frame}


\begin{frame}{Boosting : AdaBoost }
  Consider we have a dataset of $N$ samples.\\
  Sample $i$ has weight $w_i$. There are $M$ classifers in ensemble.\\
  \begin{enumerate}
    \item Initialize weights of data samples, $w_i = \dfrac{1}{N}$
    \item For $m = 1\dots M$
          \begin{enumerate}
            \item Learn classifier using current weights $w_i's$
            \item Compute the weighted error, $err_m = \dfrac{\sum\limits_iw_i(incorrect)}{\sum\limits_iw_i}$
            \item Compute $\alpha_m = \dfrac{1}{2}log_e\left(\dfrac{1 - err_m}{err_m}\right)$
          \end{enumerate}
  \end{enumerate}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \centering
      \includegraphics[width = \textwidth]{ada_iter3_misclassify}
    \end{column}
    \begin{column}{0.5\textwidth}
      $err_3 = \dfrac{0.12}{1}$\\
      $\alpha_3 = \dfrac{1}{2}log\left(\dfrac{1-0.12}{0.12}\right) = 0.99$
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Boosting: Adaboost}
  Intuitively, after each iteration, importance of wrongly classified samples is increased by increasing their weights and importance of correctly classified     samples is decreased by decreasing their weights.
\end{frame}

\begin{frame}{Boosting: Adaboost}
  \textbf{Testing}\\
  \begin{itemize}
	\item For each sample $x$, compute the prediction of each classifier $h_m(x)$.
	\item Final prediction is the sign of the sum of weighted predictions, given as:
	\item SIGN($\alpha_1 h_1(x)$ +  $\alpha_2 h_2(x)$ + $\dots$ +  $\alpha_M h_M(x)$)
  \end{itemize}

\end{frame}

\begin{frame}{Boosting: Adaboost}
  \textbf{Example}
	\begin{columns}
		\pause 			\begin{column}{0.33\textwidth}
				\centering
				\begin{figure}
					\includegraphics[width = \textwidth]{ada_iter1_misclassify}
									\vspace{-20pt}
					\caption{$\alpha_1=0.42$}
				\end{figure}
				
			\end{column}
			
			
			\pause \begin{column}{0.4\textwidth}
				\centering
				\begin{figure}
					\includegraphics[width = \textwidth]{ada_iter2_misclassify}
					\vspace{-20pt}
					\caption{$\alpha_2=0.66$}
				\end{figure}
	
			\pause \end{column}
					\begin{column}{0.3\textwidth}
				\centering
				\begin{figure}
					\includegraphics[width = \textwidth]{ada_iter3_misclassify}
									\vspace{-20pt}
					\caption{$\alpha_3=0.99$}
				\end{figure}
				
			\end{column}
		\end{columns}
	
		\begin{columns}
			\pause \begin{column}{0.5\textwidth}
				\begin{figure}
				\includegraphics[scale=0.1]{testing}
				\end{figure}
			\end{column}
		
		\begin{column}{0.5\textwidth}
			\pause Let us say, yellow class is +1 and blue class is -1
			
			\pause Prediction = SIGN(0.42*-1 + 0.66*-1 + 0.99*+1) = Negative = blue
		\end{column}
		\end{columns}
	
\end{frame}

% \begin{frame}

%     \pause \begin{column}{0.4\textwidth}
%       \centering
%       \begin{figure}
%         \includegraphics[width = \textwidth]{ada_iter2_misclassify}
%         \vspace{-20pt}
%         \caption{$\alpha_2=0.66$}
%       \end{figure}
% 	\end{column}
% \end{columns}

% \end{frame}
		
% 	\begin{frame}
%       \pause \end{column}
%     \begin{column}{0.3\textwidth}
%       \centering
%       \begin{figure}
%         \includegraphics[width = \textwidth]{ada_iter3_misclassify}
%         \vspace{-20pt}
%         \caption{$\alpha_3=0.99$}
%       \end{figure}

%     \end{column}
%   \end{columns}

%   \begin{columns}
%     \pause \begin{column}{0.5\textwidth}
%       \begin{figure}
%         \includegraphics[scale=0.1]{testing}
%       \end{figure}
%     \end{column}

%     \begin{column}{0.5\textwidth}
%       \pause Let us say, yellow class is +1 and blue class is -1

%       \pause Prediction = SIGN(0.42*-1 + 0.66*-1 + 0.99*+1) = Negative = blue
%     \end{column}
%   \end{columns}

% \end{frame}


\begin{frame}{Intuition behind weight update formula}
  \begin{columns}
    \pause \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
        \includegraphics[scale=0.7]{alpha-boosting.pdf}

        \label{fig:alpha-boosting}
      \end{figure}
    \end{column}
    \pause \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
        \includegraphics[scale=0.7]{alpha-boosting-weight.pdf}

        \label{fig:alpha-boosting}
      \end{figure}
    \end{column}
  \end{columns}


\end{frame}

\begin{frame}{Random Forest}
  \begin{itemize}
    \item Random Forest is an ensemble of decision trees.
    \item We have two types of bagging: bootstrap (on data) and random subspace (of features).
    \item As features are randomly selected, we learn decorrelated trees and helps in reducing variance.
  \end{itemize}
\end{frame}

\begin{frame}{Random Forest}
There are 3 parameters while training a random forest number of trees, number of features (m), maximum depth.\\
\vspace{1cm}
\underline{Training Algorithm}\\
\begin{itemize}
	\item For $i^{th}$ tree ($i \in \{1 \cdots N\}$), select $n$ samples from total $N$ samples with replacement.\\
%   \item for $tree$ in $1, \dots,$ $number$ $of$ $trees$ $\]$
		
		\item Learn Decision Tree on selected samples for $i^{th}$ round.
		 

\end{itemize}

\underline{Learning Decision Tree (for RF)}\\
\begin{itemize}
	\item For each split, select $m$ features from total available $M$ features and train a decision tree on selected features
\end{itemize}


\end{frame}

% \begin{frame}
	

% \begin{itemize}
%   \item for $tree$ in $[1, \dots,$ $number$ $of$ $trees$ $]$
%         \begin{itemize}
%           \item For each split, select $m$ features from total available $M$ features and train a decision tree on selected features

%         \end{itemize}
% \end{itemize}

% \end{frame}


\begin{frame}{Dataset }
  \includegraphics[scale=0.5]{dataset-iris.png}
\end{frame}


% \begin{frame}{Random Forest}
% There are 3 parameters while training a random forest ``$number$ $of$ $trees$'', ``$number$ $of$ $features'' (m)$, ``$maximum$ $depth$''.\\
% \vspace{1cm}
% \underline{Training Algorithm}\\
% \begin{itemize}
%   \item for $depth$ in $[1, \dots,$ $maximum$ $depth$ $]$
%     \begin{itemize}
%       \item for $tree$ in $[1, \dots,$ $number$ $of$ $trees$ $]$
%       \begin{itemize}
%         \item For each split, select ``$m$'' features from total available $M$ features and train a decision tree on selected features
%         
%       \end{itemize}
%     \end{itemize}
% \end{itemize}
% \end{frame}


\begin{frame}{Decision Tree \# 0}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-0.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 1}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-1.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 2}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-2.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 3}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-3.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 4}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-4.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 5}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-5.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 6}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-6.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 7}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-7.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 8}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-8.pdf}
  \end{figure}
\end{frame}


\begin{frame}{Decision Tree \# 9}
  \begin{figure}
    \includegraphics[scale=0.7]{tree-9.pdf}
  \end{figure}
\end{frame}

\begin{frame}{Feature Importance\footnotemark}
  \begin{figure}
    \includegraphics[scale=0.4]{mdi.pdf}
  \end{figure}
  Importance of variable $X_j$ for an ensemble of $M$ trees $\varphi_{m}$ is:
  \begin{equation*}
    \text{Imp}(X_j) = \frac{1}{M} \sum_{m=1}^M \sum_{t \in \varphi_{m}} 1(j_t = j) \Big[ p(t) \Delta i(t) \Big],
  \end{equation*}
  where $j_t$ denotes the variable used at node $t$, $p(t)=N_t/N$ and $\Delta i(t)$ is the impurity reduction at node $t$:
  \begin{equation*}
    \Delta i(t) = i(t) - \frac{N_{t_L}}{N_t} i(t_L) - \frac{N_{t_r}}{N_t} i(t_R)
  \end{equation*}
  \footnotetext[1]{Slide Courtesy Gilles Louppe}

\end{frame}


\begin{frame}{Computed Feature Importance}
  \includegraphics[scale=0.6]{feature-importance.pdf}
\end{frame}

\end{document}