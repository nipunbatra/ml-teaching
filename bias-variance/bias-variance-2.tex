\documentclass{beamer}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{../notation}
\usepackage{subfig}

%\beamerdefaultoverlayspecification{<+->}
% \newcommand{\data}{\mathcal{D}}
% \newcommand\Item[1][]{%
% 	\ifx\relax#1\relax  \item \else \item[#1] \fi
% 	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}

\graphicspath{ {imgs/} }

\usetheme{metropolis}           % Use metropolis theme


\title{Bias-Variance}
\date{\today}
\author{Nipun Batra and teaching staff}
\institute{IIT Gandhinagar}
\begin{document}
	\maketitle

\begin{frame}{The Scenario: True Function}
\vspace*{0.4cm}
For the purpose of this lecture we assume that there exists a relation between \textbf{Housing Prices} and \textbf{area of the house}.
\pause

Here, the true function $f_{\theta (true)}$ is used to model the relation $y_t = f_{\theta (true)}(x_t)$
\begin{figure}
\includegraphics[width=0.6\textwidth]{images/true.pdf}
\vspace*{-0.3cm}
\caption{Modeling the relation}
\end{figure}
\end{frame}

\begin{frame}{The 3 Sources of Error}
Any prediction made is effected by 3 sources of error:
\begin{itemize}
\item Noise
\item Bias
\item Variance
\end{itemize}
\end{frame}

\begin{frame}{Noise}
\vspace*{0.4cm}

\only<1-2>{A relation between \textbf{price} and \textbf{size} will be affected by other factors that we have not considered or cannot be perfectly captured. Such factors would include:
\begin{itemize}
\item the condition of the house (cannot be measured perfectly)
\item sale prices of other houses in the neighborhood (measurements that have biases in themselves)
\end{itemize}}

\only<2-2>{It is because of this data is inherently noisy.}
\only<3-4>{This is \textbf{not} a property of data and is instead an \textbf{irreducible error}.}

\only<4-4>{
This error can be captured by the error term $\epsilon$ which causes the final value of the house to follow the equation: $y_t = f_{\theta (true)}(x_t)+\epsilon_t$
}

\only<5->{This noise can be assumed to be mean centered around $0$ and has spread that is called the variance of the noise. 

This causes $y_t$ to become mean centered around the true relation.}
\vspace*{-0.6cm}
\only<4-4>{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/data.pdf}
\vspace*{-0.3cm}
\caption{Modeling the relation}
\end{figure}}
\only<5->{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/data_var.pdf}
\vspace*{-0.3cm}
\caption{Modeling the relation}
\end{figure}}

\end{frame}

\begin{frame}{Bias}
\vspace*{0.4cm}
\only<1-2>{\textbf{Bias} is a measure of how well a model can fit a given relation.}

\only<2-3>{
To understand this let us take an example where we try to learn the relation that models the \textit{Price} and \textit{Size} of a house using a \textit{constant function}. \vspace*{-0.4cm}}

\only<2>{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/biasn_1.pdf}
\vspace*{-0.3cm}
\caption{An example dataset}
\end{figure}}

\only<3>{\begin{figure}
\includegraphics[width=0.8\textwidth]{images/biasn_2.pdf}
\vspace*{-0.3cm}
\caption{An example dataset}
\end{figure}}

\only<4>{
So the bias in this scenario looks something like this:
\begin{figure}
\includegraphics[width=0.8\textwidth]{images/biasn_3.pdf}
\vspace*{-0.3cm}
\caption{An example dataset}
\end{figure}
}

\end{frame}

\begin{frame}{Bias}
\vspace*{0.4cm}
\only<1-4>{But it is important to understand that there are a large number of different datasets possible for a given situation, with each having their individual fits.}

\only<2>{Assume that we have two datasets of houses sold.}
\only<2>{\begin{figure}
\includegraphics[width=\textwidth]{images/bias1.pdf}
\vspace*{-0.3cm}
\caption{Two Datasets from same relation}
\end{figure}}

\only<3>{If we try to fit a constant function to them.}
\only<3>{\begin{figure}
\includegraphics[width=\textwidth]{images/bias2.pdf}
\vspace*{-0.3cm}
\caption{Two Datasets from same relation}
\end{figure}}

\only<4>{We see that they show different predictions.}
\only<4>{\begin{figure}
\includegraphics[width=\textwidth]{images/bias3.pdf}
\vspace*{-0.3cm}
\caption{Two Datasets from same relation}
\end{figure}}

\only<5-6>{Doing so for all possible size N training sets we get}

\only<6>{A way of consolidating all these possible fits is to calculate an average fit that is weighted by how likely they are to appear.}

\only<5-6>{\begin{figure}
\includegraphics[width=0.8\textwidth]{images/bias4.pdf}
\vspace*{-0.4cm}
\caption{Fits on different training sets}
\end{figure}}

\only<7>{Averaging all the fits (as in this scenario all datasets are equally likely) we get the average fit.}
\only<7>{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/bias5.pdf}
\vspace*{-0.3cm}
\caption{Average fit on all different training sets}
\end{figure}}

\end{frame}

\begin{frame}{Bias Contribution}
\only<1->{$$\text{Bias}(x) = f_{\theta(true)}(x) - f_{\bar\theta}(x)$$}
\only<2->{It is measure of how flexible the fit is in capturing $f_{\theta(true)}(x)$}
\only<1->{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/bias6.pdf}
\vspace*{-0.3cm}
\caption{Bias of the fit}
\end{figure}}

\end{frame}

\begin{frame}{Bias Contribution: Effect of Complexity}
\only<1->{As we increase the complexity of the fit \\ $\implies$ fit becomes more flexible \\ $\implies$ bias decreases}
\only<1>{\begin{figure}
\includegraphics[width=\textwidth]{images/bias7.pdf}
\vspace*{-0.3cm}
\caption{Effect of degree on Bias of the fit}
\end{figure}}
\only<2>{\begin{figure}
\includegraphics[width=\textwidth]{images/bias8.pdf}
\vspace*{-0.3cm}
\caption{Effect of degree on Bias of the fit}
\end{figure}}
\end{frame}

\begin{frame}{Bias: Calculating the Bias}
Bias calculation for a model is at the core a calculation of the area under a curve. 

Therefore, finding the bias for a model in the range $(a,b)$ is the calculation of the integral: $$ \int_a^b |f_{\bar{\theta}}(x)-f_{\theta(\text{true})}(x)| dx $$
\end{frame}

\begin{frame}{Variance}
\only<1->{Variance of the fit is a measure of the variation in the fits when trained across different training sets.}
\only<1->{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/var1.pdf}
\vspace*{-0.3cm}
\caption{Variance of the fit}
\end{figure}}
\end{frame}

\begin{frame}{Variance Contribution}
\only<1>{For Low Complexity \\ $\implies$ variations between curves are less \\ $\implies$ Variance is less}
\only<1>{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/var2.pdf}
\vspace*{-0.3cm}
\caption{Variance of the low complexity fits}
\end{figure}}
\only<2>{\vspace*{0.3cm} For High Complexity we see very high variation}
\only<2>{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/var3.pdf}
\vspace*{-0.3cm}
\caption{Variance in high complexity fits}
\end{figure}}
\only<3>{\vspace*{0.3cm} For High Complexity \\ $\implies$ high variation \\ $\implies$ Variance is high}
\only<3>{\begin{figure}
\includegraphics[width=0.8\textwidth]{images/var4.pdf}
\vspace*{-0.3cm}
\caption{Variance of the high complexity fits}
\end{figure}}

\end{frame}

\begin{frame}{The Bias-Variance Trade off}
\only<1>{\begin{figure}
\includegraphics[width=\textwidth]{images/bv-2.pdf}
\vspace*{-0.3cm}
\caption{Variance in high complexity fits}
\end{figure}}
\only<2>{\textbf{Plot Graph - 3:06 Variance and the bias-variance trade off}}
\end{frame}

\section{Mathematically Formulating the Error of a Model}

\begin{frame}{Measuring the goodness of a Model}
To measure the goodness of a model, we have to understand how well it can predict the behavior of the phenomenon it is trying to model.
\pause

This behavior varies due to training set randomness.
\pause 

Therefore, it is important to measure performance \textbf{averaged over all possible training sets} (of size N). 
\pause

$$E_{\text{training set}}[\text{error of } \hat\theta(\text{training set})] $$
gives a measure of the average error by doing an expectation of the errors of all possible training sets of size N.
\end{frame}

\begin{frame}{Expected Prediction Error at a point}
Any prediction made is effected by 3 sources of error:
\begin{itemize}
\item Noise
\item Bias
\item Variance
\end{itemize}
\pause

Therefore, $E_{train}[\text{at a point } x_t] = f(\text{noise, bias, variance}) $ 

\end{frame}

\begin{frame}{Formally defining the 3 sources of error: Noise}
\only<1->{Noise is an \textbf{irreducible error} that is capture by the error term $\epsilon$.

The equation of the relation becomes $y_t = f_{\theta (true)}(x_t)+\epsilon_t$}

\only<2->{The noise is mean centered around $0$ and has spread that is called the variance of the noise which is denoted by $\sigma^2$. }

\only<3>{That is it can be denoted by $\epsilon_t \in \mathcal{N}(0, \sigma^2)$}

\vspace*{-0.6cm}
\only<2>{\begin{figure}
\includegraphics[width=0.7\textwidth]{images/data_var.pdf}
\vspace*{-0.3cm}
\caption{Variance in the noise}
\end{figure}}

\end{frame}

\begin{frame}{Formally defining the 3 sources of error: Bias}
\only<1->{Bias is a measure of how flexible the fit is in capturing the true function $f_{\theta(true)}(x)$}
\only<1->{$$\text{Bias}(x_t) = f_{\theta(true)}(x_t) - f_{\bar\theta}(x_t)$$
where $f_{\bar\theta}$ denotes the average fit over all datasets.
}
\only<1>{\begin{figure}
\includegraphics[width=0.65\textwidth]{images/bias6.pdf}
\vspace*{-0.3cm}
\caption{Bias of the fit}
\end{figure}}

\only<2->{As $f_{\bar\theta}$ denotes the average fit over all datasets, it can be expressed by $f_{\bar\theta}(x_t) = E_{train}[f_{\hat{\theta}}(x_t)]$
}
\end{frame}

\begin{frame}{Formally defining the 3 sources of error: Variance}
\only<1->{Variance of the fit is a measure of the variation in the fits when trained across different training sets.}

\only<2->{Variance of the fit can be defined by
$$\text{var}(f_{\hat{\theta}}(x_t)) = E_{train}[(f_{\hat{\theta}}(x) - f_{\bar\theta}(x_t))^2]$$

where $f_{\hat{\theta}}(x) - f_{\bar\theta}(x_t)$ denotes the deviation that a specific fit has from the average.
}
\only<1>{\begin{figure}
\includegraphics[width=0.8\textwidth]{images/var4.pdf}
\vspace*{-0.3cm}
\caption{Variance of a fit}
\end{figure}}
\end{frame}

\begin{frame}{Deriving Expected Prediction Error}
Now we will see how, 
$ E_{train}[\text{at a point } x_t] = \sigma^2 + [\text{bias}(f_{\hat{\theta}}(x_t))]^2 + \text{var}(f_{\hat{\theta}}(x_t)) $

where, 

given a training set, the parameters $\hat{\theta}$ of the fit are learned as $f_{\hat{\theta}}$ 

and, the prediction at a point $x_t$ for the model trained on that training set is $f_{\hat{\theta}}(x_t)$
\end{frame}

\begin{frame}{Deriving Expected Prediction Error}
Prediction Error at a point $x_t$ can be calculated using the squared loss function.\\
\vspace{0.5cm}
 Prediction error at $x_t$ = $(y_t - f_{\hat\theta(train)}(x_t))^2$\\
\vspace{0.5cm}
To find the ``Expected Prediction Error'' at a point $x_t$ we average out the prediction error at that point over all possible learned models. This can be done by finding the expectation of prediction error for that point over all possible training datasets ($train$) and labels for that point ($y_t$).  \\
\vspace{0.5cm}
Expected prediction error at $x_t$ = $E_{train,y_t}[(y_t - f_{\hat\theta(train)}(x_t))^2]$\\
\end{frame}

\begin{frame}{Deriving Expected Prediction Error}
Expected prediction error at $x_t$ = $E_{train,y_t}[(y_t - f_{\hat\theta(train)}(x_t))^2]$\\
\vspace{0.5cm}
\only<2>{
=  $E_{train,y_t}[(          (y_t - f_{\theta(true)}(x_t))     + (f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )            )^2]$
}
\only<3->{
=  $E_{train,y_t}[(      \underbrace{(y_t - f_{\theta(true)}(x_t))}_\text{a}   +   \underbrace{(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) ) }_\text{b}          )^2]$\\
\vspace{0.5cm}
}
\only<4->{
=  $E_{train,y_t}[(a + b) ^ 2]$\\
\vspace{0.5cm}
}
\only<5->{
=  $E_{train,y_t}[a^2 + 2ab + b^2]$\\
\vspace{0.5cm}
}
\only<6->{
(Using Linearity of Expectation)\\
=  $E_{train,y_t}[a^2] + 2E_{train,y_t}[ab] + E_{train,y_t}[b^2]$.......................(Eqn. 1) \\
}

\end{frame}


\begin{frame}{Deriving Expected Prediction Error}
$E_{train,y_t}[a^2]  = E_{train,y_t}[(y_t - f_{\theta(true)}(x_t))^2] $\\
\vspace{0.5cm}
\only<2>{
 \hspace{1.70cm}(Since there is no dependence on training set)\\
\hspace{1.70cm} $ =  E_{y_t}[(y_t - f_{\theta(true)}(x_t))^2] $\\
 \vspace{0.5cm}
}
\only<3->{
 \hspace{1.70cm}($\because$ there is no dependence on training set)\\
 \vspace{0.3cm}
\hspace{1.70cm} $ =  E_{y_t}[\underbrace{(y_t - f_{\theta(true)}(x_t))^2}_\text{$\epsilon_t^2$}] $\\
 \vspace{0.5cm}
}
\only<3->{
\hspace{1.70cm} $ =  E_{y_t}[\epsilon_t^2] $\\
 \vspace{0.5cm}
}
\only<4->{
\hspace{1.70cm} $ =  \sigma^2 $(By definition) \\
 \vspace{0.5cm}
}

\only<5->{
$ E_{train,y_t}[a^2] =  \sigma^2  $.................(Eqn. 2)\\
 \vspace{0.5cm}
}
\end{frame}


\begin{frame}{Deriving Expected Prediction Error}
\only<1>{
$E_{train,y_t}[ab]  = E_{train,y_t}[(y_t - f_{\theta(true)}(x_t))(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )] $\\
\vspace{0.5cm}
}
\only<2->{
$E_{train,y_t}[ab]  = E_{train,y_t}[\underbrace{(y_t - f_{\theta(true)}(x_t))}_\text{$\epsilon_t$}(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )] $\\
\vspace{0.5cm}
}
\only<3->{
\hspace{1.70cm} $= E_{train,y_t}[\epsilon_t (f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )] $\\
\vspace{0.5cm}
}
\only<4>{
\hspace{1.70cm} ( $\because \epsilon_t$ and $(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t))$ are independent)\\
\vspace{0.3cm}
\hspace{1.70cm} $= E_{train,y_t}[\epsilon_t] \times E_{train,y_t}[(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )] $\\
\vspace{0.5cm}
}
\only<5->{
\hspace{1.70cm} ( $\because \epsilon_t$ and $(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t))$ are independent)\\
\vspace{0.3cm}
\hspace{1.70cm} $= \underbrace{E_{train,y_t}[\epsilon_t]}_\text{ = 0 } \times E_{train,y_t}[(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )] $\\
\vspace{0.25cm}
\hspace{1.70cm} (By definition $\epsilon_t$ has mean 0)\\
\vspace{0.5cm}
}
\only<6->{
$E_{train,y_t}[ab] = 0$..............(Eqn. 3)
}


\end{frame}

\begin{frame}{Deriving Expected Prediction Error}
$ E_{train,y_t}[b^2] =  E_{train, y_t}[(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )^2]$\\
\vspace{0.5cm}
\only<2->{
($f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t)$ is independent of $y_t$)\\
\vspace{0.5cm}
\hspace{1.70cm} $=  E_{train}[(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )^2]$\\
\vspace{0.5cm}
}
\only<3->{
\hspace{1.70cm} $ = MSE( f_{\hat\theta(train)}(x_t))$\\
\vspace{0.5cm}
}
\only<4->{
$ E_{train,y_t}[b^2] = MSE( f_{\hat\theta(train)}(x_t))$ ............ (Eqn. 4)
}
\end{frame}

\begin{frame}{Deriving Expected Prediction Error}
From Eqn. 1, 2, 3 and 4, we get, \\
\vspace{1cm}
Expected prediction error at $x_t$ = $\sigma^2 + MSE( f_{\hat\theta(train)}(x_t)) $ \\
\vspace{1cm}
Now, we will further simplify the MSE term into bias and variance.
\end{frame}

\begin{frame}{Deriving Expected Prediction Error}
$MSE( f_{\hat\theta(train)}(x_t)) =  E_{train}[(f_{\theta(true)}(x_t) -  f_{\hat\theta(train)}(x_t) )^2]$\\
\vspace{0.5cm}
\only<2>{
$= E_{train}[(   (f_{\theta(true)}(x_t)  -  f_{\bar\theta}(x_t) ) + (f_{\bar\theta}(x_t)  -  f_{\hat\theta(train)}(x_t)      ) )^2]$\\
}
\only<3->{
$= E_{train}[(   \underbrace{(f_{\theta(true)}(x_t)  -  f_{\bar\theta}(x_t) )}_\text{$\alpha$} + \underbrace{(f_{\bar\theta}(x_t)  -  f_{\hat\theta(train)}(x_t)      )}_\text{$\beta$} )^2]$\\
\vspace{0.5cm}
}
\only<4->{
$= E_{train}[( \alpha + \beta )^2]$\\
\vspace{0.5cm}
}
\only<5->{
$= E_{train}[ \alpha^2 + 2\alpha\beta + \beta ^2]$\\
\vspace{0.5cm}
}
\only<6->{
(Using Linearity of Expectation)
$= E_{train}[ \alpha^2] + 2E_{train}[ \alpha\beta] + E_{train}[ \beta^2]$ ..........(Eqn. 5)\\
\vspace{0.5cm}
}
\end{frame}


\begin{frame}{Deriving Expected Prediction Error}
$E_{train}[\alpha^2]  = E_{train}[(f_{\theta(true)}(x_t)  -  f_{\bar\theta}(x_t))^2]$\\
\vspace{0.5cm}
\only<2->{
\hspace{1.45cm} $  = E_{train}[(f_{\theta(true)}(x_t)  -  E_{train}[f_{\hat\theta(train)}(x_t)]^2]$\\
\vspace{0.5cm}
}
\only<3->{
\hspace{1.45cm} $ = E_{train}[bias(f_{\hat\theta}(x_t))^2]$\hfill(By definition of bias)\\
\vspace{0.5cm}
}
\only<4->{
\hspace{1.45cm} $ = bias(f_{\hat\theta}(x_t))^2 $\\
\vspace{0.5cm}
\hspace{1.45cm} ($\because$ bias is not a function of training data)\\

}
\only<5->{
\vspace{0.5cm}
$E_{train}[\alpha^2]  = bias(f_{\hat\theta}(x_t))^2$ .............(Eqn. 6)
}
\end{frame}


\begin{frame}{Deriving Expected Prediction Error}
$E_{train}[\alpha\beta] $ \\
$= E_{train}[(f_{\theta(true)}(x_t)  -  f_{\bar\theta}(x_t))(f_{\bar\theta}(x_t)  -  f_{\hat\theta(train)}(x_t)   )]$\\
\vspace{0.5cm}
\only<2->{
$ = E_{train}[bias_t \times (f_{\bar\theta}(x_t)  -  f_{\hat\theta(train)}(x_t)   )]$\\
\vspace{0.5cm}
}
\only<3->{
$ = bias_t \times E_{train}[f_{\bar\theta}(x_t)  -  f_{\hat\theta(train)}(x_t)  ]$\\
\vspace{0.5cm}
($\because$ bias$_t$ is not a function of training data)\\
\vspace{0.5cm}
}
\only<4->{
$ = bias \times \left( E_{train}[f_{\bar\theta}(x_t)]  -  E_{train}[f_{\hat\theta(train)}(x_t) ] \right)$\\
\vspace{0.5cm}
}
\only<5>{
$ = bias \times \left( f_{\bar\theta}(x_t)  -  f_{\bar\theta}(x_t)  \right) $\\
\vspace{0.5cm}
($\because f_{\bar\theta}(x_t) =  E_{train}[f_{\hat\theta(train)}(x_t)$ )\\
\vspace{0.5cm}
}
\only<6->{
$ = bias \times \left( f_{\bar\theta}(x_t)  -  f_{\bar\theta}(x_t)  \right) $\\
\vspace{0.5cm}
$E_{train}[\alpha\beta]  = 0$..........................(Eqn. 7) 
\vspace{0.5cm}
}

\end{frame}



\begin{frame}{Deriving Expected Prediction Error}
$E_{train}[\beta^2]  = E_{train}[(f_{\bar\theta}(x_t)  -  f_{\hat\theta(train)}(x_t) )^2]$ \\
\vspace{0.5cm}
\only<2->{
\hspace{1.45cm} $=  E_{train}[(f_{\hat\theta(train)}(x_t) - f_{\bar\theta}(x_t))^2]$\\
\vspace{0.5cm}
}
\only<3->{
\hspace{1.45cm} $=  E_{train}[(f_{\hat\theta(train)}(x_t) - E_{train}[(f_{\hat\theta(train)}(x_t)])^2]$\\
\vspace{0.5cm}
\hspace{1.45cm} ($\because f_{\bar\theta}(x_t) = E_{train}[(f_{\hat\theta(train)}(x_t)] $ )\\
\vspace{0.5cm}
}
\only<4->{
\hspace{1.45cm} $= variance(f_{\hat\theta}(x_t))$\\
\vspace{0.5cm}
}
\only<5->{
$E_{train}[\beta^2] = variance(f_{\hat\theta}(x_t))$...............(Eqn. 8)\\
}

\end{frame}



\begin{frame}{Deriving Expected Prediction Error}
From Eqn. 1 - 8, we get, \\
\vspace{0.5cm}
Expected prediction error at $x_t$ \\
\vspace{0.5cm}
$ = \sigma^2 + MSE( f_{\hat\theta(train)}(x_t)) $\\
\vspace{0.5cm}
$ = \sigma^2 +bias(f_{\hat\theta}(x_t))^2 + variance(f_{\hat\theta}(x_t))$

\end{frame}





\end{document}