\documentclass[usenames,dvipsnames]{beamer}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subcaption}

\usepackage[labelformat=empty]{caption}
\usepackage{color, colortbl}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\newcommand*{\mathcolor}{}
\def\mathcolor#1#{\mathcoloraux{#1}}
\newcommand*{\mathcoloraux}[3]{%
	\protect\leavevmode
	\begingroup
	\color#1{#2}#3%
	\endgroup
}

\usepackage{tikz}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\newcommand{\DoTikzmark}[1]{%
	\tikz[remember picture] \coordinate[shift={(0,.7ex)}](#1);%
}

\newcommand{\colrow}[3][]{%
	\tikz[overlay,remember picture, line width=10pt]
	\draw[shorten >=-.1em, shorten <=-.1em, #1] (#2)--(#3);
}


\newcolumntype{g}{>{\columncolor{Lavender}}c}
\newcolumntype{t}{>{\columncolor{Tan}}c}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}
	


\usetheme{metropolis}           % Use metropolis theme


\title{Convention, Accuracy metrics, Classification, Regression}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  



\begin{frame}{First ML Task: Grocery store tomatoes quality prediction}
Problem statement: You want to predict the quality/condition of a tomato given its visual features.
\end{frame}

\begin{frame}{Dataset}
Imagine you have some past data on quality of tomatoes. What visual features do you think will be useful?

\begin{itemize}
	\item \pause Size
	\item \pause Colour
	\item \pause Texture
\end{itemize}
\end{frame}
  
\begin{frame}{Dataset}
Imagine you have some past data on quality of tomatoes. 

\begin{table}[]
	\begin{tabular}{|l|l|l|l||l|}
		\hline 
		\textbf{Sample} & \textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		1      & Orange & Small & Smooth  & Good      \\
		2      & Red    & Small  & Rough  & Good \\
		3      & Orange & Medium & Smooth & Bad \\
		4      & Yellow & Large  & Smooth & Bad \\ \hline          
	\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Useful Features}
Is the sample number a useful feature for predicting quality of a tomato?

\pause Answer: It depends! Maybe, all tomatoes received after a certain date are bad! Let us ignore that for now.

\pause Let us modify our data table for now.

\begin{table}[]
	\begin{tabular}{|l|l|l||l|}
		\hline 
		\textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		Orange & Small & Smooth  & Good      \\
		Red    & Small  & Rough  & Good \\
		Orange & Medium & Smooth & Bad \\
		Yellow & Large  & Smooth & Bad \\ \hline 

	\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Training Set}

\begin{table}[]
	\begin{tabular}{|g|g|g||t|}
		\hline 
				\rowcolor{white}
		\textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		Orange & Small & Smooth  & Good      \\
		Red    & Small  & Rough  & Good \\
		Orange & Medium & Smooth & Bad \\
		Yellow & Large  & Smooth & Bad \\ \hline 
		
	\end{tabular}
\end{table}

\pause The training set consists of two parts:
\begin{enumerate}
	\item \pause \color{Lavender}{Features, Attributes or Covariates}
	\item \pause \color{Tan}{Output or Response Variable}
\end{enumerate}
\end{frame}

\begin{frame}{Training Set}
\vspace{-5pt}
\begin{table}[]
	\begin{tabular}{|g|g|g||t|}
		\hline 
		\rowcolor{white}
		\textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		Orange & Small & Smooth  & Good      \\
		Red    & Small  & Rough  & Good \\
		Orange & Medium & Smooth & Bad \\
		Yellow & Large  & Smooth & Bad \\ \hline 
		
	\end{tabular}
\end{table}


\pause We call this matrix as $\mathcal{D}$, containing:
\begin{enumerate}
	\item \pause Feature matrix ($\mathbf{X \in \mathcal{R}^{N\times P}}$) containing data of $N$ samples each of which is $P$ dimensional.
	\begin{itemize}
		\item \pause Thus, $\mathbf{X} = \{x_i^T\}_{i=1}^N$ where $x_i \in \mathcal{R}^P$
		\item \pause Example $x_1 = \begin{bmatrix}
		Orange \\
		Small \\
		Smooth \\
		\end{bmatrix}
		$
	\end{itemize}
\item \pause Output Vector ($y \in \mathcal{R}^N$) containing output variable for $N$ samples.
\item \pause Thus, we can also write $\mathcal{D} = \{(x_i^T, y_i)\}_{i=1}^N$
\end{enumerate}

\end{frame}


\begin{frame}{Prediction Task}
Estimate condition for unseen tomatoes (\#5, 6) based on data set. 

\begin{table}[]
	\begin{tabular}{|l|l|l||l|}
		\hline 
		
		\textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		Orange & Small & Smooth  & Good      \\
		Red    & Small  & Rough  & Good \\
		Orange & Medium & Smooth & Bad \\
		Yellow & Large  & Smooth & Bad \\ \hline
		Red    & Large  & Rough  & ? \\
		Orange &  Large & Rough  & ? \\ \hline          
	\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Testing Set}
\textcolor{YellowGreen}{Testing set} is similar to \textcolor{Peach}{training set}, but, does not contain labels for output variable. 

\begin{table}[]
	\begin{tabular}{|l|l|l||l|}
		\hline 
		\textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		\rowcolor{Peach}
		Orange & Small & Smooth  & Good      \\
		\rowcolor{Peach}
		Red    & Small  & Rough  & Good \\
		\rowcolor{Peach}
		Orange & Medium & Smooth & Bad \\
		\rowcolor{Peach}
		Yellow & Large  & Smooth & Bad \\ \hline
				\rowcolor{YellowGreen}
		Red    & Large  & Rough  & ? \\
		\rowcolor{YellowGreen}
		Orange &  Large & Rough  & ? \\ \hline          
	\end{tabular}
\end{table}
\end{frame}



\begin{frame}{Prediction Task}
We hope to:
\begin{enumerate}
	\item \pause Learn $f$: 		$\texttt{Condition} = f(\texttt{colour, size, texture})$
	\item \pause From Training Dataset
	\item \pause To Predict the condition for the Testing set
\end{enumerate}


\begin{table}[]
	\begin{tabular}{|l|l|l||l|}
		\hline 
		
		\textbf{Colour} & \textbf{Size} & \textbf{Texture} & \textbf{Condition} \\ \hline 
		Orange & Small & Smooth  & Good      \\
		Red    & Small  & Rough  & Good \\
		Orange & Medium & Smooth & Bad \\
		Yellow & Large  & Smooth & Bad \\ \hline
		Red    & Large  & Rough  & ? \\
		Orange &  Large & Rough  & ? \\ \hline          
	\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Generalisation}
\begin{itemize}
	\item Q: Is predicting on test set enough to say our model generalises? 
	\item \pause A: Ideally, no!
	\item \pause Ideally - we want to predict ``well'' on all possible inputs. But, can we test that?
	\item \pause No! Since, the test set is only a sample from all possible inputs.
\end{itemize}





\end{frame}

\begin{frame}{Generalisation}
\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{train-test}
	\caption{Image courtesy Google ML crash course}
	\label{fig:train-test}
\end{figure}

\pause Both the training set and the test set are samples drawn from the hidden true distribution (also sometimes called population)

\pause More discussion later once we study bias and variance
\end{frame}

\begin{frame}{Second ML Task: Predict energy consumption of campus}

Question: What factors does the campus energy consumption depend on?

Answer:\begin{itemize}
	\item \pause \# People (More people $\implies$ More Energy)
	\item \pause Temperature (Higher Temp. $\implies$ Higher Energy)

\pause \begin{table}[]
	\begin{tabular}{|l|l||l|}
		\hline 
		
		\textbf{\# People} & \textbf{Temp (C)} &  \textbf{Energy (kWh)} \\ \hline 
		
		4000 & 30 & 30 \\
		4200 & 30 & 32 \\
		4200 & 35 & 40 \\ \hline
		3000 & 20& ? \\
		1000 & 45 & ? \\ \hline          
	\end{tabular}
\end{table}	
\end{itemize}

\end{frame}





\begin{frame}{Classification v/s Regression}
\begin{itemize}
	\item Classification
	\begin{itemize}
		\item \pause Output variable is discrete
		\item \pause i.e.  $y_i\in \{1, \cdots C\}$ 
		\item \pause Examples - Predicting: 
		\begin{itemize}
			\item \pause Will I get a loan? (Yes, No)
			\item \pause What is the quality of fruit? (Good, Bad)
		\end{itemize}
	\end{itemize}
	\item \pause Regression
	\begin{itemize}
		\item \pause Output variable is continuous
		\item \pause i.e.  $y_i\in \mathcal{R}$ 
		\item \pause Examples - Predicting: 
		\begin{itemize}
			\item \pause How much energy will campus consume? 
			\item \pause How much rainfall will fall?
		\end{itemize}
	\end{itemize}
\end{itemize}

\end{frame}

%\begin{frame}{Feature Table}
%\begin{figure}[htp]
%    \centering
%    \includegraphics[width=0.7\linewidth]{accuracy/ml_2_accuracy_table_1.png}
%\end{figure}
%\end{frame}

%\captionsetup[subtable]{labelformat=empty}
%
%\begin{frame}{Metrics for Classification}
%\begin{table}[!htb]
%	\caption{}
%	\begin{subtable}{.5\linewidth}
%		\centering
%		\caption{Prediction ($\hat{y}$)}
%		\begin{tabular}{ |c| } \hline 
%			
%			Good \\
%			Good \\
%			Good \\
%			Good \\
%			Bad  \\ \hline 
%			
%		\end{tabular}
%	\end{subtable}%
%	\begin{subtable}{.5\linewidth}
%		\centering
%		\caption{Ground Truth ($y$)}
%		\begin{tabular}{ |c| } 
%			\hline 
%			Good \\
%			Good \\
%			Bad \\
%			Bad  \\
%			Bad \\ \hline 
%		\end{tabular} \\
%	\end{subtable} 
%\end{table}
%
%\begin{align*}
%\texttt{Accuracy} &= \frac{||y = \hat{y}||}{||y||} \end{align*}
%\end{frame}

%\begin{frame}{Metrics for Classification}
%\begin{table}[!htb]
%	\caption{}
%	\begin{subtable}{.5\linewidth}
%		\centering
%		\caption{Prediction ($\hat{y}$)}
%		\begin{tabular}{ |c| } \hline 
%			
%			\rowcolor{Green}Good \\
%			\rowcolor{Green}Good \\
%			\rowcolor{Red}Good \\
%			\rowcolor{Red}Good \\
%			\rowcolor{Green}Bad  \\ \hline 
%			
%		\end{tabular}
%	\end{subtable}%
%	\begin{subtable}{.5\linewidth}
%		\centering
%		\caption{Ground Truth ($y$)}
%		\begin{tabular}{ |c| } 
%			\hline 
%			Good \\
%			Good \\
%			Bad \\
%			Bad  \\
%			Bad \\ \hline 
%		\end{tabular} \\
%	\end{subtable} 
%\end{table}
%
%\begin{align*}
%\texttt{Accuracy} &= \frac{||y = \hat{y}||}{||y||} \\
%&= \frac{3}{5} = 0.6
%\end{align*}
%\end{frame}
%
%%\begin{frame}
%%\setlength{\tabcolsep}{12pt}
%%\begin{tabular}{ cc }   % top level tables, with 2 rows
%%	Ground Truth ($y$) & Prediction ($\hat{y}$) \\  
%%	% bottom left of the top level table: table 1 
%%	\begin{tabular}{ |c| } 
%%		
%%		Good \\
%%		Good \\
%%		Good \\
%%		Good \\
%%		Bad  \\
%%		
%%	\end{tabular} &  % starting bottom right of the top level table
%%	% table 2
%%	\begin{tabular}{ |c| } 
%%		Good \\
%%		Good \\
%%		Bad \\
%%		Bad  \\
%%		Bad \\
%%	\end{tabular} \\
%%\end{tabular}
%%\end{frame}
%%
%%\begin{frame}{Metrics for Classification}
%%
%%
%%
%%$
%%Prediction (\hat{y}) = \begin{bmatrix}
%%	Good \\
%%	Good \\
%%	Good \\
%%	Good \\
%%	Bad  \\
%%\end{bmatrix} ~~~~Ground Truth (y) = \begin{bmatrix}
%%Good \\
%%Good \\
%%Bad \\
%%Bad  \\
%%Bad \\
%%\end{bmatrix}$ 
%%\vspace{30pt}
%%
%%\begin{itemize}
%%	\item Ground Truth: From the actual training set 
%%	\item Prediction: Made by the model
%%\end{itemize}
%%
%%\end{frame}
%%
%%\begin{frame}{Metrics for Classification}
%%%\DoTikzmark{num-3}{-}3 & 0 & {4}\DoTikzmark{num4}
%%$
%%Prediction (\hat{y}) = \begin{bmatrix}
%%Good \\
%%Good \\
%%Good \\
%%Good \\
%%Bad  \\
%%\end{bmatrix} ~~~~Ground Truth (y) = \begin{bmatrix}
%%Good \\
%%Good \\
%%Bad \\
%%Bad  \\
%%Bad \\
%%\end{bmatrix}$ 
%%
%%
%%\vspace{30pt}
%
%
%
%%\end{frame}



\begin{frame}{Metrics for Classification}



$$\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Bad}}
                \qquad \qquad
   \bordermatrix{&\texttt{Ground Truth}\;(y)\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Bad}\cr
                &\texttt{Bad}\cr
                &\texttt{Bad}}
$$

\vspace{1cm}

\begin{tabular}{ll}
Ground Truth: & From the actual training set \\ 
Prediction: & Made by the model \\ 
\end{tabular}

\end{frame}

\begin{frame}{Accuracy}
$$
\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
               \checkmark&\texttt{Good}\cr
               \checkmark&\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
               \checkmark&\texttt{Bad}}
\qquad \qquad
\bordermatrix{&\texttt{Ground Truth}\;(y)\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Bad}\cr
                &\texttt{Bad}\cr
                &\texttt{Bad}}
$$

\pause \begin{align*}
\texttt{Accuracy} &= \frac{||y = \hat{y}||}{||y||} \\ 
&= \frac{3}{5} = 0.6
\end{align*}

\end{frame}

\begin{frame}{Types of Data: Imbalanced Classes}
\[
  \begin{array}{@{} c @{}}
    \begin{array}{@{} r @{}}
      \text{1 sample}~\{\hspace{\nulldelimiterspace} \\
      \text{100 samples}~\left\{\begin{array}{@{}c@{}}\null\\\null\\\null\\\null\end{array}\right.
    \end{array}
    \left (
      \begin{array}{ *{1}{c} }
        \texttt{Bad}  \\
        \texttt{Good}  \\
        \texttt{Good}  \\
        \ldots  \\
        \texttt{Good}  \\
      \end{array}
    \right ) \\
    \mathstrut
  \end{array}
  \quad \quad \quad
  \text{Imbalanced Classes}
\]

\pause

Cases for this:
\begin{itemize}%[<+->]
\item Cancer Screening
\item Planet Detection
\end{itemize}

\end{frame}

\begin{frame}{Accuracy Metrics: Precision}
$$
\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
               \rightarrow\checkmark&\texttt{Good}\cr
               \rightarrow\checkmark&\texttt{Good}\cr
                \rightarrow&\texttt{Good}\cr
                \rightarrow&\texttt{Good}\cr
               &\texttt{Bad}}
\qquad \qquad
\bordermatrix{&\texttt{Ground Truth}\;(y)\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Bad}\cr
                &\texttt{Bad}\cr
                &\texttt{Good}}
$$

\begin{align*}
\pause \texttt{Precision} &= \frac{||y = \hat{y} = \texttt{Good}||}{||\hat{y} = \texttt{Good}||} = \frac{2}{4} = 0.5
\end{align*}

``the fraction of relevant instances among the retrieved instances'', i.e. ``out of the number of times we predict \texttt{Good}, how many times is the condition actually \texttt{Good}''

\end{frame}

\begin{frame}{Accuracy Metrics: Recall}
$$
\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
               \rightarrow\checkmark&\texttt{Good}\cr
               \rightarrow\checkmark&\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
               \rightarrow&\texttt{Bad}}
\qquad \qquad
\bordermatrix{&\texttt{Ground Truth}\;(y)\cr
                &\texttt{Good}\cr
                &\texttt{Good}\cr
                &\texttt{Bad}\cr
                &\texttt{Bad}\cr
                &\texttt{Good}}
$$

\begin{align*}
\texttt{Recall} &= \frac{||y = \hat{y} = \texttt{Good}||}{||y = \texttt{Good}||} = \frac{2}{3} = 0.67
\end{align*}

``the fraction of the total amount of relevant instances that were actually retrieved''

\end{frame}

\begin{frame}{Types of Data: Imbalanced Classes}
Given predictions of whether a tissue is cancerous or not ($n = 100$).
$$
\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
               \rightarrow&\texttt{No}\cr
               &\texttt{Yes}\cr
                &\texttt{Yes}\cr
                &\ldots\cr
               &\texttt{Yes}}
\qquad \qquad
\bordermatrix{&\texttt{Ground Truth}\;(y)\cr
                &\texttt{Yes}\cr
                &\texttt{Yes}\cr
                &\ldots\cr
                &\texttt{Yes}\cr
                \rightarrow&\texttt{No}}
$$

\pause 
\begin{align*}
\texttt{Accuracy} = \frac{98}{100} = 0.98 \qquad \qquad \qquad
\texttt{Recall} &= \frac{0}{99} = 0 \\
\texttt{Precision} &= \frac{0}{99} = 0
\end{align*}


\end{frame}

\begin{frame}{Accuracy Metrics: Confusion Matrix}
For the same data
\pause $$
\bordermatrix{&\texttt{G.T. Positive}&\texttt{G.T. Negative}\cr
               \texttt{Pred Positive}&0&1\cr
               \texttt{Pred Positive}&1&98}
$$
\pause $$
\bordermatrix{&\texttt{G.T. Positive}&\texttt{G.T. Negative}\cr
               \texttt{Pred Positive}&\texttt{True Positive}&\texttt{False Positive}\cr
               \texttt{Pred Positive}&\texttt{True Negative}&\texttt{False Negative}}
$$


\pause \begin{align*}
\texttt{Recall} = \frac{\texttt{T.P}}{\texttt{T.P + F.P}} \qquad \qquad 
\texttt{Precision} = \frac{\texttt{T.P}}{\texttt{T.P + F.N}}
\end{align*}
\end{frame}

\begin{frame}{Accuracy Metrics: F-Score}
For the same data
$$
\bordermatrix{&\texttt{G.T. Positive}&\texttt{G.T. Negative}\cr
               \texttt{Pred Positive}&\texttt{True Positive}&\texttt{False Positive}\cr
               \texttt{Pred Positive}&\texttt{True Negative}&\texttt{False Negative}}
$$

$$
\texttt{F-Score} = \frac{2\times\texttt{Precision}\times\texttt{Recall}}{\texttt{Precision} + \texttt{Recall}}
$$


\end{frame}


\begin{frame}{Accuracy Metrics: Matthew's Correlation Coefficient}
For the same data
$$
\bordermatrix{&\texttt{G.T. Positive}&\texttt{G.T. Negative}\cr
               \texttt{Pred Positive}&\texttt{True Positive}&\texttt{False Positive}\cr
               \texttt{Pred Positive}&\texttt{True Negative}&\texttt{False Negative}}
$$

\texttt{Matthew's Correlation Coefficient =}
$$
\frac{\texttt{TP $\times$ TN} - \texttt{FP $\times$ FN}}{\sqrt{\texttt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}}
$$
\end{frame}

\begin{frame}{Accuracy Metrics: Example}
For the same data
$$
\bordermatrix{&\texttt{G.T. Positive}&\texttt{G.T. Negative}\cr
               \texttt{Pred Positive}&90&4\cr
               \texttt{Pred Positive}&1&1}
$$

Precision = ?  \\
Recall = ?\\
F-Score = ?\\
Matthew's Coeff. = ?\\
\end{frame}

\begin{frame}{Accuracy Metrics: Answer}
For the same data
$$
\bordermatrix{&\texttt{G.T. Positive}&\texttt{G.T. Negative}\cr
               \texttt{Pred Positive}&90&4\cr
               \texttt{Pred Positive}&1&1}
$$

Precision = $\frac{90}{94}$ \\
Recall = $\frac{90}{91}$ \\
F-Score = 0.9524 \\
Matthew's Coeff. = 0.14
\end{frame}




%\begin{frame}{Regression Examples}
%\begin{figure}[htp]
%    \centering
%    \includegraphics[width=0.7\linewidth]{accuracy/reg1.png}
%\end{figure}
%
%\pause
%
%Regression involves continuous data.
%\end{frame}

\begin{frame}{Metrics for Regression MSE \& MAE}
$$
\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
               &10\cr
               &20\cr
                &30\cr
                &40\cr
               &50}
\qquad \qquad
\bordermatrix{&\texttt{Ground Truth}\;(y)\cr
               &20\cr
               &30\cr
                &40\cr
                &50\cr
               &60}
$$

\begin{align*}
\texttt{\textcolor{red}{Mean} \textcolor{green}{Squared} \textcolor{Tan}{Error} (MSE)} &=  \frac{\mathcolor{red}{\sum_{i=1}^{N}}(\mathcolor{Tan}{\hat{y}_i-y_i)}^{\mathcolor{green}2}}{\mathcolor{red}{N}} \\ 
\texttt{Root Mean Square Error (RMSE)} &=  \sqrt{\texttt{MSE}}
\end{align*}

\end{frame}

\begin{frame}{Accuracy Metrics: MAE \& ME}
$$
\bordermatrix{&\texttt{Prediction}\;(\hat{y})\cr
               &10\cr
               &20\cr
                &30\cr
                &40\cr
               &50}
\qquad \qquad
\bordermatrix{&\texttt{Ground Truth}\cr
               &20\cr
               &30\cr
                &40\cr
                &50\cr
               &60}
$$

\begin{align*}
\texttt{\textcolor{red}{Mean} \textcolor{green}{Absolute} \textcolor{Tan}{Error} (ME)} &=  \frac{\mathcolor{red}{\sum_{i=1}^{N}}\mathcolor{green}|\mathcolor{Tan}{\hat{y}_i-y_i\mathcolor{green}|}}{\mathcolor{red}{N}} \\ 
\texttt{Mean Error} &=  \frac{\sum_{i=1}^{N}\hat{y}_i-y_i}{N}
\end{align*}
\pause Is there any downside with using mean error?\\
\pause Errors can get cancelled out

\end{frame}


\begin{frame}{The Importance of Plotting}
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.6\linewidth]{accuracy/ans1.png}
    \caption{Anscombeâ€™s Quartet}
\end{figure}
\end{frame}

\begin{frame}{The Importance of Plotting}
\begin{tabular}{|c|c|c|}
\hline 
Property & Value & Accross datasets \\ 
\hline 
mean(X) & 9 & exact \\ 
mean(Y) & 7.5 & upto 3 decimal places \\ 
Linear regression line & 	y = 3.00 + 0.500x & upto 2 decimal places \\ 
\hline 
\end{tabular} 

Try to play with the \href{https://colab.research.google.com/drive/12Rrh7sf_lR-WUd4nYciOAKiil5kqWPAq}{colab link} to see how similar the metrics like variance and correlation are.

\end{frame}

\end{document}