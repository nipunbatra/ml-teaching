[
  {
    "objectID": "bias-variance/Charts.html",
    "href": "bias-variance/Charts.html",
    "title": "Bias New",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.legend()\nplt.savefig('images/true.pdf', transparent=True)\n\n\n\n\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\nplt.savefig('images/data.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n# plt.fill_between(data['x'], y_true-dy, y_true+dy, color='green',alpha=0.2, label='Variance')\nplt.errorbar(data['x'][15], y_true[15], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\n\nplt.legend()\n\n\nplt.savefig('images/data_var.pdf', transparent=True)\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\n\nplt.savefig('images/biasn_1.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.legend()\n\nplt.savefig('images/biasn_2.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.fill_between(x, y_true, [data['y'].mean() for _ in data['x']], color='green',alpha=0.2, label='Bias')\nplt.legend()\n\nplt.savefig('images/biasn_3.pdf', transparent=True)"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-old",
    "href": "bias-variance/Charts.html#bias-old",
    "title": "Bias New",
    "section": "Bias Old",
    "text": "Bias Old\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\n\n\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nplt.savefig('images/bias1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\nax[1].plot(x, [y2.mean() for _ in x], 'r:', label='Prediction')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train4)}$')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias4.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\n\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias5.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(x, y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nfit = np.array([(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x])\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nplt.fill_between(x, y_true, fit, color='green',alpha=0.2, label='Bias')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias6.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nVarying Degree on Bias\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nax[0].plot(x, [y.mean() for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nax[0].plot(data['x'], data['y'], '.b', label='Actual Prices')\nax[0].plot(data['x'], y_true,'g', label='True Function')\nax[0].fill_between(x, y_true, [y.mean() for _ in x], color='green',alpha=0.2, label='Bias')\nax[0].set_title(f\"Degree = 0\")\nfor i,deg in enumerate([1]):\n    i=i+1\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias7.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nfor i,deg in enumerate([2,3]):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias8.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#variance",
    "href": "bias-variance/Charts.html#variance",
    "title": "Bias New",
    "section": "Variance",
    "text": "Variance\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 25\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\n# plt.errorbar(x[::3], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::3], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\nplt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nVaraince Variation\n\nfrom sklearn.linear_model import LinearRegression\n\nfig, ax = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(10, 8))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles = []\n\nfor i,seed in enumerate([2,4,8,16]):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n    \n    ax[int(i/2)][i%2].plot(data_s['x'],data_s['y'], '.b', label='Data Point')\n#     ax[i].plot(data_n['x'],data_n['y'], 'ok', label='UnSelected Points')\n    ax[int(i/2)][i%2].plot(data_s['x'], y_pred,'r-.', label='Prediction')\n    ax[int(i/2)][i%2].plot(data['x'], y_true,'g-', label='True Function')\n#     ax[i].set_title(f\"{deg} : {max(regressor.coef_, key=abs):.2f}\")\n    \n\nhandles, labels = ax[0][0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nmodles = []\n\nfor i,seed in enumerate(range(1,50)):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(8, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles=np.array(modles)\n\n# ax[0].plot(x, modles.mean(axis=0), 'r-.', label='Average Fit')\n# ax[0].plot(data['x'], y_true,'g-', label='True Function')\n# ax[0].set_xlabel('Size (sq.ft)')\n# ax[0].set_ylabel('Price (\\$)')\n\n# ax[1].errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\n# ax[1].plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\n# ax[1].plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\n# ax[1].plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\n# ax[1].plot(data['x'], y_true,'g-', label='True Function')\n# ax[1].set_xlabel('Size (sq.ft)')\n\nax.errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\nax.plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\nax.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\nax.plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\nax.plot(data['x'], y_true,'g-', label='True Function')\nax.set_xlabel('Size (sq.ft)')\n\n# plt.plot(x, modles.mean(axis=0), 'k.-', label=r'Average Fit')\n# plt.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train3)}$')\n# plt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\n# handles, labels = [(a + b) for a, b in zip(ax[0].get_legend_handles_labels(), ax[1].get_legend_handles_labels())]\nhandles, labels = ax.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var4.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-variance-tradeoff",
    "href": "bias-variance/Charts.html#bias-variance-tradeoff",
    "title": "Bias New",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nx = x = np.linspace(0, 4*np.pi, 201)\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 1\np = np.poly1d([1, 2, 3])\ny = np.sin(x) + 0.5*x -  0.05*x**2 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5*x - 0.05*x**2\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\n# plt.xlabel('Size (sq.ft)')\n# plt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\n# plt.ylim(-2,2)\n# plt.xlim(0,4*np.pi)\nplt.plot(data['x'], data['y'], '.', label='Data Points')\nplt.legend()\nplt.savefig('images/bv-1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(10, 5))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,4*np.pi))\n\ndegs = [1,3,7]\nfor i,deg in enumerate(degs):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n    \n#     print(predictors)\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n#     ax[0][i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[0][i].plot(data['x'], y_pred,'-.r', label='Prediction')\n    ax[0][i].plot(data['x'], y_true,'g', label='True Function')\n    ax[0][i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[0][i].set_title(f\"Degree = {deg}\")\n\nfor i,deg in enumerate(degs):    \n    predictors = ['x']\n    models=[]\n    \n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for t,seed in enumerate(range(1,50)):\n        np.random.seed(seed)\n        y_random = np.sin(x) + 0.5*x - 0.05*x**2 + np.random.normal(0,var,len(x))\n        data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n        data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n        data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n\n        predictors = ['x']\n        if deg &gt;= 2:\n            predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n        regressor = LinearRegression(normalize=True)  \n        regressor.fit(data_s[predictors],data_s['y'])\n        y_pred = regressor.predict(data_s[predictors])\n\n        models.append(y_pred)\n    \n    models=np.array(models)\n    ax[1][i].errorbar(x[::7], models.mean(axis=0)[::7], yerr=2*models.std(axis=0)[::7], fmt=':k', capsize=3, label='Variance')\n    ax[1][i].plot(data['x'], y_true,'g-', label='True Function')\n\nhandles, labels = [(a + b) for a, b in zip(ax[0][0].get_legend_handles_labels(), ax[1][0].get_legend_handles_labels())]\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bv-2.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "ensemble/binomial.html",
    "href": "ensemble/binomial.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nSPINE_COLOR = 'gray'\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\ndef bino(n, p, k)\n\n\n choose(n, k) * p**k * (1-p)**(n-k)\n\n\nstats.binom.pmf(n=21, p=0.3, k=4)\n\n0.11277578372328753\n\n\n\nfrom scipy import stats\na=range(21)\nfor error in [0.3, 0.6]:\n    fig, ax = plt.subplots(figsize=(4,3))\n    ax.bar(a,stats.binom.pmf(n=21, p=error, k=a), color='grey', alpha=0.3)\n    ax.bar(a[11:],stats.binom.pmf(n=21, p=error, k=a[11:]), color='grey', alpha=0.9)\n\n\n    ax.set_ylabel(r'$P(X=k)$')\n    ax.set_xlabel(r'$k$')\n    #ax.set_ylim((0,0.4))\n    #ax.legend()\n    format_axes(ax)\n    #plt.fill_betweenx(3, 11, 20)\n    plt.axvline(10.5, color='k',lw=2, label=r'$k=11, \\epsilon={}$'.format(error))\n    plt.title(\"Probability that majority vote \\n (11 out of 21) is wrong = {}\".format(sum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(3)))\n    plt.legend()\n    import tikzplotlib\n\n    tikzplotlib.save(\"test-{}.tex\".format(error), )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!cat test.tex\n\n% This file was created by tikzplotlib v0.9.0.\n\\begin{tikzpicture}\n\n\\definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}\n\n\\begin{axis}[\ntick align=outside,\ntick pos=left,\ntitle={Simple plot \\(\\displaystyle \\frac{\\alpha}{2}\\)},\nx grid style={white!69.0196078431373!black},\nxlabel={time (s)},\nxmin=-0.75, xmax=15.75,\nxtick style={color=black},\ny grid style={white!69.0196078431373!black},\nylabel={Voltage (mV)},\nymin=0, ymax=0.24892,\nytick style={color=black}\n]\n\\draw[draw=none,fill=color0] (axis cs:0,0) rectangle (axis cs:1.5,0.00293333333333333);\n\\draw[draw=none,fill=color0] (axis cs:1.5,0) rectangle (axis cs:3,0.0157333333333333);\n\\draw[draw=none,fill=color0] (axis cs:3,0) rectangle (axis cs:4.5,0.110866666666667);\n\\draw[draw=none,fill=color0] (axis cs:4.5,0) rectangle (axis cs:6,0.116866666666667);\n\\draw[draw=none,fill=color0] (axis cs:6,0) rectangle (axis cs:7.5,0.237066666666667);\n\\draw[draw=none,fill=color0] (axis cs:7.5,0) rectangle (axis cs:9,0.0889333333333333);\n\\draw[draw=none,fill=color0] (axis cs:9,0) rectangle (axis cs:10.5,0.0760666666666667);\n\\draw[draw=none,fill=color0] (axis cs:10.5,0) rectangle (axis cs:12,0.012);\n\\draw[draw=none,fill=color0] (axis cs:12,0) rectangle (axis cs:13.5,0.00593333333333333);\n\\draw[draw=none,fill=color0] (axis cs:13.5,0) rectangle (axis cs:15,0.000266666666666667);\n\\end{axis}\n\n\\end{tikzpicture}\n\n\n\nsum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(2)\n\n0.83"
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "1-introduction-ml.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nCNN.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nLogistic-vectorisation.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nMovieRecommendation.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nSGD.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\naccuracy_convention.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nautograd.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nbias-variance.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nconvexity.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\ncross-validation.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\ndecision-trees.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nensemble.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nfind-widths.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\ngradient-descent.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nguest-lecture.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nlasso-regression.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nlinear-regression.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nlogistic-1.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nmisc.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nml-maths-2-contour.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nml-maths.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nmlp.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nmock-quiz-2-solution.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nmock-quiz-2.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nmock-quiz.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nnext-token.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\nridge-regression.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\n\n\n\n\n\ntime_complexity.pdf\n\n\n3/13/24, 5:00:26 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cnn/vgg-minst.html",
    "href": "cnn/vgg-minst.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nmodel = VGG16()\n\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 362s 1us/step\n\n\n\nmodel.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n\n\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n\n\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "cnn/convolution-operation.html",
    "href": "cnn/convolution-operation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\n\n\n\n\nfrom scipy.signal import convolve2d\n\n\nconvolve2d(inp, filter_conv, mode='valid')\n\narray([[ 2, -3, -4],\n       [ 4,  8, -9],\n       [ 0, 14, -1]])\n\n\n\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt; from scipy.misc import lena as lena\n\n&gt;&gt;&gt; scharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n...                    [-10+0j, 0+ 0j, +10 +0j],\n...                    [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy\n&gt;&gt;&gt; grad = signal.convolve2d(lena, scharr, boundary='symm', mode='same')\n\nImportError: cannot import name 'lena' from 'scipy.misc' (/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/scipy/misc/__init__.py)\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 1\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.array([[ 1,0, -1], [1, 0,-1], [ 1,0, -1]])\nf = kernel.shape[0]\n\npadding = True\n\nif padding:\n    # visualization array (2 bigger in each direction)\n    va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n    va[p:-p,p:-p] = a\n    va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n    va_color[p:-p,p:-p] = 0.5\nelse:\n    va = a\n    va_color = np.zeros_like(a)\n\n#output array\nres = np.zeros((n-f+1+2*p, n-f+1+2*p))\n\n\n\n#####################\n# Create inital plot\n#####################\nfig = plt.figure(figsize=(8,4))\n\ndef add_axes_inches(fig, rect):\n    w,h = fig.get_size_inches()\n    return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\naxwidth = 3.\ncellsize = axwidth/va.shape[1]\naxheight = cellsize*va.shape[0]\n\nax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\nax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                   (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                   kernel.shape[1]*cellsize,  \n                                   kernel.shape[0]*cellsize])\nax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                               2*cellsize, \n                               res.shape[1]*cellsize,  \n                               res.shape[0]*cellsize])\nax_kernel.set_title(\"Kernel\", size=12)\n\nim_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\nax_va.set_title(\"Image size: {}X{}\\n Padding: {}\".format(n, n, p))\nfor i in range(va.shape[0]):\n    for j in range(va.shape[1]):\n        ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\nax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\nfor i in range(kernel.shape[0]):\n    for j in range(kernel.shape[1]):\n        ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\nim_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\nres_texts = []\nfor i in range(res.shape[0]):\n    row = []\n    for j in range(res.shape[1]):\n        row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n    res_texts.append(row)    \n\nax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\nfor ax  in [ax_va, ax_kernel, ax_res]:\n    ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.grid(color=\"k\")\n\n###############\n# Animation\n###############\ndef init():\n    for row in res_texts:\n        for text in row:\n            text.set_text(\"\")\n\ndef animate(ij):\n    i,j=ij\n    o = kernel.shape[1]//2\n    # calculate result\n    \n   \n    res_ij = (kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]).sum()\n    \n    res_texts[i][j].set_text(res_ij)\n    # make colors\n    c = va_color.copy()\n    c[1+i-o:1+i+o+1, 1+j-o:1+j+o+1] = 1.\n    im_va.set_array(c)\n\n    r = res.copy()\n    r[i,j] = 1\n    im_res.set_array(r)\n    \n\n\ni,j = np.indices(res.shape)\nani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                         frames=zip(i.flat, j.flat), interval=5)\nani.save(\"algo.gif\", writer=\"imagemagick\")\n\n\n\n\n\n\n\n\n\nva\n\narray([[0, 2, 2, 2, 0],\n       [4, 3, 0, 2, 2],\n       [1, 3, 3, 4, 2],\n       [3, 0, 0, 0, 2],\n       [0, 3, 4, 2, 3]])\n\n\n\ni\n\narray([[0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4]])\n\n\n\ni = 0\nj = 3\no =kernel.shape[1]//2\n(kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1])\n\nValueError: operands could not be broadcast together with shapes (3,3) (3,2) \n\n\n\n(kernel)\n\narray([[ 1,  0, -1],\n       [ 1,  0, -1],\n       [ 1,  0, -1]])\n\n\n\nva[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]\n\narray([[3, 2],\n       [2, 1],\n       [3, 0]])"
  },
  {
    "objectID": "cnn/visualisation.html",
    "href": "cnn/visualisation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n%matplotlib inline\n\n\nsomeX, someY = 0.5, 0.5\nfig,ax = plt.subplots()\nax.set_aspect(\"equal\")\nax.add_patch(patches.Rectangle((0.5, 0.5), 0.1, 0.1,\n                      alpha=1, facecolor='gray'))\n\n\n\n\n\n\n\n\n\nimport sys\nsys.path.append(\"convnet-drawer/\")\n\n\nfrom convnet_drawer import Model, Conv2D, MaxPooling2D, Flatten, Dense\nfrom matplotlib_util import save_model_to_file\nchannel_scale = 1/5\n\nmodel = Model(input_shape=(32, 32, 1))\nmodel.add(Conv2D(6, (3, 3), (1, 1)))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(256, (5, 5), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(256, (3, 3), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(4096))\nmodel.add(Dense(4096))\nmodel.add(Dense(1000))\n\n# save as svg file\nmodel.save_fig(\"example.svg\")\n\n\n\n# save via matplotlib\nsave_model_to_file(model, \"example.pdf\")\n\n\n\n\n\n\n\n\nmodel\n\n##### from mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef cuboid_data(o, size=(1,1,1)):\n    # code taken from\n    # https://stackoverflow.com/a/35978146/4124317\n    # suppose axis direction: x: to left; y: to inside; z: to upper\n    # get the length, width, and height\n    l, w, h = size\n    x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  \n    y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1], o[1], o[1]],          \n         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]   \n    z = [[o[2], o[2], o[2], o[2], o[2]],                       \n         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],   \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]],               \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]]]               \n    return np.array(x), np.array(y), np.array(z)\n\ndef plotCubeAt(pos=(0,0,0), size=(1,1,1), ax=None,**kwargs):\n    # Plotting a cube element at position pos\n    if ax !=None:\n        X, Y, Z = cuboid_data( pos, size )\n        ax.plot_surface(X, Y, Z, rstride=1, cstride=1, **kwargs)\n\nsizes = [(32,32,1), (28, 28, 6), (14, 14, 6), (10, 10, 16), (5, 5, 16), (1, 120, 1)]\npositions = [(0, 0, 0)]*len(sizes)\nfor i in range(1, len(sizes)):\n    positions[i] = (positions[i-1][0] + sizes[i-1][0]+10, 0, 0)\ncolors = [\"grey\"]*len(sizes)\n\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.view_init(84, -90)\nax.set_aspect('equal')\nax.set_axis_off()\nax.set_xlabel('X')\nax.set_xlim(-5, positions[-1][0]+10)\nax.set_ylabel('Y')\nax.set_ylim(-1, 130)\nax.set_zlabel('Z')\nax.set_zlim(-1, 5)\n#ax.set_visible(False)\nfor p,s,c in zip(positions,sizes,colors):\n    plotCubeAt(pos=p, size=s, ax=ax, color=c)\nax.w_zaxis.line.set_lw(0.)\nax.set_zticks([])\n\nfor i in range(len(positions)):\n    ax.text(positions[i][0], -5, 0, \"X\".join(str(x) for x in sizes[i]), color='black', fontsize=4)\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1)\nfig.tight_layout()\nplt.tight_layout()\nplt.savefig(\"lenet.pdf\", bbox_inches=\"tight\", transparent=True, dpi=600)"
  },
  {
    "objectID": "knn/knn/knn.html",
    "href": "knn/knn/knn.html",
    "title": "IRIS Dataset",
    "section": "",
    "text": "# https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html\n\nimport numpy as np\nimport pylab as plt\nfrom sklearn import neighbors, datasets\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import train_test_split\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "knn/knn/knn.html#random-blobs",
    "href": "knn/knn/knn.html#random-blobs",
    "title": "IRIS Dataset",
    "section": "Random Blobs",
    "text": "Random Blobs\n\nX, Y = datasets.make_blobs(n_samples=500, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('big.pdf'.format(K, B), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .02 # step size in the mesh\nknn=neighbors.KNeighborsClassifier()\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.show()\n\n\n\n\n\n\n\n\n\nK = 1000\nB = 2\n\nh = .1 # step size in the mesh\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = np.array([probDist(temp, K, B) for temp in np.c_[xx.ravel(), yy.ravel()]])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.contourf(xx, yy, Z, 20, cmap='viridis')\nplt.colorbar();\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n\n\n\n\n\n\n\n\nExample 1\n\nX = np.array([[1,5],[2,5],[3,5],[4,5],[1,3],[2,3],[3,3],[4,3],[2.5,1]])\nY = np.array([0,0,0,0,1,1,1,1,0])\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.savefig('exp.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nh = .1 # step size in the mesh\nfor K in [ 1, 3, 9]:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=K)\n    knn.fit(X, Y)\n    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.set_cmap(plt.cm.Paired)\n    plt.pcolormesh(xx, yy, Z)\n\n    # Plot also the training points\n    plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    plt.savefig('exp_knn_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n    #plt.show()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\nExample 2\n\n# X, Y = datasets.make_blobs(n_samples=50, centers=3, n_features=2, random_state=12, cluster_std=2)\n\n# plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n# plt.xlabel('X')\n# plt.ylabel('Y')\n\n\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrain_err = []\ntest_err = []\nk_list = [t for t in range(1,100)]\nfor k in k_list:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n    Z_train = knn.predict(X_train)\n    Z_test = knn.predict(X_test)\n    \n    train_err.append((Z_train != Y_train).sum()/Y_train.size)\n    test_err.append((Z_test != Y_test).sum()/Y_test.size)\n    #print(k,(Z_train != Y_train).sum()/Y_train.size, (Z_test != Y_test).sum()/Y_test.size)\n\n\nplt.plot(k_list, train_err)\nplt.plot(k_list, test_err)"
  },
  {
    "objectID": "knn/knn/knn.html#other-distance-metrics",
    "href": "knn/knn/knn.html#other-distance-metrics",
    "title": "IRIS Dataset",
    "section": "Other Distance Metrics",
    "text": "Other Distance Metrics\n\nX, Y = datasets.make_blobs(n_samples=100, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('iris_knn_data.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K)\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_def_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"manhattan\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_man_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"chebyshev\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_che_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCurse of Dimensionality"
  },
  {
    "objectID": "linear-reg/Linear Regression Notebook.html",
    "href": "linear-reg/Linear Regression Notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "notebooks/bias-variance.html",
    "href": "notebooks/bias-variance.html",
    "title": "Bias Variance Tradeoff",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nfrom latexify import latexify, format_axes\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nimport pandas as pd\nimport ipywidgets as widgets\n\n\nlatexify(columns=2)\n\n\nx_overall = np.linspace(0, 10, 50)\nf_x = 0.2*np.sin(x_overall) + 0.2*np.cos(2*x_overall)+ 0.6*x_overall - 0.05*x_overall**2 - 0.003*x_overall**3\n\neps = np.random.normal(0, 1, 50)\ny_overall = f_x + eps\nplt.plot(x_overall, f_x, label = 'True function')\nplt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\nformat_axes(plt.gca())\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef fit_plot_tree(x, y, depth=1, extra=None):\n    dt = DecisionTreeRegressor(max_depth=depth)\n    dt.fit(x.reshape(-1, 1), y)\n    y_pred = dt.predict(x.reshape(-1, 1))\n    plt.figure()\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    label = r\"$\\hat{f}$\" if not extra else fr\"$\\hat{{f}}_{{{extra}}}$\"\n\n    plt.plot(x, y_pred, label = label, lw=2)\n\n    format_axes(plt.gca())\n    plt.legend()\n    plt.title(f\"Depth = {depth}\")\n    return dt\n\n\nfor i in range(1, 10):\n    fit_plot_tree(x_overall, y_overall, i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef fit_plot_polynomial(x, y, degree=1, extra=None, ax=None):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x.reshape(-1, 1), y)\n    y_pred = model.predict(x.reshape(-1, 1))\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    ax.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    ax.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    label = r\"$\\hat{f}$\" if not extra else fr\"$\\hat{{f}}_{{{extra}}}$\"\n\n    ax.plot(x, y_pred, label = label, lw=2)\n\n    format_axes(ax)\n    ax.legend()\n    ax.set_title(f\"Degree = {degree}\")\n    return model\n\n\nfit_plot_polynomial(x_overall, y_overall, 5)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])PolynomialFeaturesPolynomialFeatures(degree=5)LinearRegressionLinearRegression()\n\n\n\n\n\n\n\n\n\n\ndef plot_degree(degree=1):\n    regs = []\n    fig, axes = plt.subplots(5, 2, figsize=(8, 12), sharex=True, sharey=True)\n\n    for i, ax in enumerate(axes.flatten()):\n        idx = np.random.choice(np.arange(1, 49), 15, replace=False)\n        idx = np.concatenate([[0], idx, [49]])\n        idx.sort()\n        x = x_overall[idx]\n        y = y_overall[idx]\n        regs.append(fit_plot_polynomial(x, y, degree=degree, extra=i, ax=ax))\n        # remove legend\n        #ax.legend().remove()\n        ax.scatter(x_overall[idx], y_overall[idx], s=50, c='b', label='Sample', alpha=0.1)\n        ax.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return regs\n\n\n_ = plot_degree(5)\n\n\n\n\n\n\n\n\n\nregs = {}\nfor i in range(0, 10):\n    regs[i] = plot_degree(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef plot_predictions(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    plt.plot(x_test, y_pred.mean(axis=0), label = r'$\\hat{f}$', lw=2)\n    plt.plot(x_test, f_x, label = r'$f_{true}$', lw=2)\n    plt.plot(x_test, y_pred.T, lw=1, c='k', alpha=0.5)  \n    format_axes(plt.gca())\n    plt.legend()\n\nplot_predictions(regs[1])\n\n\n\n\n\n\n\n\n\ndef plot_bias(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    #plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    plt.plot(x_test, y_pred_mean, label = r'$\\bar{f}$', lw=2)\n    plt.fill_between(x_test, y_pred_mean, f_x, alpha=0.2, color='green', label = 'Bias')\n    plt.legend()\n\nplot_bias(regs[7])\n\n\n\n\n\n\n\n\n\ndef plot_variance(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    #plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    plt.plot(x_test, y_pred_mean, label = r'$\\bar{f}$', lw=2)\n    plt.fill_between(x_test, y_pred_mean - y_pred_var, y_pred_mean + y_pred_var, alpha=0.2, color='red', label = 'Variance')\n    plt.legend()\n\n\nplot_variance(regs[8])\n\n\n\n\n\n\n\n\n\n# Plot bias^2 and variance for different depths as bar plot\n\ndef plot_bias_variance(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    bias = (y_pred_mean - f_x)**2\n    var = y_pred_var\n    return bias.sum(), var.sum()\n\n\nbs = {}\nvs = {}\nfor i in range(1, 8):\n    bs[i], vs[i] = plot_bias_variance(regs[i])\n\n\ndf = pd.DataFrame({'Bias': bs, 'Variance': vs})\n\n\ndf.plot.bar(rot=0)"
  },
  {
    "objectID": "notebooks/Gradient Descent-2d.html",
    "href": "notebooks/Gradient Descent-2d.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib inline\n\n\nimport numpy as np\n\n\n4.1*4.1\n\n16.81\n\n\n\n4.1-0.2*2*4.1\n\n2.46\n\n\n\nx = 4.1\nalpha = 0.2\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    print(\"(\",round(x, 2),\",\" ,round(x*x, 2),\")\")\n\n( 2.46 , 6.05 )\n( 1.48 , 2.18 )\n( 0.89 , 0.78 )\n( 0.53 , 0.28 )\n( 0.32 , 0.1 )\n( 0.19 , 0.04 )\n( 0.11 , 0.01 )\n( 0.07 , 0.0 )\n( 0.04 , 0.0 )\n( 0.02 , 0.0 )\n\n\n\nx = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    st = \"\"\"\n    \n    \\begin{frame}{Iteration %d}\n    \\begin{columns}\n\n\n        \\begin{column}{0.6\\textwidth}\n            \\begin{adjustbox}{max totalsize={\\textwidth},center}\n                \\begin{tikzpicture}\n\n                    \\begin{axis}[\n                        xlabel=$x$,\n                        ylabel=$y$,\n                        xmin=-4.2,\n                        xmax=4.2,\n                        axis x line*=bottom,\n                        axis y line*=left,\n                        xtick align=outside,\n                        ytick align=outside,\n                        legend pos=outer north east\n                        ]\n                        \\addplot[mark=none, gray] {x^2};\\addlegendentry{$y=x^2$}\n                        \\addplot[only marks, mark=*]\n                        coordinates{ % plot 1 data set\n                            (%s,%s)\n                            }; \n\n\n\n                        \\end{axis}\n\n                \\end{tikzpicture}\n            \\end{adjustbox}\n        \\end{column}\n    \\begin{column}{0.5\\textwidth}\n    \\begin{adjustbox}{max totalsize={\\textwidth},center}\n        \\begin{tikzpicture}\n        \\begin{axis}\n        [\n        title={Contour plot, view from top},\n        view={0}{90},\n        xlabel=$x$,\n        ylabel=$y$,\n        axis x line*=bottom,\n        axis y line*=left,\n        xtick align=outside,\n        ytick align=outside,\n        unit vector ratio*=1 1 1,\n        ]\n        \\addplot3[\n        contour gnuplot={number=14,}\n        ]\n        {x^2};\n        \\addplot[only marks, mark=*]\n        coordinates{ % plot 1 data set\n            (%f,%f)\n        }; \n        \\end{axis}\n        \\end{tikzpicture}\n        \\end{adjustbox}\n    \\end{column}\n    \\end{columns}\n\n\n    \\end{frame}\n    \"\"\" %(i, i, i, i, i)\n\nValueError: unsupported format character 'p' (0x70) at index 793\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\n8.2*4.1\n\n33.62\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\nlatexify()\nval = -7.2\n\nplt.scatter([val],func(np.array([val])), color='k')\nax.annotate('Local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='grey', shrink=0.0001))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y, color='grey')\nformat_axes(plt.gca())\nplt.xlabel(\"x\")\nplt.ylabel(\"y=f(x)\")\nplt.savefig(\"../gradient-descent/local-minima.eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nimport sys\nsys.path.append(\"../\")\n\n\nfrom latexify import format_axes, latexify\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = 0.95\niterations = 10\nlatexify()\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5, color='grey')\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/decision-tree-discrete-input-discrete-output.html",
    "href": "notebooks/decision-tree-discrete-input-discrete-output.html",
    "title": "Decision Trees Discrete Input and Discrete Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-discrete-output.csv\", index_col=0)\n\n\ndf\n\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\nNo\n\n\nD2\nSunny\nHot\nHigh\nStrong\nNo\n\n\nD3\nOvercast\nHot\nHigh\nWeak\nYes\n\n\nD4\nRain\nMild\nHigh\nWeak\nYes\n\n\nD5\nRain\nCool\nNormal\nWeak\nYes\n\n\nD6\nRain\nCool\nNormal\nStrong\nNo\n\n\nD7\nOvercast\nCool\nNormal\nStrong\nYes\n\n\nD8\nSunny\nMild\nHigh\nWeak\nNo\n\n\nD9\nSunny\nCool\nNormal\nWeak\nYes\n\n\nD10\nRain\nMild\nNormal\nWeak\nYes\n\n\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\nD12\nOvercast\nMild\nHigh\nStrong\nYes\n\n\nD13\nOvercast\nHot\nNormal\nWeak\nYes\n\n\nD14\nRain\nMild\nHigh\nStrong\nNo\n\n\n\n\n\n\n\n\n\ndef entropy(ser):\n    \"\"\"\n    Calculate entropy for a categorical variable.\n\n    Parameters:\n    - ser: pd.Series of categorical data\n\n    Returns:\n    - Entropy value\n    \"\"\"\n    # Count the occurrences of each unique value in the series\n    value_counts = ser.value_counts()\n\n    # Calculate the probabilities of each unique value\n    probabilities = value_counts / len(ser)\n\n    # Calculate entropy using the formula: H(S) = -p1*log2(p1) - p2*log2(p2) - ...\n    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n\n    return entropy_value\n    \n\n\nentropy(df[\"Play\"])\n\n0.9402859586706311"
  },
  {
    "objectID": "notebooks/object-detection-segmentation.html",
    "href": "notebooks/object-detection-segmentation.html",
    "title": "Object detection using YOLO and segmentation using Segment Anything",
    "section": "",
    "text": "References\n\nhttps://blog.roboflow.com/how-to-use-segment-anything-model-sam/\n\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#config retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load pre-trained Faster R-CNN model\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n_ = model.eval()\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nimg = Image.open('../datasets/images/office.jpg')\n\n\n# Transform the image\nimg_tensor = F.to_tensor(img)\nimg_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n\n\nfig = plt.figure(figsize=(6, 6))\nplt.imshow(img)\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\n# Forward pass through the model\nwith torch.no_grad():\n    prediction = model(img_tensor)\n\n\nclass_names = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n\nthreshold = 0.5\n\n# Filter boxes based on confidence scores\nfiltered_boxes = prediction[0]['boxes'][prediction[0]['scores'] &gt; threshold]\nfiltered_scores = prediction[0]['scores'][prediction[0]['scores'] &gt; threshold]\nfiltered_labels = prediction[0]['labels'][prediction[0]['scores'] &gt; threshold]\n\n# Plot the image with bounding boxes and class names\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(img)\n\n# Add bounding boxes and labels to the plot\nfor i in range(filtered_boxes.size(0)):\n    box = filtered_boxes[i].cpu().numpy()\n    score = filtered_scores[i].item()\n    label = filtered_labels[i].item()\n\n    # Create a Rectangle patch\n    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n\n    # Add the patch to the Axes\n    ax.add_patch(rect)\n\n    # Add class name and score as text\n    class_name = class_names[label]\n    ax.text(box[0], box[1], f'{class_name} ({score:.2f})', color='r', verticalalignment='top')\n\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n%pip install -q roboflow supervision\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\ntry:\n    from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\nexcept ImportError:\n    %pip install git+https://github.com/facebookresearch/segment-anything.git\n    pip install -q roboflow supervision\n    from segment_anything import SamPredictor, sam_model_registry\n\n\n# Place the model weights\nimport os\npth_path = os.path.expanduser('~/.cache/torch/sam.pth')\npth_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n\nif not os.path.exists(pth_path):\n    import urllib.request\n    print(f'Downloading SAM weights to {pth_path}')\n    urllib.request.urlretrieve(pth_url, pth_path)\n\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nMODEL_TYPE = \"vit_h\"\n\nsam = sam_model_registry[MODEL_TYPE](checkpoint=pth_path)\n_ = sam.to(device=DEVICE)\n\n\nimport cv2\nfrom segment_anything import SamAutomaticMaskGenerator\n\nmask_generator = SamAutomaticMaskGenerator(sam)\n\nimage_bgr = cv2.imread('../datasets/images/office.jpg')\nimage_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\nresult = mask_generator.generate(image_rgb)\n\n\nimport supervision as sv\n\nmask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\ndetections = sv.Detections.from_sam(result)\nannotated_image = mask_annotator.annotate(image_bgr, detections)\n\n\n# Blending the annotated image with the original image\nalpha = 0.5  # Adjust the alpha value as needed\nblended_image = cv2.addWeighted(image_bgr, 1 - alpha, annotated_image, alpha, 0)\n\n# Display the original image, annotated image, and the blended result\nfig, ax = plt.subplots(1, 3, figsize=(18, 6))\nax[0].imshow(img_tensor.squeeze(0).permute(1, 2, 0))\nax[0].set_title('Original Image')\nax[1].imshow(annotated_image)\nax[1].set_title('Annotated Image')\nax[2].imshow(blended_image)\nax[2].set_title('Blended Result')\n\nfor a in ax:\n    a.axis('off')"
  },
  {
    "objectID": "notebooks/dt_weighted.html",
    "href": "notebooks/dt_weighted.html",
    "title": "Weighted Decision Trees",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom latexify import latexify, format_axes\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n!which latex\n\n/usr/local/bin/latex\n\n\n\nlatexify(columns=2)\n\n\n# Dummy Data\nx1 = np.array([1, 3, 2, 5, 7, 8])\nx2 = np.array([1.5, 3, 5, 2, 4, 4.5])\ncategory = np.array([0, 1, 1, 1, 0, 0]) # 0 -&gt; - class and 1 -&gt; + class\n\n# Separate data points for each class\nclass_0_x1 = x1[category == 0]\nclass_0_x2 = x2[category == 0]\nclass_1_x1 = x1[category == 1]\nclass_1_x2 = x2[category == 1]\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\n\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig1.pdf\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\n\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig2.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig3.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n\nplt.fill_betweenx(y=np.arange(0, 6, 0.01), x1=0, x2= 4, color='red', alpha=0.3)\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig4.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n\nplt.fill_betweenx(y=np.arange(0, 6, 0.01), x1=4, x2= 10, color='blue', alpha=0.3)\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig5.pdf\")\nplt.show()"
  },
  {
    "objectID": "notebooks/logistic-iris.html",
    "href": "notebooks/logistic-iris.html",
    "title": "Logistic Regression - Iris dataset",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.datasets import load_iris\n\n\nd = load_iris()\nX = d['data'][:, :2]\ny = d['target']\n\n\nd['feature_names']\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\nlatexify()\ncolours = ['blue', 'red', 'green']\nspecies = ['I. setosa', 'I. versicolor', 'I. virginica']\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i] ,\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\n\n\nplt.savefig(\"../figures/logistic-regression/logisitic-iris.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(X, y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nclf.coef_\n\narray([[-55.82562338,  47.29592374],\n       [ 26.96162409, -23.85029157],\n       [ 28.86399931, -23.44563218]])\n\n\n\nX.shape\n\n(150, 2)\n\n\n\ny.shape\n\n(150,)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\n#plt.scatter(X[:, 0], X[:, 1], c=y)\nlatexify()\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i],\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\nplt.savefig(\"../figures/logistic-regression/logisitic-iris-prediction.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/logistic-apple-oranges.html",
    "href": "notebooks/logistic-apple-oranges.html",
    "title": "Logistic Regression - I",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.patches as mpatches\n\n\nx = np.array([0, 0.1, 0.2, 0.3, 0.6, 0.7, 0.9])\n\n\ny = (x&gt;0.4).astype('int')\n\n\ny\n\narray([0, 0, 0, 0, 1, 1, 1])\n\n\n\n\nlatexify()\nplt.scatter(x, np.zeros_like(x), c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n# legend outside the plot\nplt.legend(handles=[yellow_patch, blue_patch], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#plt.legend(handles=[yellow_patch, blue_patch])\n\n\nplt.xlabel('Radius')\nplt.gca().yaxis.set_visible(False) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-orange-tomatoes-original.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nFitting linear model\n\nfrom sklearn.linear_model import LinearRegression\n\n\nlinr_reg = LinearRegression()\n\n\nlinr_reg.fit(x.reshape(-1, 1), y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.axhline(y=1, color='grey', label='P(y=1)')\nplt.axhline(y=0, color='grey')\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\n (0.5-linr_reg.intercept_)/linr_reg.coef_\n\narray([0.44857143])\n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dash = np.append(x, 2.5)\ny_dash = np.append(y, 1)\nlinr_reg.fit(x_dash.reshape(-1, 1), y_dash)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nplt.plot(np.linspace(0, 2.5, 50), linr_reg.predict(np.linspace(0, 2.5, 50).reshape(-1, 1)))\nplt.scatter(x_dash, y_dash, c=y_dash)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision-modified.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(penalty=None, solver='lbfgs')\n\n\nclf.fit(x.reshape(-1,1), y)\n\nLogisticRegression(penalty=None)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(penalty=None) \n\n\n\nclf.coef_\n\narray([[55.99493009]])\n\n\n\n-clf.intercept_[0]/clf.coef_[0]\n\narray([0.4484548])\n\n\n\nclf.intercept_\n\narray([-25.11119514])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\nlatexify()\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nplt.axvline(x = -clf.intercept_[0]/clf.coef_[0],label='Decision Boundary',linestyle='--',color='k',lw=1)\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.legend(handles=[black_patch, yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.axhspan(0,1, xmin=0, xmax=0.49, linestyle='--',color='darkblue',lw=1, alpha=0.2)\nplt.axhspan(0,0.001, xmin=0, xmax=0.49, linestyle='--',color='k',lw=1, )\n\nplt.axhspan(0,1, xmax=1, xmin=0.49, linestyle='--',color='yellow',lw=1, alpha=0.2)\nplt.axhspan(1,1.001,  xmax=1, xmin=0.49, linestyle='--',color='k',lw=1, )\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision-ideal.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\nplt.scatter(x, y, c=y)\nlatexify()\nplt.axvline(-clf.intercept_[0]/clf.coef_[0], lw=2, color='black')\nplt.axhline(0.5, linestyle='--',color='k',lw=3, label='P(y=1) = P(y=0)')\nplt.ylabel(\"P(y=1)\")\nplt.xlabel('Radius')\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nsigmoid_patch = mpatches.Patch(color='steelblue', label='Sigmoid')\nplt.legend(handles=[black_patch, yellow_patch, blue_patch, sigmoid_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nformat_axes(plt.gca())\nplt.title(\"Logistic Regression\")\nplt.savefig(\"../figures/logistic-regression/logistic.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nz_lins = np.linspace(-10, 10, 500)\nplt.plot(z_lins, sigmoid(z_lins), label=r'$\\sigma(z) = \\frac{1}{1+e^{-z}}$')\nformat_axes(plt.gca())\nplt.xlabel('z')\nplt.ylabel(r'$\\sigma(z)$')\n# vline at 0\nplt.axvline(0, color='black', lw=1, linestyle='--')\n# hline at 0.5\nplt.axhline(0.5, color='black', lw=1, linestyle='--')\nplt.savefig(\"../figures/logistic-regression/logistic-function.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\n\n\nformat_axes(plt.gca())\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\sigma(z)$\")\n#plt.savefig(\"../figures/logistic-regression/logistic-function.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/autodiff.html",
    "href": "notebooks/autodiff.html",
    "title": "Autodiff",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.1'\n\n\n\n### Derviatives using numerical differentiation\n\ndef f(x):\n    return 3 * x ** 2 + 2 * x + 1\n\ndef numerical_derivative_single_side(f, x, h=0.001):\n    return (f(x + h) - f(x)) / h\n\ndef numerical_derivative_double_side(f, x, h=0.001):\n    return (f(x + h) - f(x - h)) / (2 * h)\n\nx = torch.tensor(2.0, requires_grad=False)\nprint(f'f\\'(2) = {numerical_derivative_single_side(f, x, 0.00001)}')\nprint(f'f\\'(2) = {numerical_derivative_double_side(f, x, 0.00001)}')\n\nf'(2) = 14.1143798828125\nf'(2) = 14.1143798828125\n\n\n\ndef f(theta_0, theta_1, theta_2, theta_3, theta_4):\n    return theta_0 + 2 * theta_1 + 3 * theta_2 + 4 * theta_3 + 5 * theta_4\n\n\n\ntheta_0 = torch.tensor(1.0, requires_grad=False)\ntheta_1 = torch.tensor(2.0, requires_grad=False)\ntheta_2 = torch.tensor(3.0, requires_grad=False)\ntheta_3 = torch.tensor(4.0, requires_grad=False)\ntheta_4 = torch.tensor(5.0, requires_grad=False)\n\n\ndf_dtheta_0 = numerical_derivative_single_side(lambda theta_0: f(theta_0, theta_1, theta_2, theta_3, theta_4), theta_0, 0.0001)\ndf_dtheta_0\n\ntensor(0.9918)\n\n\n\n## Above method is very expensive and not practical for large number of parameters\n\n\ntheta_0 = torch.tensor(1.0, requires_grad=True)\ntheta_1 = torch.tensor(1.0, requires_grad=True)\ntheta_2 = torch.tensor(2.0, requires_grad=True)\n\nx1 = torch.tensor(1.0)\nx2 = torch.tensor(2.0)\n\nf1 = theta_1*x1\nf2 = theta_2*x2\n\nf3 = f1 + f2\n\nf4 = f3 + theta_0\n\nf5 = f4*-1\n\nf6 = torch.exp(f5)\n\nf7 = 1 + f6\n\nf8 = 1/f7\n\nf9 = torch.log(f8)\n\nL = f9*-1\n\nall_nodes = {\"theta_0\": theta_0, \"theta_1\": theta_1, \"theta_2\": theta_2,  \n             \"f1\": f1, \"f2\": f2, \"f3\": f3, \"f4\": f4, \"f5\": f5, \"f6\": f6, \"f7\": f7, \"f8\": f8, \"f9\": f9, \"L\": L}\n\n# Retain grad for all nodes\nfor node in all_nodes.values():\n    node.retain_grad()\n\n\n# Print out the function evaluation for all nodes along with name of the node\nfor name, node in all_nodes.items():\n    print(f\"{name}: {node.item()}\")\n\ntheta_0: 1.0\ntheta_1: 1.0\ntheta_2: 2.0\nf1: 1.0\nf2: 4.0\nf3: 5.0\nf4: 6.0\nf5: -6.0\nf6: 0.0024787522852420807\nf7: 1.0024787187576294\nf8: 0.9975274205207825\nf9: -0.0024756414350122213\nL: 0.0024756414350122213\n\n\n\nL.backward()\n\n# Print out the gradient for all nodes along with name of the node\nfor name, node in all_nodes.items():\n    print(f\"{name}: {node.grad.item()}\")\n\ntheta_0: -0.00247262348420918\ntheta_1: -0.00247262348420918\ntheta_2: -0.00494524696841836\nf1: -0.00247262348420918\nf2: -0.00247262348420918\nf3: -0.00247262348420918\nf4: -0.00247262348420918\nf5: 0.00247262348420918\nf6: 0.9975274801254272\nf7: 0.9975274801254272\nf8: -1.0024787187576294\nf9: -1.0\nL: 1.0\n\n\n\n(-1/(f7**2))*-1.00247\n\ntensor(0.9975, grad_fn=&lt;MulBackward0&gt;)\n\n\n\ntorch.exp(f5)*0.9975\n\ntensor(0.0025, grad_fn=&lt;MulBackward0&gt;)\n\n\n\n### Micrograd demo: https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py\n\n\n### Example to illustrate accumulation of gradients\n\ntheta = torch.tensor(1.0, requires_grad=True)\n\nx1 = torch.tensor(1.0)\nx2 = torch.tensor(2.0)\n\nL1 = theta*x1\nL2 = theta*x2\n\nL = L1 + L2\nL.backward()\n\n\ntheta.grad\n\ntensor(3.)\n\n\n\nWhy do we need to use torch.no_grad() in the test phase?\n\n\nWhy do we need to zero out the gradients in the training phase after each update?\nmodel = SimpleModel()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Dummy data\ninputs = torch.randn((100, 10))\ntargets = torch.randn((100, 1))\n\n# Training loop\nfor epoch in range(100):\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute the loss\n    loss = criterion(outputs, targets)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update the weights\n    optimizer.step()\n\n    # Print the loss every 10 epochs\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.item()}')"
  },
  {
    "objectID": "notebooks/Gradient Descent.html",
    "href": "notebooks/Gradient Descent.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000) y = x**2 plt.plot(x,y) plt.title(Cost Function)\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/lin-reg-tutorial.html",
    "href": "notebooks/lin-reg-tutorial.html",
    "title": "Some practice problems",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\n# Create some data\nx = np.linspace(0, 1, 100).reshape(-1, 1)\nf_x = 2 * x + 1\neps = np.random.randn(100, 1)*0.5\ny = f_x + eps\n\nplt.scatter(x, y, label='Data')\nplt.plot(x, f_x, 'r', label='True function')\n\n\n\n\n\n\n\n\n\nlr = LinearRegression()\npenalty_matrix = np.diag(np.eye(x.shape[0]))\npenalty_matrix\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nlr.fit(x, y, sample_weight=penalty_matrix)\nlr.coef_, lr.intercept_\nplt.plot(x, lr.predict(x), 'g', label='Fitted model')\nplt.plot(x, f_x, 'r', label='True function')\nplt.scatter(x, y, label='Data')\nplt.legend()\n\n\n\n\n\n\n\n\n\nlr = LinearRegression()\nlr.fit(x, y)\nprint(lr.coef_, lr.intercept_)\nplt.plot(x, lr.predict(x), 'g', label='Fitted model')\nplt.plot(x, f_x, 'r', label='True function')\nplt.scatter(x, y, label='Data')\nplt.legend()\n\n[[1.9804156]] [1.05585787]\n\n\n\n\n\n\n\n\n\n\npenalty_matrix = np.linspace(0, 1, 100)\n\nlr = LinearRegression()\nlr.fit(x, y, sample_weight=penalty_matrix)\nprint(lr.coef_, lr.intercept_)\n\nplt.plot(x, lr.predict(x), 'g', label='Fitted model')\nplt.plot(x, f_x, 'r', label='True function')\nplt.scatter(x, y, label='Data')\nplt.legend()\n\n[[1.94435821]] [1.08001753]\n\n\n\n\n\n\n\n\n\n\nR = np.diag(penalty_matrix)\nprint(R)\n\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.01010101 0.         ... 0.         0.         0.        ]\n [0.         0.         0.02020202 ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.97979798 0.         0.        ]\n [0.         0.         0.         ... 0.         0.98989899 0.        ]\n [0.         0.         0.         ... 0.         0.         1.        ]]\n\n\n\nx_aug = np.hstack([np.ones_like(x), x])\ntheta_hat= np.linalg.inv(x_aug.T @ R @ x_aug) @ x_aug.T @ R @ y\nprint(theta_hat)\n\n[[1.08001753]\n [1.94435821]]\n\n\n\\(\\hat{y} = x^T\\theta\\)\n\nlr = LinearRegression(fit_intercept=False)\nlr.fit(x, y)\nprint(lr.coef_, lr.intercept_)\n\n[[3.55624367]] 0.0\n\n\n\nsum_x_y = np.sum(x * y)\nsum_x_x = np.sum(x * x)\n\nsum_x_y/sum_x_x\n\n3.5562436711266945\n\n\n\\(\\hat{y} = \\theta\\)\n\nlr = LinearRegression(fit_intercept=False)\nlr.fit(np.ones_like(y), y)\nprint(lr.coef_, lr.intercept_)\n\n[[2.04606567]] 0.0\n\n\n\nnp.mean(y)\n\n2.0460656658200334"
  },
  {
    "objectID": "notebooks/basis2.html",
    "href": "notebooks/basis2.html",
    "title": "Basis Expansion in Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nx = np.linspace(-1, 1, 100)\n\n\nfrom sklearn.kernel_approximation import RBFSampler\n\n\nRBFSampler?\n\nInit signature: RBFSampler(*, gamma=1.0, n_components=100, random_state=None)\nDocstring:     \nApproximate a RBF kernel feature map using random Fourier features.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`.\n\nParameters\n----------\ngamma : 'scale' or float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2).\n    If ``gamma='scale'`` is passed then it uses\n    1 / (n_features * X.var()) as value of gamma.\n\n    .. versionadded:: 1.2\n       The option `\"scale\"` was added in 1.2.\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\nNystroem : Approximate a kernel map using a subset of the training data.\nPolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\nSkewedChi2Sampler : Approximate feature map for\n    \"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1, 1]\n&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)\n&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)\n&gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3)\n&gt;&gt;&gt; clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n&gt;&gt;&gt; clf.score(X_features, y)\n1.0\nFile:           ~/miniforge3/lib/python3.9/site-packages/sklearn/kernel_approximation.py\nType:           type\nSubclasses:     \n\n\n\nr= RBFSampler(n_components=5)\n\n\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=0.1)\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=20)\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))"
  },
  {
    "objectID": "notebooks/taylor.html",
    "href": "notebooks/taylor.html",
    "title": "Taylors Series",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nf = lambda x, y: x**2 + y**2\n\ndef f(x, y):\n    return x**2 + y**2\n\nf_dash = torch.func.grad(f, argnums=(0, 1))\n\n\ndef f2(argument):\n    x, y= argument\n    return x**2 + y**2\n\n\ntorch.func.grad(f2)(torch.tensor([1.0, 1.0]))\n\ntensor([2., 2.])\n\n\n\nf_dash\n\n&lt;function __main__.f(x, y)&gt;\n\n\n\nf_dash(torch.tensor(1.0), torch.tensor(1.0))\n\n(tensor(2.), tensor(2.))\n\n\n\nx = torch.tensor(1.0, requires_grad=True)\ny = torch.tensor(1.0, requires_grad=True)\nprint(\"Before backward: \", x.grad, y.grad)\nz = f(x, y)\nz.backward()\nprint(\"After backward: \", x.grad, y.grad)\n\nBefore backward:  None None\nAfter backward:  tensor(2.) tensor(2.)\n\n\n\nf = lambda x: torch.cos(x)\n\n\nx_range = torch.arange(-2*np.pi, 2*np.pi, 0.01)\ny_range = f(x_range)\nplt.plot(x_range, y_range)\n\n\n\n\n\n\n\n\n\ndef nth_order_appx(f, n, x0=0.0, verbose=False):\n    x0 = torch.tensor(x0)\n    derivs = {1:torch.func.grad(f)}\n    vals = {0:f(x0), 1:derivs[1](x0)}\n    if verbose:\n        print(\"f(x0) = {}\".format(vals[0]))\n        print(\"f'(x0) = {}\".format(vals[1]))\n\n    for i in range(2, n+1):\n        derivs[i] = torch.func.grad(derivs[i-1])\n        vals[i] = derivs[i](x0)\n        if verbose:\n            d = \"'\"*i\n            print(\"f{}(x0) = {}\".format(d, vals[i]))\n    \n    def g(x):\n        x_diff = x - x0\n        str_rep = \"f(x) = f(x0) + \"\n        out = vals[0].repeat(x.shape)\n        for i in range(1, n+1):\n            str_rep += f\"{vals[i]} * (x-{x0.item()})^{i} / {i}! + \"\n            out += vals[i] * x_diff**i / torch.math.factorial(i)\n        if verbose:\n            print(\"--\"*40)\n            print(str_rep)\n        return out\n\n    return g\n        \n\n\nf = lambda x: torch.cos(x)\n_ = nth_order_appx(f, 2, 0.0, verbose=True)(x_range)\n\nf(x0) = 1.0\nf'(x0) = -0.0\nf''(x0) = -1.0\n--------------------------------------------------------------------------------\nf(x) = f(x0) + -0.0 * (x-0.0)^1 / 1! + -1.0 * (x-0.0)^2 / 2! + \n\n\n\nplt.plot(x_range, f(x_range), label=\"f(x) = cos(x)\")\nfor i in range(13, 17, 2):\n    plt.plot(x_range, nth_order_appx(f,  i, 0.0)(x_range), label=f\"order {i} appx\")\nplt.ylim(-2, 2)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: torch.cos(x)\nx0 = 3.14\nplt.plot(x_range, f(x_range))\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nfor i in range(1, 11, 2):\n    \n    plt.plot(x_range, nth_order_appx(f,  i, 3.14)(x_range), label=f\"order {i}\")\nplt.ylim(-2, 2)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: x**2 + 2\nplt.plot(x_range, f(x_range))\nx0 = 2.0\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\n\n\n\n\n\n\n\n\n\nplt.plot(x_range, f(x_range), label=\"f(x) = x^2 + 2\", lw=3, alpha=0.5, ls='--')\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\nplt.xlim(1.5, 2.5)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: x**2 + 2\nplt.plot(x_range, f(x_range))\nx0 = 2.0\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\n\n\n\n\n\n\n\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\ndef plot_gd(alpha=0.1, iter=3):\n    x0 = torch.tensor(2.0)\n\n    xi = x0\n    plt.plot(x_range, f(x_range), label=r\"$f(x) = x^2 + 2$\", lw=3, alpha=0.5, ls='--', color='k')\n    for i in range(iter):\n        plt.scatter(xi, f(xi), label=fr'$x_{i}$ = {xi.item():0.2f}', s=100, c=f\"C{i}\")\n        with torch.no_grad():\n            appx = nth_order_appx(f, 1, xi)(x_range)\n        plt.plot(x_range, appx, label=fr\"order 1 appx. at $x=x_{i}$\", c=f\"C{i}\", alpha=0.5)\n        xi = xi - alpha * torch.func.grad(f)(xi)\n\n    plt.xlim(-2.5, 2.5)\n    plt.ylim(0, 8)\n    # legend outside plot\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    format_axes(plt.gca())\n\n\nplot_gd(alpha=0.1, iter=5)\nplt.savefig(\"../figures/mml/gd-lr-0.1.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=0.8, iter=4)\nplt.savefig(\"../figures/mml/gd-lr-0.8.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=1.01, iter=4)\nplt.savefig(\"../figures/mml/gd-lr-1.01.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=0.01, iter=4)\nplt.savefig(\"../figures/mml/gd-lr-0.01.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-real-output.html",
    "href": "notebooks/decision-tree-real-input-real-output.html",
    "title": "Decision Trees [Real I/P Real O/P, Bias vs Variance]",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\nFIG_WIDTH = 5\nFIG_HEIGHT = 4\n\n\n# Create dataset\nx = np.array([1, 2, 3, 4, 5, 6])\ny = np.array([0, 0, 1, 1, 2, 2])\n\n# plot data\nlatexify(columns=2)\nplt.scatter(x, y, color='k')\nformat_axes(plt.gca()) \nplt.savefig(\"../figures/decision-trees/ri-ro-dataset.pdf\")\n\n\n\n\n\n\n\n\n\n# Depth 0 tree\n# Average of all y values\ny_pred = np.mean(y)\n# Plot data\nlatexify(columns=2)\nplt.scatter(x, y, color='C1', label='data')\n# Plot prediction\nplt.plot([0, 7], [y_pred, y_pred], color='k', linestyle='-', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/ri-ro-depth-0.pdf\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef create_DT_Regressor(x, y, depth, filename):\n    dt = DecisionTreeRegressor(max_depth=depth)\n    dt.fit(x.reshape(-1, 1), y)\n\n    # Plot data\n    latexify(columns=2)\n    plt.scatter(x, y, color='C1', label='Data')\n\n    x_test = np.linspace(0, 7, 500)\n    y_test = dt.predict(x_test.reshape(-1, 1))\n    plt.plot(x_test, y_test, color='k', label='Prediction')\n    format_axes(plt.gca())\n    plt.legend()\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n    return dt\n    \n\n\ndt_one = create_DT_Regressor(x, y, 1, \"ri-ro-depth-1\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\ndef create_graph(dt, filename, feature_names=['x']):\n    dot_data = export_graphviz(dt, out_file=None, feature_names=feature_names, filled=True)\n    graph = graphviz.Source(dot_data)\n    graph.format = 'pdf'\n    graph.render(f\"../figures/decision-trees/{filename}\")\n    return graph\n\n\ncreate_graph(dt_one, \"ri-ro-depth-1-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_two = create_DT_Regressor(x, y, 2, \"ri-ro-depth-2\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_two, \"ri-ro-depth-2-sklearn\")\n\n\n\n\n\n\n\n\nSine Dataset\n\n### Sine daatset\nx = np.linspace(0, 2*np.pi, 200)\ny = np.sin(x)\n\nlatexify(columns=2)\nplt.scatter(x, y, color='k', s=1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/sine-dataset.pdf\")\n\n\n\n\n\n\n\n\n\ndt_sine_one = create_DT_Regressor(x, y, 1, \"sine-depth-1\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_sine_one, \"sine-depth-1-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_sine_four = create_DT_Regressor(x, y, 4, \"sine-depth-4\")\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff - Dataset I\n\n### Dataset for showing bias-variance tradeoff\nX = np.array([[1, 1],[2, 1],[3, 1],[5, 1],\n              [6, 1],[7, 1],[1, 2],[2, 2],\n              [6, 2],[7, 2],[1, 4],[7, 4]])\ny = np.array([0, 0, 0, 1, 1, 1, 0, 1, 0, 1 ,0, 1])\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset.pdf\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\ndef create_DT_Classifier(X,y,depth,filename):\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n\n    # Predict in entire 2d space and contour plot\n    x1 = np.linspace(0, 8, 100)\n    x2 = np.linspace(0, 5, 100)\n\n    X1, X2 = np.meshgrid(x1, x2)\n    X_test = np.stack([X1.flatten(), X2.flatten()], axis=1)\n    y_test = dt.predict(X_test)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.contourf(X1, X2, y_test.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n    format_axes(plt.gca())\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n    return dt\n\n\ndt_bias_variance_one = create_DT_Classifier(X, y, 1, \"bias-variance-depth-1\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_bias_variance_one, \"bias-variance-depth-1-sklearn\", feature_names=['x1', 'x2'])\n\n\n\n\n\n\n\n\n\ndt_bias_variance_full_depth = create_DT_Classifier(X, y, None, \"bias-variance-full-depth\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_bias_variance_full_depth, \"bias-variance-full-depth-sklearn\", feature_names=['x1', 'x2'])\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff - Dataset II\n\n# Bias variance dataset 2\n# X is all integers from (1, 1) to (6, 6)\nX = np.array([[i, j] for i in range(1, 7) for j in range(1, 7)])\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\n\n\n\n\n\n\n\n\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 4)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2.pdf\")\n\n\n\n\n\n\n\n\n\n# X_test random uniform frmo (1, 1) to (6, 6) of size 1000\nX_test = np.random.uniform(1, 6, size=(1000, 2))\ny_test = np.zeros(len(X_test), dtype=int)\ny_test[(2 &lt;= X_test[:, 0]) & (X_test[:, 0] &lt;= 5) & (2 &lt;= X_test[:, 1]) & (X_test[:, 1] &lt;= 5)] = 1\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-test.pdf\")\n\n\n\n\n\n\n\n\n\ndef create_DT_Classifier_with_graph(X,y,depth,filename):\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n\n    # Predict in entire 2d space and contour plot\n    x1 = np.linspace(0.5, 6.5, 100)\n    x2 = np.linspace(0.5, 6.5, 100)\n\n    X1, X2 = np.meshgrid(x1, x2)\n    X_contour = np.stack([X1.flatten(), X2.flatten()], axis=1)\n    y_contour = dt.predict(X_contour)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n    format_axes(plt.gca())\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n\n    # Export tree\n    dot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\n    graph = graphviz.Source(dot_data)\n    graph.format = 'pdf'\n    graph.render(f\"../figures/decision-trees/{filename}-sklearn\")\n\n\n#Underfitting\ncreate_DT_Classifier_with_graph(X, y, 2, \"bias-variance-depth-2\")\n\n\n\n\n\n\n\n\n\n#Overfitting\ncreate_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth\")\n\n\n\n\n\n\n\n\n\n#Good Fit\ncreate_DT_Classifier_with_graph(X, y, 4, \"bias-variance-good-fit\")\n\n\n\n\n\n\n\n\nTest Accuracies\n\nfrom sklearn.metrics import accuracy_score\n### Train and test accuracy vs depth\ndepths = np.arange(2, 10)\ntrain_accs = {}\ntest_accs = {}\nfor depth in depths:\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n    train_accs[depth] = accuracy_score(y, dt.predict(X))\n    test_accs[depth] = accuracy_score(y_test, dt.predict(X_test))\n\n\ntrain_accs = pd.Series(train_accs)\ntest_accs = pd.Series(test_accs)\n\n\ntrain_accs\n\n2    0.722222\n3    0.833333\n4    0.944444\n5    0.944444\n6    0.944444\n7    0.944444\n8    0.944444\n9    0.944444\ndtype: float64\n\n\n\n\n\nax = train_accs.plot(label='Train')\ntest_accs.plot(label='Test', ax=ax)\nplt.xlabel(\"Depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.ylim(0, 1.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth.pdf\")\n\n# Highlight area of underfitting (depth &lt; 4) fill with green \nplt.fill_between(depths, 0, 1, where=depths &lt;= 4, color='g', alpha=0.1, label='Underfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-underfitting.pdf\")\n\n\n# Highlight area of overfitting (depth &gt;7 4) fill with red\nplt.fill_between(depths, 0, 1, where=depths &gt;= 7, color='r', alpha=0.1, label='Overfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-overfitting.pdf\")\n\n\n# Highlight good fit area (4 &lt; depth &lt; 7) fill with blue\nplt.fill_between(depths, 0, 1, where=(depths &gt;= 4) & (depths &lt;= 7), color='b', alpha=0.1, label='Good fit')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-good-fit.pdf\")\n\n\n\n\n\n\n\n\n\n# Slight variation of the dataset leads to a completely different tree\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 3)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-2.pdf\")\n\n\n\n\n\n\n\n\n\ncreate_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth-2\")"
  },
  {
    "objectID": "notebooks/logistic-regression-cost.html",
    "href": "notebooks/logistic-regression-cost.html",
    "title": "Logistic Regression - Cost Function",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\n# Matplotlib retina\n%config InlineBackend.figure_format = 'retina'\n\n\nX = np.array([\n    [1],\n    [2],\n    [3],\n    [4],\n    [5],\n    [6]\n])\n\ny = np.array([1, 1, 1, 0, 0, 0])\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nlr = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nlr.fit(X, y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nlr.coef_\n\narray([[-18.33148189]])\n\n\n\nlr.intercept_\n\narray([64.11147504])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(z))\n\n\ntheta_0_li, theta_1_li = np.meshgrid(np.linspace(-10, 10, 200), np.linspace(-10, 10, 200))\n\n\ndef cost_rmse(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    err = np.sum((y-y_hat)**2)\n    return err\n\n\nz = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z[i, j] = cost_rmse(theta_0_li[i, j], theta_1_li[i, j])\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z)\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nplt.colorbar()\nplt.title('RMSE contour plot')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-sse-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z)\nax.set_title('RMSE surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"../figures/logistic-regression/logistic-sse-loss-3d.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n\npd.DataFrame(z).min().min()\n\n9.01794626038055\n\n\n\ndef cost_2(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    \n    err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n    return err\n\n\nz2 = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z2[i, j] = cost_2(theta_0_li[i, j], theta_1_li[i, j])\n\n/tmp/ipykernel_851067/1266618369.py:4: RuntimeWarning: divide by zero encountered in log\n  err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n/tmp/ipykernel_851067/1266618369.py:4: RuntimeWarning: invalid value encountered in multiply\n  err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z2)\nax.set_title('Cross-entropy surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"../figures/logistic-regression/logistic-cross-loss-surface.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z2)\nplt.title('Cross-entropy contour plot')\nplt.colorbar()\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-cross-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\ny.shape, y_bar.shape\n\n((6,), (10000,))\n\n\n\ny = 0\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 0\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"../figures/logistic-regression/logistic-cross-cost-0.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: divide by zero encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: invalid value encountered in multiply\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: invalid value encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n\n\n\n\n\n\n\n\n\n\ny = 1\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 1\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"../figures/logistic-regression/logistic-cross-cost-1.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: divide by zero encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: invalid value encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: invalid value encountered in multiply\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n\n\n\n\n\n\n\n\n\n\nLikelihood\n\nX_with_one = np.hstack((np.ones_like(X), X))\n\n\n\\[\\begin{align*}\nP(y | X, \\theta) &= \\prod_{i=1}^{n} P(y_{i} | x_{i}, \\theta) \\\\ &= \\prod_{i=1}^{n} \\Big\\{\\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{y_{i}}\\Big\\{1 - \\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{1 - y_{i}} \\\\\n\\end{align*}\\]\n\nX_with_one[1]\n\narray([1, 2])\n\n\n\ndef likelihood(theta_0, theta_1):\n    s = 1\n\n    for i in range(len(X)):\n        y_i_hat = sigmoid(-X_with_one[i]@np.array([theta_0, theta_1]))\n        s = s* ((y_i_hat**y[i])*(1-y_i_hat)**(1-y[i]))\n    \n    \n    return s\n\nx_grid_2, y_grid_2 = np.mgrid[-5:100:0.5, -30:10:.1]\n\nli = np.zeros_like(x_grid_2)\nfor i in range(x_grid_2.shape[0]):\n    for j in range(x_grid_2.shape[1]):\n        li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j])\n        \n\n\nplt.contourf(x_grid_2, y_grid_2, li)\n#plt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.colorbar()\nplt.scatter(lr.intercept_[0], lr.coef_[0], s=200, marker='*', color='r', label='MLE')\nplt.title(r\"Likelihood as a function of ($\\theta_0, \\theta_1$)\")\n#plt.gca().set_aspect('equal')\nplt.legend()\nplt.savefig(\"../figures/logistic-regression/logistic-likelihood.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/text_to_image.html",
    "href": "notebooks/text_to_image.html",
    "title": "Using Diffusion to Generate Images from Text",
    "section": "",
    "text": "References\n\nPromptHero guide\n\n\n%pip install --upgrade \\\n  diffusers \\\n  transformers \\\n  safetensors \\\n  sentencepiece \\\n  accelerate \\\n  bitsandbytes \\\n  torch \\\n  huggingface_hub --quiet\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\ntorchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom huggingface_hub import login\n\nlogin()\n\n\n\n\n\nBasic Imports\n\n# Display the images in a grid\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\nimport torch\npipe = DiffusionPipeline.from_pretrained(\n    \"prompthero/openjourney\", \n    torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.vae = vae\npipe = pipe.to(\"cuda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n\n\n\n\n\n\n\n\n\ndef generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions):\n    random_seeds = [random.randint(0, 65000) for _ in range(num_variations)]\n    images = pipe(prompt= num_variations * [prompt],\n              num_inference_steps=num_steps,\n              guidance_scale=prompt_guidance,\n              height = dimensions[0],\n              width = dimensions[1],\n              generator = [torch.Generator('cuda').manual_seed(i) for i in random_seeds]\n             ).images\n    return images\n\n\nimport random\n\n\n# Setting for image generation\nprompt = 'Small happy dog anf owner learning to walk on a rainy day. Colored photography. Leica lens. Hi-res. hd 8k --ar 2:3'\nnum_steps = 150\nnum_variations = 4\nprompt_guidance = 8\ndimensions = (400, 600) # (width, height) tuple\nrandom_seeds = [random.randint(0, 65000) for _ in range(num_variations)]\n\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\n\n\n\n\n\n\n\ndef display_images(images, num_variations, dimensions):\n    fig = plt.figure(figsize=(dimensions[0]/10, dimensions[1]/10))\n    columns = num_variations\n    rows = 1\n    for i in range(1, columns*rows +1):\n        img = images[i-1]\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img)\n        # hide axes\n        plt.axis('off')\n    plt.show()\n    \ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\nprompt = \"Batman, cinematic lighting, dark background, very high resolution 3D render.\"\n\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"A logo for a research group in India called Sustainability lab that works on AI for sustainability. Show AI as the central theme. Show applications in health, air quality\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"Portrait of a small kid with a big smile.\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"A photorealistic render of an academic campus in India, on the edges of a river, low-light, evening.\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html",
    "title": "Adversarial Examples for ML",
    "section": "",
    "text": "The colab environment already has all the necessary Python packages installed. Specifically, we are using numpy, torch and torchvision.\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\n\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Choosing backend\nif torch.backends.mps.is_available():\n    device=torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device=torch.device(\"cuda\")\nelse:\n    device=torch.device(\"cpu\")\n\n\n\nWe load in the data using the in-built data loaders in PyTorch. It offers functionality for many commonly used computer vision datasets, but we will just use MNIST (a dataset of black and white handwritten digits) for now.\n\ndef load_dataset(dataset, data_dir, training_time):\n    if dataset == 'CIFAR-10':\n        loader_train, loader_test, data_details = load_cifar_dataset(data_dir, training_time)\n    elif 'MNIST' in dataset:\n        loader_train, loader_test, data_details = load_mnist_dataset(data_dir, training_time)\n    else:\n        raise ValueError('No support for dataset %s' % args.dataset)\n\n    return loader_train, loader_test, data_details\n\n\ndef load_mnist_dataset(data_dir, training_time):\n    # MNIST data loaders\n    trainset = datasets.MNIST(root=data_dir, train=True,\n                                download=True, transform=transforms.ToTensor())\n    testset = datasets.MNIST(root=data_dir, train=False,\n                                download=True, transform=transforms.ToTensor())\n\n    loader_train = torch.utils.data.DataLoader(trainset,\n                                batch_size=128,\n                                shuffle=True)\n\n    loader_test = torch.utils.data.DataLoader(testset,\n                                batch_size=128,\n                                shuffle=False)\n    data_details = {'n_channels':1, 'h_in':28, 'w_in':28, 'scale':255.0}\n    return loader_train, loader_test, data_details\n\nHaving defined the data loaders, we now create the data loaders to be used throughout, as well as a dictionary with the details of the dataset, in case we need it.\n\nloader_train, loader_test, data_details = load_dataset('MNIST','data',training_time=True)\n\n\n\n\nSince we need the path to the directory where we will storing our models (pre-trained or not), and we also need to instantiate a copy of the model we defined above, we will run the following commands to have everything setup for test/evaluation.\n\n\n\nWe use a 2-layer fully connected network for the experiments in this tutorial. The definition of a 3 layer convolutional neural network is also provided. The former is sufficient for MNIST, but may not be large enough for more complex tasks.\n\nmodel_name='fcn'\n\n\nclass cnn_3l_bn(nn.Module):\n    def __init__(self, n_classes=10):\n        super(cnn_3l_bn, self).__init__()\n        #in-channels, no. filters, filter size, stride\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.bn1 = nn.BatchNorm2d(20)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn2 = nn.BatchNorm2d(50)\n        # Number of neurons in preceding layer, Number in current layer\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, n_classes)\n\n    def forward(self, x):\n        # Rectified linear unit activation\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nclass fcn(nn.Module):\n  def __init__(self, n_classes=10):\n    super(fcn, self).__init__()\n    self.fc1 = nn.Linear(784,200)\n    self.fc2 = nn.Linear(200,200)\n    self.fc3 = nn.Linear(200,n_classes)\n\n  def forward(self, x):\n    x = x.view(-1, 28*28)\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = self.fc3(x)\n    return F.log_softmax(x, dim=1)\n\n\nif 'fcn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/fcn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = fcn(10)\nelif 'cnn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/cnn_3l_bn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = cnn_3l_bn(10)\n\nnet.to(device)\n\ncriterion = nn.CrossEntropyLoss(reduction='none')"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#data-model-and-attack-utilities",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#data-model-and-attack-utilities",
    "title": "Adversarial Examples for ML",
    "section": "",
    "text": "The colab environment already has all the necessary Python packages installed. Specifically, we are using numpy, torch and torchvision.\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\n\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Choosing backend\nif torch.backends.mps.is_available():\n    device=torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device=torch.device(\"cuda\")\nelse:\n    device=torch.device(\"cpu\")\n\n\n\nWe load in the data using the in-built data loaders in PyTorch. It offers functionality for many commonly used computer vision datasets, but we will just use MNIST (a dataset of black and white handwritten digits) for now.\n\ndef load_dataset(dataset, data_dir, training_time):\n    if dataset == 'CIFAR-10':\n        loader_train, loader_test, data_details = load_cifar_dataset(data_dir, training_time)\n    elif 'MNIST' in dataset:\n        loader_train, loader_test, data_details = load_mnist_dataset(data_dir, training_time)\n    else:\n        raise ValueError('No support for dataset %s' % args.dataset)\n\n    return loader_train, loader_test, data_details\n\n\ndef load_mnist_dataset(data_dir, training_time):\n    # MNIST data loaders\n    trainset = datasets.MNIST(root=data_dir, train=True,\n                                download=True, transform=transforms.ToTensor())\n    testset = datasets.MNIST(root=data_dir, train=False,\n                                download=True, transform=transforms.ToTensor())\n\n    loader_train = torch.utils.data.DataLoader(trainset,\n                                batch_size=128,\n                                shuffle=True)\n\n    loader_test = torch.utils.data.DataLoader(testset,\n                                batch_size=128,\n                                shuffle=False)\n    data_details = {'n_channels':1, 'h_in':28, 'w_in':28, 'scale':255.0}\n    return loader_train, loader_test, data_details\n\nHaving defined the data loaders, we now create the data loaders to be used throughout, as well as a dictionary with the details of the dataset, in case we need it.\n\nloader_train, loader_test, data_details = load_dataset('MNIST','data',training_time=True)\n\n\n\n\nSince we need the path to the directory where we will storing our models (pre-trained or not), and we also need to instantiate a copy of the model we defined above, we will run the following commands to have everything setup for test/evaluation.\n\n\n\nWe use a 2-layer fully connected network for the experiments in this tutorial. The definition of a 3 layer convolutional neural network is also provided. The former is sufficient for MNIST, but may not be large enough for more complex tasks.\n\nmodel_name='fcn'\n\n\nclass cnn_3l_bn(nn.Module):\n    def __init__(self, n_classes=10):\n        super(cnn_3l_bn, self).__init__()\n        #in-channels, no. filters, filter size, stride\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.bn1 = nn.BatchNorm2d(20)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn2 = nn.BatchNorm2d(50)\n        # Number of neurons in preceding layer, Number in current layer\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, n_classes)\n\n    def forward(self, x):\n        # Rectified linear unit activation\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nclass fcn(nn.Module):\n  def __init__(self, n_classes=10):\n    super(fcn, self).__init__()\n    self.fc1 = nn.Linear(784,200)\n    self.fc2 = nn.Linear(200,200)\n    self.fc3 = nn.Linear(200,n_classes)\n\n  def forward(self, x):\n    x = x.view(-1, 28*28)\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = self.fc3(x)\n    return F.log_softmax(x, dim=1)\n\n\nif 'fcn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/fcn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = fcn(10)\nelif 'cnn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/cnn_3l_bn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = cnn_3l_bn(10)\n\nnet.to(device)\n\ncriterion = nn.CrossEntropyLoss(reduction='none')"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-the-benignstandard-model",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-the-benignstandard-model",
    "title": "Adversarial Examples for ML",
    "section": "Training the benign/standard model",
    "text": "Training the benign/standard model\nThis is sample code for training your own model. Since it takes time to run, for the purposes of the tutorial, we will assume we already have trained models.\n\n########################################  Benign/standard training ########################################\ndef train_one_epoch(model, optimizer, loader_train, verbose=True):\n    losses = []\n    model.train()\n    for t, (x, y) in enumerate(loader_train):\n        x.to(device)\n        y.to(device)\n        x_var = Variable(x, requires_grad= True).to(device)\n        y_var = Variable(y, requires_grad= False).to(device)\n        scores = model(x_var)\n        # loss = loss_fn(scores, y_var)\n        loss_function = nn.CrossEntropyLoss(reduction='none')\n        batch_loss = loss_function(scores, y_var)\n        loss = torch.mean(batch_loss)\n        losses.append(loss.data.cpu().numpy())\n        optimizer.zero_grad()\n        loss.backward()\n        # print(model.conv1.weight.grad)\n        optimizer.step()\n    if verbose:\n        print('loss = %.8f' % (loss.data))\n    return np.mean(losses)\n\n\nActual training loop\nWe define the necessary parameters for training (batch size, learning rate etc.), instantiate the optimizer and then train for 50 epochs.\nIn each epoch, the model is trained using all of the training data, which is split into batches of size 128. Thus, one step of the optimizer uses 128 samples, and there are a total of 50*(50,000/128) steps in the entire process.\n\n# Training parameters\nbatch_size=128\nlearning_rate=0.1 #\nweight_decay=2e-4\nsave_checkpoint=True\n\n# Torch optimizer\noptimizer = torch.optim.SGD(net.parameters(),\n                            lr=learning_rate,\n                            momentum=0.9,\n                            weight_decay=weight_decay)\n\n# if args.lr_schedule == 'cosine':\n#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n#             T_max=args.train_epochs, eta_min=0, last_epoch=-1)\n# elif args.lr_schedule == 'linear0':\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150,200], gamma=0.1)\n\n\nfor epoch in range(0, 10):\n    start_time = time.time()\n    # lr = update_hyparam(epoch, args)\n    lr = optimizer.param_groups[0]['lr']\n    print('Current learning rate: {}'.format(lr))\n    # if not args.is_adv:\n    ben_loss = train_one_epoch(net, optimizer,\n                          loader_train, verbose=False)\n    print('time_taken for #{} epoch = {:.3f}'.format(epoch+1, time.time()-start_time))\n    if save_checkpoint:\n        ckpt_path = 'checkpoint_' + str(0)\n        torch.save(net.state_dict(), model_dir_name + ckpt_path)\n    print('Train loss - Ben: %s' %\n        (ben_loss))\n    scheduler.step()"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#generating-adversarial-examples",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#generating-adversarial-examples",
    "title": "Adversarial Examples for ML",
    "section": "Generating Adversarial Examples",
    "text": "Generating Adversarial Examples\nWe will look at how to generate adversarial examples using the Projected Gradient Descent (PGD) method for the model we have trained and visualize the adversarial examples thus generated.\n\n# Attack utils\n\n# Random initialization within the L2 ball\ndef rand_init_l2(img_variable, eps_max):\n    random_vec = torch.FloatTensor(*img_variable.shape).normal_(0, 1).to(device)\n    random_vec_norm = torch.max(\n               random_vec.view(random_vec.size(0), -1).norm(2, 1), torch.tensor(1e-9).to(device))\n    random_dir = random_vec/random_vec_norm.view(random_vec.size(0),1,1,1)\n    random_scale = torch.FloatTensor(img_variable.size(0)).uniform_(0, eps_max).to(device)\n    random_noise = random_scale.view(random_vec.size(0),1,1,1)*random_dir\n    img_variable = Variable(img_variable.data + random_noise, requires_grad=True).to(device)\n\n    return img_variable\n\n# Random initialization within the L_inf ball\ndef rand_init_linf(img_variable, eps_max):\n    random_noise = torch.FloatTensor(*img_variable.shape).uniform_(-eps_max, eps_max).to(device)\n    img_variable = Variable(img_variable.data + random_noise, requires_grad=True).to(device)\n\n    return img_variable\n\n# Tracking the best adversarial examples during the generation process\ndef track_best(blosses, b_adv_x, curr_losses, curr_adv_x):\n    if blosses is None:\n        b_adv_x = curr_adv_x.clone().detach()\n        blosses = curr_losses.clone().detach()\n    else:\n        replace = curr_losses &lt; blosses\n        b_adv_x[replace] = curr_adv_x[replace].clone().detach()\n        blosses[replace] = curr_losses[replace]\n\n    return blosses.to(device), b_adv_x.to(device)\n\n# Loss calculation\ndef cal_loss(y_out, y_true, targeted):\n    losses = torch.nn.CrossEntropyLoss(reduction='none')\n    losses_cal = losses(y_out, y_true).to(device)\n    loss_cal = torch.mean(losses_cal).to(device)\n    if targeted:\n        return loss_cal, losses_cal\n    else:\n        return -1*loss_cal, -1*losses_cal\n\n# Generating targets for each adversarial example\ndef generate_target_label_tensor(true_label, n_classes):\n    t = torch.floor(n_classes*torch.rand(true_label.shape)).type(torch.int64)\n    m = t == true_label\n    t[m] = (t[m]+ torch.ceil((n_classes-1)*torch.rand(t[m].shape)).type(torch.int64)) % n_classes\n    return t.to(device)\n\nThis provides the core loop of the attack algorithm which goes as follows: 1. The perturbation is initialized to 0. 2. The gradient of the model with respect to the current state of the adversarial example is found. 3. The gradient is appropriately normalized and added to the current state of the example 4. The complete adversarial example is clipped to lie within the input bounds 5. Steps 2,3 and 4 are repeated for a fixed number of steps or until some condition is met\n\n# Attack code\ndef pgd_attack(model, image_tensor, img_variable, tar_label_variable,\n               n_steps, eps_max, eps_step, clip_min, clip_max, targeted, rand_init):\n    \"\"\"\n    image_tensor: tensor which holds the clean images.\n    img_variable: Corresponding pytorch variable for image_tensor.\n    tar_label_variable: Assuming targeted attack, this variable holds the targeted labels.\n    n_steps: number of attack iterations.\n    eps_max: maximum l_inf attack perturbations.\n    eps_step: l_inf attack perturbation per step\n    \"\"\"\n\n    best_losses = None\n    best_adv_x = None\n    image_tensor = image_tensor.to(device)\n\n    if rand_init:\n        img_variable = rand_init_linf(img_variable, eps_max)\n\n    output = model.forward(img_variable)\n    for i in range(n_steps):\n        if img_variable.grad is not None:\n            img_variable.grad.zero_()\n        output = model.forward(img_variable)\n        loss_cal, losses_cal = cal_loss(output, tar_label_variable, targeted)\n        best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n\n        loss_cal, losses_cal = cal_loss(output, tar_label_variable, targeted)\n        loss_cal.backward()\n        # Finding the gradient of the loss\n        x_grad = -1 * eps_step * torch.sign(img_variable.grad.data)\n        # Adding gradient to current state of the example\n        adv_temp = img_variable.data + x_grad\n        total_grad = adv_temp - image_tensor\n        total_grad = torch.clamp(total_grad, -eps_max, eps_max)\n        x_adv = image_tensor + total_grad\n        # Projecting adversarial example back onto the constraint set\n        x_adv = torch.clamp(torch.clamp(\n            x_adv-image_tensor, -1*eps_max, eps_max)+image_tensor, clip_min, clip_max)\n        img_variable.data = x_adv\n\n    best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n\n    return best_adv_x\n\ndef pgd_l2_attack(model, image_tensor, img_variable, tar_label_variable,\n               n_steps, eps_max, eps_step, clip_min, clip_max, targeted,\n               rand_init, num_restarts):\n    \"\"\"\n    image_tensor: tensor which holds the clean images.\n    img_variable: Corresponding pytorch variable for image_tensor.\n    tar_label_variable: Assuming targeted attack, this variable holds the targeted labels.\n    n_steps: number of attack iterations.\n    eps_max: maximum l_inf attack perturbations.\n    eps_step: l_inf attack perturbation per step\n    \"\"\"\n\n    best_losses = None\n    best_adv_x = None\n    image_tensor_orig = image_tensor.clone().detach()\n    tar_label_orig = tar_label_variable.clone().detach()\n\n    for j in range(num_restarts):\n        if rand_init:\n            img_variable = rand_init_l2(img_variable, eps_max)\n\n        output = model.forward(img_variable)\n        for i in range(n_steps):\n            if img_variable.grad is not None:\n                img_variable.grad.zero_()\n            output = model.forward(img_variable)\n            loss_cal, losses_cal = cal_loss(output, tar_label_variable, targeted)\n            best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n            loss_cal.backward()\n            raw_grad = img_variable.grad.data\n            grad_norm = torch.max(\n                   raw_grad.view(raw_grad.size(0), -1).norm(2, 1), torch.tensor(1e-9))\n            grad_dir = raw_grad/grad_norm.view(raw_grad.size(0),1,1,1)\n            adv_temp = img_variable.data +  -1 * eps_step * grad_dir\n            # Clipping total perturbation\n            total_grad = adv_temp - image_tensor\n            total_grad_norm = torch.max(\n                   total_grad.view(total_grad.size(0), -1).norm(2, 1), torch.tensor(1e-9))\n            total_grad_dir = total_grad/total_grad_norm.view(total_grad.size(0),1,1,1)\n            total_grad_norm_rescale = torch.min(total_grad_norm, torch.tensor(eps_max))\n            clipped_grad = total_grad_norm_rescale.view(total_grad.size(0),1,1,1) * total_grad_dir\n            x_adv = image_tensor + clipped_grad\n            x_adv = torch.clamp(x_adv, clip_min, clip_max)\n            img_variable.data = x_adv\n\n        best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n\n        diff_array = np.array(x_adv.cpu())-np.array(image_tensor.data.cpu())\n        diff_array = diff_array.reshape(len(diff_array),-1)\n\n        img_variable.data = image_tensor_orig\n\n    return best_adv_x\n\nNow we can call the core adversarial example generation function over our data and model to determine how robust the model actually is!\n\ndef robust_test(model, loss_fn, loader, att_dir, n_batches=0, train_data=False,\n                training_time=False):\n    \"\"\"\n    n_batches (int): Number of batches for evaluation.\n    \"\"\"\n    model.eval()\n    num_correct, num_correct_adv, num_samples = 0, 0, 0\n    steps = 1\n    losses_adv = []\n    losses_ben = []\n    adv_images = []\n    adv_labels = []\n    clean_images = []\n    correct_labels = []\n\n    for t, (x, y) in enumerate(loader):\n        x=x.to(device)\n        y=y.to(device)\n        x_var = Variable(x, requires_grad= True).to(device)\n        y_var = Variable(y, requires_grad=False).to(device)\n        if att_dir['targeted']:\n            y_target = generate_target_label_tensor(\n                               y_var.cpu(), 10).to(device)\n        else:\n            y_target = y_var\n        if 'PGD_linf' in att_dir['attack']:\n            adv_x = pgd_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'])\n        elif 'PGD_l2' in att_dir['attack']:\n            adv_x = pgd_l2_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'],\n                           att_dir['num_restarts'])\n        # Predictions\n        # scores = model(x.cuda())\n        scores = model(x)\n        _, preds = scores.data.max(1)\n        scores_adv = model(adv_x)\n        _, preds_adv = scores_adv.data.max(1)\n        # Losses\n        batch_loss_adv = loss_fn(scores_adv, y)\n        loss_adv = torch.mean(batch_loss_adv)\n        losses_adv.append(loss_adv.data.cpu().numpy())\n        batch_loss_ben = loss_fn(scores, y)\n        loss_ben = torch.mean(batch_loss_ben)\n        losses_ben.append(loss_ben.data.cpu().numpy())\n        # Correct count\n        num_correct += (preds == y).sum()\n        num_correct_adv += (preds_adv == y).sum()\n        num_samples += len(preds)\n        # Adding images and labels to list\n        adv_images.extend(adv_x)\n        adv_labels.extend(preds_adv)\n        clean_images.extend(x)\n        correct_labels.extend(preds)\n\n        if n_batches &gt; 0 and steps==n_batches:\n            break\n        steps += 1\n\n    acc = float(num_correct) / num_samples\n    acc_adv = float(num_correct_adv) / num_samples\n    print('Clean accuracy: {:.2f}% ({}/{})'.format(\n        100.*acc,\n        num_correct,\n        num_samples,\n    ))\n    print('Adversarial accuracy: {:.2f}% ({}/{})'.format(\n        100.*acc_adv,\n        num_correct_adv,\n        num_samples,\n    ))\n\n    return 100.*acc, 100.*acc_adv, np.mean(losses_ben), np.mean(losses_adv), adv_images, adv_labels, clean_images, correct_labels\n\nThe most important parameters below are epsilon (which controls the magnitude of the perturbation), gamma (which determines how far outside the constraint set intial search is allowed) and attack_iter (which is just the number of attack iterations).\n\n# Attack setup\ngamma=2.5\nepsilon=0.2\nattack_iter=10\ndelta=epsilon*gamma/attack_iter\nattack_params = {'attack': 'PGD_linf', 'epsilon': epsilon,\n              'attack_iter': 10, 'eps_step': delta,\n              'targeted': True, 'clip_min': 0.0,\n              'clip_max': 1.0,'rand_init': True,\n              'num_restarts': 1}\n\nNow, we load the model (remember to first upload it into the models folder!) and then generate adversarial examples.\n\nckpt_path = 'checkpoint_' + str(0)\nnet.to(device)\nnet.eval()\nnet.load_state_dict(torch.load(model_dir_name + ckpt_path, map_location=device))\nn_batches_eval = 10\nprint('Test set validation')\n# Running validation\nacc_test, acc_adv_test, test_loss, test_loss_adv, adv_images, adv_labels, clean_images, correct_labels = robust_test(net,\n    criterion, loader_test, attack_params, n_batches=n_batches_eval,\n    train_data=False, training_time=True)\n# print('Training set validation')\n# acc_train, acc_adv_train, train_loss, train_loss_adv, _ = robust_test(net,\n#     criterion, loader_train_all, args, attack_params, n_batches=n_batches_eval,\n#     train_data=True, training_time=True)\n\n\nVisualizing the adversarial examples\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfig = plt.figure(figsize=(9, 13))\ncolumns = 4\nrows = 5\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(columns*rows):\n    image_count=int(i/2)\n    if i%2==1:\n      img = adv_images[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(adv_labels[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    else:\n      img = clean_images[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(correct_labels[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    plt.imshow(img, interpolation='nearest',cmap='gray')\n\n\nplt.show()  # finally, render the plot"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-robust-models",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-robust-models",
    "title": "Adversarial Examples for ML",
    "section": "Training robust models",
    "text": "Training robust models\nThis training loop is very similar to the benign one, except that we now call the adversarial example generation function to generate adversarial examples during the training process.\n\n########################################  Adversarial training ########################################\ndef robust_train_one_epoch(model, optimizer, loader_train, att_dir,\n                           epoch):\n    # print('Current eps: {}, delta: {}'.format(eps, delta))\n    losses_adv = []\n    losses_ben = []\n    model.train()\n    for t, (x, y) in enumerate(loader_train):\n        x=x.to(device)\n        y=y.to(device)\n        x_var = Variable(x, requires_grad= True)\n        y_var = Variable(y, requires_grad= False)\n        if att_dir['targeted']:\n            y_target = generate_target_label_tensor(\n                               y_var.cpu(), 10).to(device)\n        else:\n            y_target = y_var\n        if 'PGD_linf' in att_dir['attack']:\n            adv_x = pgd_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'])\n        elif 'PGD_l2' in att_dir['attack']:\n            adv_x = pgd_l2_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'],\n                           att_dir['num_restarts'])\n        scores = model(adv_x)\n        loss_function = nn.CrossEntropyLoss(reduction='none')\n        batch_loss_adv = loss_function(scores, y_var)\n        batch_loss_ben = loss_function(model(x),y_var)\n        loss = torch.mean(batch_loss_adv)\n        loss_ben = torch.mean(batch_loss_ben)\n        losses_ben.append(loss_ben.data.cpu().numpy())\n        losses_adv.append(loss.data.cpu().numpy())\n        # GD step\n        optimizer.zero_grad()\n        loss.backward()\n        # print(model.conv1.weight.grad)\n        optimizer.step()\n    return np.mean(losses_adv), np.mean(losses_ben)\n\n\nfor epoch in range(0, 10):\n    start_time = time.time()\n    # lr = update_hyparam(epoch, args)\n    lr = optimizer.param_groups[0]['lr']\n    print('Current learning rate: {}'.format(lr))\n    curr_loss, ben_loss = robust_train_one_epoch(net,\n                            optimizer, loader_train, attack_params,\n                            epoch)\n    print('time_taken for #{} epoch = {:.3f}'.format(epoch+1, time.time()-start_time))\n    if save_checkpoint:\n        ckpt_path = 'checkpoint_adv' + str(0)\n        torch.save(net.state_dict(), model_dir_name + ckpt_path)\n    print('Train loss - Ben: %s, Adv: %s' %\n        (ben_loss, curr_loss))\n    scheduler.step()"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#evaluating-the-robust-model",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#evaluating-the-robust-model",
    "title": "Adversarial Examples for ML",
    "section": "Evaluating the robust model",
    "text": "Evaluating the robust model\nEvaluating the robust model, we find its accuracy on adversarial examples has increased significantly!\n\nckpt_path = 'checkpoint_adv' + str(0)\nnet.eval()\nnet.load_state_dict(torch.load(model_dir_name + ckpt_path, map_location=device))\nn_batches_eval = 10\nprint('Test set validation')\n# Running validation\nacc_test_r, acc_adv_test_r, test_loss_r, test_loss_adv_r, adv_images_r, adv_labels_r, clean_images_r, correct_labels_r = robust_test(net,\n    criterion, loader_test, attack_params, n_batches=n_batches_eval,\n    train_data=False, training_time=True)\n# print('Training set validation')\n# acc_train, acc_adv_train, train_loss, train_loss_adv, _ = robust_test(net,\n#     criterion, loader_train_all, args, attack_params, n_batches=n_batches_eval,\n#     train_data=True, training_time=True)\n\n\nfig = plt.figure(figsize=(9, 13))\ncolumns = 4\nrows = 5\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(columns*rows):\n    image_count=int(i/2)\n    if i%2==1:\n      img = adv_images_r[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(adv_labels_r[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    else:\n      img = clean_images_r[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(correct_labels_r[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    plt.imshow(img, interpolation='nearest',cmap='gray')\n\n\nplt.show()  # finally, render the plot"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#discussion-questions",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#discussion-questions",
    "title": "Adversarial Examples for ML",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nDoesnt robust training solve the problem of adversarial examples? Why is there still so much research on the topic?\nHow would a real-world attacker try to carry out this attack without access to the classifier being used?\nWhat does the existence of adversarial examples tell us about modern ML models?"
  },
  {
    "objectID": "notebooks/cross-validation-diagrams.html",
    "href": "notebooks/cross-validation-diagrams.html",
    "title": "Cross Validation Diagrams",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nimport matplotlib.pyplot as plt\n\n# Define the train/test split percentages\ntrain_percentage = 0.7\ntest_percentage = 1 - train_percentage\n\n# Create a rectangular plot to represent the train/test split\ntrain_rectangle = plt.Rectangle((0, 0), train_percentage, 1, fill=True, color='lightgreen', label='Train Set')\ntest_rectangle = plt.Rectangle((train_percentage, 0), test_percentage, 1, fill=True, color='lightcoral', label='Test Set')\n\n# Add rectangles to the plot\nplt.gca().add_patch(train_rectangle)\nplt.gca().add_patch(test_rectangle)\n\n# Set labels and legend\nplt.xlabel('Data Split')\nplt.ylabel('Data Points')\nplt.title('Train/Test Split Illustration')\nplt.legend()\n\n# Remove x and y ticks\nplt.xticks([])\nplt.yticks([])\n\n\n\n\n\n\n\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/logistic-regression-torch.html",
    "href": "notebooks/logistic-regression-torch.html",
    "title": "Logistic Regression Torch",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, 1)\n    \n    def forward(self, x):\n        logits = self.linear(x)\n        return logits\n\n\nfrom sklearn.datasets import make_moons\n\n\nX, y = make_moons(n_samples=100, noise=0.1)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n\n\n\n\n\n\n\n\n\nlog_reg = LogisticRegression(2)\n\n\n# Predict with the model\n\ndef predict_plot_grid(model):\n    XX, YY = torch.meshgrid(torch.linspace(-2, 3, 100), torch.linspace(-2, 2, 100))\n    X_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim=-1)\n    logits = model(X_grid)\n    probs = torch.sigmoid(logits).reshape(100, 100)\n    plt.contourf(XX, YY, probs.detach().numpy(), levels=[0.0, 0.1, 0.2,0.3, 0.4,0.5, 0.6,0.7, 0.8,0.9, 1.0], \n                 cmap=plt.cm.Spectral, alpha=0.5)\n    plt.colorbar()\n    \n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    \npredict_plot_grid(log_reg)\n    \n    \n\n/Users/nipun/mambaforge/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n\n\n\n\n\n\nopt = torch.optim.Adam(log_reg.parameters(), lr=0.01)\n\nconverged = False\nprev_loss = 1e8 \ni = 0\nwhile not converged:\n    opt.zero_grad()\n    logits = log_reg(torch.tensor(X, dtype=torch.float32))\n    loss = nn.BCEWithLogitsLoss()(logits, torch.tensor(y, dtype=torch.float32).view(-1, 1))\n    loss.backward()\n    opt.step()\n    if i%10==0:\n        print(i, loss.item())\n    if np.abs(prev_loss - loss.item()) &lt; 1e-5:\n        converged = True\n    prev_loss = loss.item() \n    i = i + 1\n\n0 0.9349006414413452\n10 0.8724980354309082\n20 0.817572832107544\n30 0.7688440084457397\n40 0.7244434952735901\n50 0.6841480731964111\n60 0.648054838180542\n70 0.6158449053764343\n80 0.5871520638465881\n90 0.5616370439529419\n100 0.5389482378959656\n110 0.5187534093856812\n120 0.5007484555244446\n130 0.48465821146965027\n140 0.4702383279800415\n150 0.4572741687297821\n160 0.4455784261226654\n170 0.434988796710968\n180 0.4253652095794678\n190 0.4165867269039154\n200 0.4085492193698883\n210 0.4011631906032562\n220 0.3943515419960022\n230 0.3880476653575897\n240 0.38219425082206726\n250 0.3767416477203369\n260 0.37164685130119324\n270 0.36687251925468445\n280 0.36238619685173035\n290 0.3581596612930298\n300 0.3541681170463562\n310 0.3503900170326233\n320 0.34680625796318054\n330 0.3434002697467804\n340 0.340157151222229\n350 0.33706405758857727\n360 0.3341093361377716\n370 0.3312827944755554\n380 0.32857537269592285\n390 0.3259788155555725\n400 0.3234858810901642\n410 0.32109004259109497\n420 0.3187854290008545\n430 0.3165667653083801\n440 0.3144291937351227\n450 0.3123684227466583\n460 0.31038039922714233\n470 0.3084614872932434\n480 0.30660852789878845\n490 0.30481836199760437\n500 0.30308809876441956\n510 0.30141526460647583\n520 0.2997974157333374\n530 0.2982322573661804\n540 0.29671770334243774\n550 0.2952517569065094\n560 0.2938326597213745\n570 0.29245856404304504\n580 0.29112792015075684\n590 0.2898390293121338\n600 0.28859052062034607\n610 0.2873808741569519\n620 0.2862089276313782\n630 0.2850732207298279\n640 0.2839725613594055\n650 0.28290584683418274\n660 0.28187182545661926\n670 0.28086957335472107\n680 0.2798978090286255\n690 0.27895569801330566\n700 0.278042197227478\n710 0.2771564722061157\n720 0.2762974798679352\n730 0.2754644453525543\n740 0.2746565043926239\n750 0.27387282252311707\n760 0.2731126844882965\n770 0.2723752558231354\n780 0.27165982127189636\n790 0.27096569538116455\n800 0.270292192697525\n810 0.26963862776756287\n820 0.2690044343471527\n830 0.26838886737823486\n840 0.2677914798259735\n850 0.2672116458415985\n860 0.2666487991809845\n870 0.2661023437976837\n880 0.26557186245918274\n890 0.2650567889213562\n900 0.2645567059516907\n910 0.26407110691070557\n920 0.2635995149612427\n930 0.26314154267311096\n940 0.2626967132091522\n950 0.2622646987438202\n960 0.26184508204460144\n970 0.26143744587898254\n980 0.26104146242141724\n990 0.2606567442417145\n1000 0.260282963514328\n1010 0.25991976261138916\n1020 0.25956690311431885\n1030 0.259223997592926\n1040 0.2588907480239868\n1050 0.2585669755935669\n1060 0.25825226306915283\n1070 0.2579464018344879\n1080 0.2576490640640259\n1090 0.25736016035079956\n1100 0.25707927346229553\n1110 0.25680628418922424\n1120 0.25654086470603943\n1130 0.25628283619880676\n1140 0.2560320496559143\n1150 0.2557882070541382\n1160 0.25555115938186646\n1170 0.25532066822052\n1180 0.25509655475616455\n1190 0.2548786997795105\n1200 0.25466686487197876\n1210 0.254460871219635\n1220 0.2542605996131897\n1230 0.2540658116340637\n1240 0.2538764476776123\n1250 0.2536923289299011\n1260 0.25351327657699585\n1270 0.25333911180496216\n1280 0.25316980481147766\n1290 0.25300514698028564\n1300 0.25284504890441895\n1310 0.25268933176994324\n1320 0.25253793597221375\n1330 0.25239071249961853\n1340 0.25224751234054565\n1350 0.25210827589035034\n1360 0.25197288393974304\n1370 0.2518412172794342\n1380 0.25171321630477905\n1390 0.25158870220184326\n1400 0.25146764516830444\n1410 0.25134992599487305\n1420 0.2512354850769043\n1430 0.25112417340278625\n1440 0.25101593136787415\n1450 0.2509107291698456\n1460 0.2508084177970886\n\n\n\npredict_plot_grid(log_reg)\n\n\n\n\n\n\n\n\n\n# Iris dataset\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n\nX = iris.data\ny = iris.target\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nText(0, 0.5, 'Sepal width')\n\n\n\n\n\n\n\n\n\n\nclass ThreeClassLogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(ThreeClassLogisticRegression, self).__init__()\n        self.linear1 = nn.Linear(input_dim, 1)\n        self.linear2 = nn.Linear(input_dim, 1)\n        self.linear3 = nn.Linear(input_dim, 1)\n    \n    def forward(self, x):\n        logits1 = self.linear1(x)\n        logits2 = self.linear2(x)\n        logits3 = self.linear3(x)\n        return torch.cat([logits1, logits2, logits3], dim=-1)\n    \n\nclass MultiClassLogisticRegression(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(MultiClassLogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, num_classes)\n    \n    def forward(self, x):\n        logits = self.linear(x)\n        return logits\n\n\nmlr = ThreeClassLogisticRegression(2)\n\n\nX_tensor = torch.tensor(X, dtype=torch.float32)[:,:2]\ny_tensor = torch.tensor(y, dtype=torch.long)\n\n\nmlr(X_tensor).shape\n\ntorch.Size([150, 3])\n\n\n\nmlr_efficient = MultiClassLogisticRegression(2, 3)\nmlr_efficient(X_tensor).shape\n\ntorch.Size([150, 3])\n\n\n\nmlr_efficient(X_tensor[:5])\n\ntensor([[-0.4276,  2.0524, -1.1928],\n        [-0.4378,  1.7861, -1.3077],\n        [-0.4070,  1.8339, -1.1492],\n        [-0.4033,  1.7681, -1.1480],\n        [-0.4122,  2.0762, -1.1135]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\n\nnn.Softmax(dim=-1)(mlr_efficient(X_tensor[:5])).sum(dim=-1)\n\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n\n\n\ndef plot_most_probable_class(model):\n    XX, YY = torch.meshgrid(torch.linspace(4, 8, 100), torch.linspace(1.5, 4.5, 100))\n    X_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim=-1)\n    logits = model(X_grid)\n    predicted_class = torch.argmax(logits, dim=-1)\n    plt.contourf(XX, YY, predicted_class.reshape(100, 100).detach().numpy(), levels=[-0.5, 0.5, 1.5, 2.5], \n                 cmap=plt.cm.Spectral, alpha=0.5)\n    plt.colorbar()\n\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    \nplot_most_probable_class(mlr_efficient)\n\n\n\n\n\n\n\n\n\nlogits = mlr_efficient(X_tensor)\nnn.Softmax(dim=-1)(logits[:5])\n\ntensor([[0.0746, 0.8907, 0.0347],\n        [0.0938, 0.8669, 0.0393],\n        [0.0919, 0.8643, 0.0438],\n        [0.0976, 0.8560, 0.0464],\n        [0.0739, 0.8895, 0.0366]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\nopt = torch.optim.Adam(mlr_efficient.parameters(), lr=0.01)\n\nconverged = False\nprev_loss = 1e8\n\ni = 0\nwhile not converged:\n    opt.zero_grad()\n    logits = mlr_efficient(X_tensor)\n    loss = F.cross_entropy(logits, y_tensor)\n    loss.backward()\n    opt.step()\n    if i%10==0:\n        print(i, loss.item())\n    if np.abs(prev_loss - loss.item()) &lt; 1e-5:\n        converged = True\n    prev_loss = loss.item() \n    i = i + 1\n\n0 2.2876317501068115\n10 1.4277923107147217\n20 1.1896789073944092\n30 1.173270583152771\n40 1.1399130821228027\n50 1.1091488599777222\n60 1.0889300107955933\n70 1.065254807472229\n80 1.0434192419052124\n90 1.0214812755584717\n100 1.0000522136688232\n110 0.9791292548179626\n120 0.9588465094566345\n130 0.9392611980438232\n140 0.9204307794570923\n150 0.9023836255073547\n160 0.885133683681488\n170 0.8686791062355042\n180 0.8530094027519226\n190 0.8381044268608093\n200 0.8239390254020691\n210 0.8104835748672485\n220 0.797706127166748\n230 0.7855730056762695\n240 0.7740503549575806\n250 0.7631040811538696\n260 0.7527009844779968\n270 0.7428091168403625\n280 0.733397364616394\n290 0.7244365811347961\n300 0.7158986330032349\n310 0.7077574729919434\n320 0.6999881863594055\n330 0.6925678849220276\n340 0.685474693775177\n350 0.6786885261535645\n360 0.6721901893615723\n370 0.6659626960754395\n380 0.6599892973899841\n390 0.6542547345161438\n400 0.6487451791763306\n410 0.6434471607208252\n420 0.6383485794067383\n430 0.633437991142273\n440 0.628704845905304\n450 0.624139130115509\n460 0.6197317242622375\n470 0.6154740452766418\n480 0.611358106136322\n490 0.6073764562606812\n500 0.6035220623016357\n510 0.5997886061668396\n520 0.5961697101593018\n530 0.5926598906517029\n540 0.5892537236213684\n550 0.5859463214874268\n560 0.5827329754829407\n570 0.5796091556549072\n580 0.5765710473060608\n590 0.5736144781112671\n600 0.5707359313964844\n610 0.56793212890625\n620 0.565199613571167\n630 0.5625355243682861\n640 0.5599370002746582\n650 0.5574012994766235\n660 0.5549259781837463\n670 0.5525084137916565\n680 0.5501465797424316\n690 0.5478382110595703\n700 0.5455812811851501\n710 0.5433740019798279\n720 0.5412144660949707\n730 0.5391008257865906\n740 0.5370318293571472\n750 0.535005509853363\n760 0.5330204963684082\n770 0.5310754179954529\n780 0.5291690826416016\n790 0.527300238609314\n800 0.5254673957824707\n810 0.5236696004867554\n820 0.5219057202339172\n830 0.5201747417449951\n840 0.5184756517410278\n850 0.5168076157569885\n860 0.5151694416999817\n870 0.5135605335235596\n880 0.5119800567626953\n890 0.5104270577430725\n900 0.5089008212089539\n910 0.5074005722999573\n920 0.5059257745742798\n930 0.5044757723808289\n940 0.5030497908592224\n950 0.5016472935676575\n960 0.5002675652503967\n970 0.49891015887260437\n980 0.49757450819015503\n990 0.4962601363658905\n1000 0.49496641755104065\n1010 0.4936929941177368\n1020 0.4924393594264984\n1030 0.49120497703552246\n1040 0.4899895191192627\n1050 0.4887924790382385\n1060 0.4876135289669037\n1070 0.48645222187042236\n1080 0.48530831933021545\n1090 0.4841812551021576\n1100 0.4830707311630249\n1110 0.4819765090942383\n1120 0.4808981418609619\n1130 0.4798354208469391\n1140 0.478787899017334\n1150 0.47775542736053467\n1160 0.4767375588417053\n1170 0.4757342040538788\n1180 0.47474488615989685\n1190 0.4737694263458252\n1200 0.4728076756000519\n1210 0.47185924649238586\n1220 0.47092387080192566\n1230 0.4700014293193817\n1240 0.4690915048122406\n1250 0.4681941270828247\n1260 0.467308908700943\n1270 0.4664357006549835\n1280 0.4655742347240448\n1290 0.4647243618965149\n1300 0.4638858437538147\n1310 0.46305856108665466\n1320 0.4622422754764557\n1330 0.4614367187023163\n1340 0.4606419801712036\n1350 0.4598575830459595\n1360 0.45908355712890625\n1370 0.45831960439682007\n1380 0.457565575838089\n1390 0.45682138204574585\n1400 0.4560868740081787\n1410 0.455361932516098\n1420 0.4546462893486023\n1430 0.45393991470336914\n1440 0.4532424509525299\n1450 0.4525540769100189\n1460 0.45187443494796753\n1470 0.4512034058570862\n1480 0.4505409896373749\n1490 0.44988682866096497\n1500 0.4492410719394684\n1510 0.44860342144966125\n1520 0.44797372817993164\n1530 0.44735199213027954\n1540 0.4467381238937378\n1550 0.4461318850517273\n1560 0.4455331563949585\n1570 0.444941908121109\n1580 0.4443580210208893\n1590 0.44378137588500977\n1600 0.4432118237018585\n1610 0.44264933466911316\n1620 0.44209375977516174\n1630 0.4415450394153595\n1640 0.44100311398506165\n1650 0.4404677450656891\n1660 0.4399389624595642\n1670 0.4394165575504303\n1680 0.43890058994293213\n1690 0.438390851020813\n1700 0.4378872811794281\n1710 0.4373898208141327\n1720 0.436898410320282\n1730 0.4364128112792969\n1740 0.43593311309814453\n1750 0.4354592263698578\n1760 0.43499094247817993\n1770 0.4345283508300781\n1780 0.4340711832046509\n1790 0.4336196482181549\n1800 0.43317320942878723\n1810 0.4327322244644165\n1820 0.432296484708786\n1830 0.4318659007549286\n1840 0.4314402639865875\n1850 0.43101978302001953\n1860 0.4306041896343231\n1870 0.4301934838294983\n1880 0.42978763580322266\n1890 0.4293864071369171\n1900 0.4289900064468384\n1910 0.4285980761051178\n1920 0.4282107651233673\n1930 0.427827924489975\n1940 0.4274495542049408\n1950 0.4270755648612976\n1960 0.4267057776451111\n1970 0.4263403117656708\n1980 0.42597898840904236\n1990 0.4256218671798706\n2000 0.4252687394618988\n2010 0.4249196946620941\n2020 0.4245745837688446\n2030 0.42423343658447266\n2040 0.42389607429504395\n2050 0.42356255650520325\n2060 0.423232764005661\n2070 0.4229067265987396\n2080 0.42258426547050476\n2090 0.42226549983024597\n2100 0.42195025086402893\n2110 0.42163845896720886\n2120 0.42133021354675293\n2130 0.42102527618408203\n2140 0.4207237660884857\n2150 0.4204255938529968\n2160 0.4201306700706482\n2170 0.41983893513679504\n2180 0.41955047845840454\n2190 0.41926509141921997\n2200 0.4189828038215637\n2210 0.4187036156654358\n2220 0.41842740774154663\n2230 0.41815415024757385\n2240 0.41788384318351746\n2250 0.41761642694473267\n2260 0.4173519015312195\n2270 0.41709014773368835\n2280 0.41683119535446167\n2290 0.4165749251842499\n2300 0.41632139682769775\n2310 0.41607049107551575\n2320 0.41582220792770386\n2330 0.41557666659355164\n2340 0.41533347964286804\n2350 0.41509291529655457\n2360 0.41485482454299927\n2370 0.41461917757987976\n2380 0.41438594460487366\n2390 0.4141550362110138\n2400 0.4139265716075897\n2410 0.4137003421783447\n2420 0.41347646713256836\n2430 0.41325485706329346\n2440 0.41303539276123047\n2450 0.41281819343566895\n2460 0.41260311007499695\n2470 0.41239017248153687\n2480 0.41217929124832153\n2490 0.4119705855846405\n2500 0.4117638170719147\n2510 0.411559134721756\n2520 0.4113563895225525\n2530 0.41115570068359375\n2540 0.41095682978630066\n2550 0.4107598662376404\n2560 0.4105648696422577\n2570 0.41037166118621826\n2580 0.4101802706718445\n2590 0.40999075770378113\n2600 0.40980294346809387\n2610 0.4096169173717499\n2620 0.40943264961242676\n2630 0.40925002098083496\n2640 0.40906909108161926\n2650 0.4088898301124573\n2660 0.4087121784687042\n2670 0.4085361659526825\n2680 0.4083617031574249\n2690 0.4081888198852539\n2700 0.40801751613616943\n2710 0.40784770250320435\n2720 0.40767937898635864\n2730 0.4075125753879547\n2740 0.407347172498703\n2750 0.4071832001209259\n2760 0.4070206582546234\n2770 0.40685951709747314\n2780 0.4066997766494751\n2790 0.4065413773059845\n2800 0.40638434886932373\n2810 0.40622857213020325\n2820 0.4060741662979126\n2830 0.40592101216316223\n2840 0.40576907992362976\n2850 0.40561842918395996\n2860 0.40546900033950806\n2870 0.40532082319259644\n2880 0.40517380833625793\n2890 0.40502792596817017\n2900 0.4048832654953003\n2910 0.40473973751068115\n2920 0.40459728240966797\n2930 0.4044559895992279\n2940 0.4043157994747162\n2950 0.40417665243148804\n2960 0.40403860807418823\n2970 0.403901606798172\n2980 0.40376555919647217\n2990 0.40363067388534546\n3000 0.40349671244621277\n3010 0.40336373448371887\n3020 0.40323179960250854\n3030 0.40310072898864746\n3040 0.40297067165374756\n3050 0.4028415381908417\n3060 0.4027132987976074\n3070 0.4025859832763672\n3080 0.40245962142944336\n3090 0.4023340940475464\n3100 0.40220946073532104\n3110 0.4020856022834778\n3120 0.40196263790130615\n3130 0.4018405079841614\n3140 0.40171927213668823\n3150 0.40159881114959717\n3160 0.401479035615921\n3170 0.4013602137565613\n3180 0.40124207735061646\n3190 0.4011247456073761\n3200 0.40100815892219543\n3210 0.40089231729507446\n3220 0.4007771611213684\n3230 0.4006628096103668\n3240 0.4005492031574249\n3250 0.400436133146286\n3260 0.40032392740249634\n3270 0.40021228790283203\n3280 0.40010136365890503\n3290 0.39999115467071533\n3300 0.3998815715312958\n3310 0.39977261424064636\n3320 0.3996643126010895\n3330 0.3995566666126251\n3340 0.39944958686828613\n3350 0.3993431627750397\n3360 0.3992373049259186\n3370 0.39913210272789\n3380 0.3990274667739868\n3390 0.398923397064209\n3400 0.3988198935985565\n3410 0.3987169563770294\n3420 0.3986146152019501\n3430 0.3985127806663513\n3440 0.39841151237487793\n\n\n\nmlr_efficient(X_tensor[:5])\n\ntensor([[ 3.3141, -2.0900, -3.7057],\n        [ 1.1409, -1.1217, -3.2703],\n        [ 3.4758, -1.6582, -4.0551],\n        [ 3.3553, -1.4965, -4.0989],\n        [ 4.4815, -2.3582, -4.0981]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\npred = F.softmax(mlr_efficient(X_tensor), dim=-1).detach().numpy()\n\n\nimport pandas as pd\n\n\ndf = pd.DataFrame(pred, columns=iris.target_names)\ndf[\"GT\"] = iris.target\ndf\n\n\n\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\nGT\n\n\n\n\n0\n0.994636\n0.004474\n0.000889\n0\n\n\n1\n0.895877\n0.093246\n0.010877\n0\n\n\n2\n0.993612\n0.005855\n0.000533\n0\n\n\n3\n0.991676\n0.007750\n0.000574\n0\n\n\n4\n0.998743\n0.001069\n0.000188\n0\n\n\n...\n...\n...\n...\n...\n\n\n145\n0.000091\n0.306116\n0.693793\n2\n\n\n146\n0.000021\n0.511070\n0.488909\n2\n\n\n147\n0.000359\n0.380096\n0.619545\n2\n\n\n148\n0.070860\n0.428247\n0.500893\n2\n\n\n149\n0.018367\n0.611113\n0.370520\n2\n\n\n\n\n150 rows  4 columns\n\n\n\n\n\n# Find prediction\ny_pred = pred.argmax(axis=-1)\n\ndf[\"Predicted Class\"] = y_pred\n\n\ndf\n\n\n\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\nGT\nPredicted Class\n\n\n\n\n0\n0.994636\n0.004474\n0.000889\n0\n0\n\n\n1\n0.895877\n0.093246\n0.010877\n0\n0\n\n\n2\n0.993612\n0.005855\n0.000533\n0\n0\n\n\n3\n0.991676\n0.007750\n0.000574\n0\n0\n\n\n4\n0.998743\n0.001069\n0.000188\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n0.000091\n0.306116\n0.693793\n2\n2\n\n\n146\n0.000021\n0.511070\n0.488909\n2\n1\n\n\n147\n0.000359\n0.380096\n0.619545\n2\n2\n\n\n148\n0.070860\n0.428247\n0.500893\n2\n2\n\n\n149\n0.018367\n0.611113\n0.370520\n2\n1\n\n\n\n\n150 rows  5 columns\n\n\n\n\n\n# Accuracy\n(y_pred == iris.target).mean()\n\n0.8266666666666667\n\n\n\nplot_most_probable_class(mlr_efficient)\n\n\n\n\n\n\n\n\n\n# Add more features like x^2, xy, y^2\n\nX = iris.data\ny = iris.target\n\nX = np.concatenate([X, X**2, X[:, [0]]*X[:, [1]], X[:, [1]]**2], axis=-1)\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\n\nmlr_efficient = MultiClassLogisticRegression(10, 3)\n\n\nopt = torch.optim.Adam(mlr_efficient.parameters(), lr=0.01)\n\nconverged = False\n\nprev_loss = 1e8\n\ni = 0\nwhile not converged:\n    opt.zero_grad()\n    logits = mlr_efficient(X_tensor)\n    loss = F.cross_entropy(logits, y_tensor)\n    loss.backward()\n    opt.step()\n    if i%10==0:\n        print(i, loss.item())\n    if np.abs(prev_loss - loss.item()) &lt; 1e-5:\n        converged = True\n    prev_loss = loss.item() \n    i = i + 1\n    \n\n0 4.175729751586914\n10 0.7128309011459351\n20 0.42325228452682495\n30 0.35874807834625244\n40 0.2861122786998749\n50 0.24453814327716827\n60 0.21917879581451416\n70 0.19883905351161957\n80 0.18379421532154083\n90 0.17121852934360504\n100 0.16081109642982483\n110 0.15182484686374664\n120 0.14402928948402405\n130 0.13717801868915558\n140 0.13111360371112823\n150 0.125710129737854\n160 0.1208672821521759\n170 0.11650418490171432\n180 0.11255431920289993\n190 0.10896281152963638\n200 0.1056838408112526\n210 0.10267896950244904\n220 0.09991569072008133\n230 0.09736639261245728\n240 0.09500744193792343\n250 0.0928184986114502\n260 0.0907820537686348\n270 0.08888282626867294\n280 0.08710754662752151\n290 0.08544456958770752\n300 0.0838836133480072\n310 0.0824156403541565\n320 0.0810326412320137\n330 0.07972748577594757\n340 0.07849381864070892\n350 0.07732591778039932\n360 0.07621872425079346\n370 0.07516761124134064\n380 0.07416845113039017\n390 0.0732174664735794\n400 0.07231126725673676\n410 0.07144675403833389\n420 0.07062112540006638\n430 0.06983178853988647\n440 0.06907644867897034\n450 0.06835290044546127\n460 0.06765919178724289\n470 0.06699351221323013\n480 0.0663541629910469\n490 0.06573962420225143\n500 0.06514845788478851\n510 0.06457936018705368\n520 0.06403109431266785\n530 0.06350252777338028\n540 0.0629926398396492\n550 0.06250040978193283\n560 0.062024928629398346\n570 0.06156538799405098\n580 0.061120934784412384\n590 0.06069087237119675\n600 0.060274481773376465\n610 0.0598711334168911\n620 0.05948017165064812\n630 0.05910108610987663\n640 0.05873329937458038\n650 0.05837631970643997\n660 0.0580296628177166\n670 0.057692889124155045\n680 0.057365577667951584\n690 0.05704731494188309\n700 0.05673772841691971\n710 0.056436482816934586\n720 0.05614320933818817\n730 0.055857621133327484\n740 0.05557936802506447\n750 0.05530823394656181\n760 0.055043887346982956\n770 0.05478610843420029\n780 0.054534632712602615\n790 0.05428922176361084\n800 0.05404968187212944\n810 0.053815752267837524\n820 0.05358729138970375\n830 0.05336408689618111\n840 0.053145941346883774\n850 0.05293269827961922\n860 0.052724193781614304\n870 0.05252022668719292\n880 0.052320707589387894\n890 0.052125465124845505\n900 0.0519343726336956\n910 0.05174724757671356\n920 0.05156403407454491\n930 0.05138456076383591\n940 0.0512087419629097\n950 0.05103643983602524\n960 0.05086757242679596\n970 0.05070202797651291\n980 0.05053970590233803\n990 0.05038050189614296\n1000 0.05022436007857323\n1010 0.05007114261388779\n1020 0.04992081969976425\n1030 0.04977325350046158\n1040 0.049628421664237976\n1050 0.049486223608255386\n1060 0.04934656620025635\n1070 0.04920942336320877\n1080 0.0490746907889843\n1090 0.04894234612584114\n1100 0.04881228506565094\n1110 0.04868446663022041\n1120 0.048558831214904785\n1130 0.048435330390930176\n1140 0.04831390082836151\n1150 0.04819450154900551\n1160 0.0480770505964756\n1170 0.04796154424548149\n1180 0.0478479266166687\n1190 0.04773610457777977\n1200 0.04762609302997589\n1210 0.047517817467451096\n1220 0.04741125553846359\n1230 0.047306355088949203\n1240 0.04720309004187584\n1250 0.047101400792598724\n\n\n\n# Accuracy\npred = F.softmax(mlr_efficient(X_tensor), dim=-1).detach().numpy()\ny_pred = pred.argmax(axis=-1)\n(y_pred == iris.target).mean()\n\n0.9866666666666667"
  },
  {
    "objectID": "notebooks/cost-iteration-notebook.html",
    "href": "notebooks/cost-iteration-notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n\n\n\nX = np.array([1, 2, 3])\ny = np.array([1, 2, 3])\n\n\ndef y_hat(X, theta_0, theta_1):\n    return theta_0 + theta_1*X\n\n\ndef cost(X, y, theta_0, theta_1):\n    yh = y_hat(X, theta_0, theta_1)\n    return (y-yh).T@(y-yh)\n\n\ntheta_0 = 4\ntheta_1 = 0\nalpha = 0.1\ncosts = np.zeros(1000)\ntheta_0_list = np.zeros(1000)\ntheta_1_list = np.zeros(1000)\n\nfor i in range(1000):\n    costs[i] = cost(X, y, theta_0, theta_1)\n    theta_0 = theta_0 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).mean())\n    theta_1 = theta_1 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).T@X)/len(X)\n    theta_0_list[i] = theta_0\n    theta_1_list[i] = theta_1\n\n\nimport sys\nsys.path.append(\"../\")\nfrom latexify import *\n\n\nlatexify()\nplt.plot(costs[:200], 'k')\nformat_axes(plt.gca())\nplt.ylabel(\"Cost\")\nplt.xlabel(\"Iteration\")\nplt.savefig(\"../gradient-descent/gd-iter-cost.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\n\nfor i in range(0, 200, 20):\n    plt.title(label=\"Fit at iteration {}\".format(i))\n    plt.plot(X, theta_0_list[i]+theta_1_list[i]*X, color='k')\n    plt.scatter(X, y, color='k')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/fit-iteration-{}.pdf\".format(i), bbox_inches=\"tight\", transparent=True)\n    plt.cla()\n\n\n\n\n\n\n\n\n\ntheta_0 = 4\ntheta_1 = 0\n(y_hat(X, theta_0, theta_1)-y).mean()\n\n2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1)).mean()\n\n-2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1))@X\n\n-10"
  },
  {
    "objectID": "notebooks/ensemble-representation.html",
    "href": "notebooks/ensemble-representation.html",
    "title": "Representation of Ensemble Models",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nh = .02  # step size in the mesh\n\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            ]\n\n\n\nfrom latexify import latexify\ntry:\n    latexify()\nexcept:\n    pass\n\n\ndef plot_comparison(maximum_depth):\n    names = [\"Decision Tree (Depth %d)\" %maximum_depth, \"Random Forest\"]\n    classifiers = [\n    DecisionTreeClassifier(max_depth=maximum_depth),\n    RandomForestClassifier(max_depth=maximum_depth, n_estimators=200, max_features=1),\n   ]\n    figure = plt.figure(figsize=(8, 4))\n    i = 1\n    # iterate over datasets\n    for ds_cnt, ds in enumerate(datasets):\n        # preprocess dataset, split into training and test part\n        X, y = ds\n        X = StandardScaler().fit_transform(X)\n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=.4, random_state=42)\n\n        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                            np.arange(y_min, y_max, h))\n\n        # just plot the dataset first\n        cm = plt.cm.RdBu\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        if ds_cnt == 0:\n            ax.set_title(\"Input data\")\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n                edgecolors='k')\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        i += 1\n\n        # iterate over classifiers\n        for name, clf in zip(names, classifiers):\n            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n            clf.fit(X_train, y_train)\n            score = clf.score(X_test, y_test)\n\n            # Plot the decision boundary. For that, we will assign a color to each\n            # point in the mesh [x_min, x_max]x[y_min, y_max].\n            if hasattr(clf, \"decision_function\"):\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n            # Put the result into a color plot\n            Z = Z.reshape(xx.shape)\n            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n            # Plot the training points\n            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                    edgecolors='k')\n            # Plot the testing points\n            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                    edgecolors='k', alpha=0.6)\n\n            ax.set_xlim(xx.min(), xx.max())\n            ax.set_ylim(yy.min(), yy.max())\n            ax.set_xticks(())\n            ax.set_yticks(())\n            if ds_cnt == 0:\n                ax.set_title(name)\n            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                    size=15, horizontalalignment='right')\n            i += 1\n\n\n    plt.savefig(f\"../figures/ensemble/{str(maximum_depth)}-representation.pdf\" , transparent=True, bbox_inches=\"tight\")\n\n\nplot_comparison(2)\n\n\n\n\n\n\n\n\n\nplot_comparison(1)"
  },
  {
    "objectID": "notebooks/polynomial_features.html",
    "href": "notebooks/polynomial_features.html",
    "title": "Polynomial Regression with Basis Functions",
    "section": "",
    "text": "Reference: https://alexshtf.github.io/2024/01/21/Bernstein.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport numpy.polynomial.polynomial as poly\nfrom sklearn.linear_model import Ridge\n\n\n1. Simple Polynomial Regression\n\nm = number of data points\nn = degree of polynomial\nInput: \\[\\{X_i\\}_{i=0}^{m}\\]\nTarget: \\[\\{y_i\\}_{i=0}^{m}\\]\nFeature Expansion: \\[\\mathbb{E}_n = {1, x, x^2, ..., x^n}\\]\nPrediction:\n\n\\[\\hat{y}_i = \\alpha_0 \\cdot 1 + \\alpha_1 \\cdot x + \\alpha_2 \\cdot x^2 + \\cdots + \\alpha_n x^n\\]\n\nLoss: \\[L = \\sum_{i=1}^m (\\alpha_0 + \\alpha_1 x_i + \\dots + \\alpha_n x_i^n - y_i)^2\\]\nFeature expanded X:\n\\[\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^n \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^n \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_m & x_m^2 & \\dots & x_m^n \\\\\n\\end{pmatrix}\\]\nPolynomials have different degrees, therefore, different units.\nnumpy.polynomial.polynomial.polyvander takes X and expands it to the above matrix.\n\n\nX = np.array([1, 2, 3, 4])\nn = 3\n\n\nX_poly = poly.polyvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [1 2 3 4]\n\nX_poly:\n\n\narray([[ 1.,  1.,  1.,  1.],\n       [ 1.,  2.,  4.,  8.],\n       [ 1.,  3.,  9., 27.],\n       [ 1.,  4., 16., 64.]])\n\n\n\n\n\n2. Chebyshev polynomials\n\\[\\mathbb{T}_n = \\{ T_0, T_1, \\dots, T_n \\}\\]\n\\[\\begin{align*}\nT_0(x) &= 1 \\\\\nT_1(x) &= x \\\\\nT_{n+1}(x) &= 2xT_n(x) - T_{n-1}(x)\n\\end{align*}\\]\n\nFeature expansion:\n\\[\\begin{pmatrix}\nT_0(x_1) & T_1(x_1) & \\dots & T_n(x_1) \\\\\nT_0(x_2) & T_1(x_2) & \\dots & T_n(x_2) \\\\\n\\vdots & \\vdots  & \\ddots& \\vdots  \\\\\nT_0(x_m) & T_1(x_m) & \\dots & T_n(x_m) \\\\\n\\end{pmatrix}\\]\nPolynomial \\(T_k\\) are k-degree polynomials. Therefore, their units are different.\nnumpy.polynomial.chebyshev.chebvander(X, deg) takes X and expands it to the above matrix.\n\n\ndef chebyshev_polynomial(n, x):\n    if n == 0:\n        return 1\n    elif n == 1:\n        return x\n    else:\n        return 2 * x * chebyshev_polynomial(n - 1, x) - chebyshev_polynomial(n - 2, x)\n\n# Example: Generate and print Chebyshev polynomials T_0(x) to T_4(x) at x = 0.5\nx_value = 4\nchebyshev_results = []\nfor i in range(4):\n    result = chebyshev_polynomial(i, x_value)\n    chebyshev_results.append(result)\n\nprint(x_value)\nprint(chebyshev_results)\n\n4\n[1, 4, 31, 244]\n\n\n\nimport numpy.polynomial.chebyshev as cheb\n\nX_poly = cheb.chebvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [1 2 3 4]\n\nX_poly:\n\n\narray([[  1.,   1.,   1.,   1.],\n       [  1.,   2.,   7.,  26.],\n       [  1.,   3.,  17.,  99.],\n       [  1.,   4.,  31., 244.]])\n\n\n\n\n\n3. Legendre polynomials\n\\[\\mathbb{P}_n = \\{ P_0, P_1, \\dots, P_n \\}\\]\n\\[\\begin{align*}\nP_0(x) &= 1 \\\\\nP_1(x) &= x \\\\\n(n+1)P_{n+1}(x) &= (2n+1)xP_n(x) - nP_{n-1}(x)\n\\end{align*}\\]\n\nFeature expansion:\n\\[\\begin{pmatrix}\nP_0(x_1) & P_1(x_1) & \\dots & P_n(x_1) \\\\\nP_0(x_2) & P_1(x_2) & \\dots & P_n(x_2) \\\\\n\\vdots & \\vdots  & \\ddots& \\vdots  \\\\\nP_0(x_m) & P_1(x_m) & \\dots & P_n(x_m) \\\\\n\\end{pmatrix}\\]\nPolynomial \\(P_k\\) are both k-degree polynomials. Therefore, their units are different.\nnumpy.polynomial.legendre.legvander(X, degn) takes X and expands it to the above matrix.\n\n\ndef legendre_polynomial(n, x):\n    if n == 0:\n        return 1\n    elif n == 1:\n        return x\n    else:\n        return ((2 * n - 1) * x * legendre_polynomial(n - 1, x) - (n - 1) * legendre_polynomial(n - 2, x)) / n\n\n# Example usage:\nx_value = 4\ndegree = 3\nlegendre_results = []\nfor i in range(degree + 1):\n    legendre_results.append(legendre_polynomial(i, x_value))\n\nprint(x_value)\nprint(legendre_results)\n\n4\n[1, 4, 23.5, 154.0]\n\n\n\nimport numpy.polynomial.legendre as leg\n\nX_poly = leg.legvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [1 2 3 4]\n\nX_poly:\n\n\narray([[  1. ,   1. ,   1. ,   1. ],\n       [  1. ,   2. ,   5.5,  17. ],\n       [  1. ,   3. ,  13. ,  63. ],\n       [  1. ,   4. ,  23.5, 154. ]])\n\n\n\n\n\n4. Bernstein basis\n\\[\\mathbb{B}_n = \\{  B_{0,n}, \\dots, B_{n, n} \\}\\]\n\\[B_{i,n}(x) = \\binom{n}{i} x^i (1-x)^{n-i}\\] - here, x is probability of success, and i is the number of successes. Therefore, \\[0=&lt;x&lt;=1\\] - Feature expansion:\n\\[\\begin{pmatrix}\nB_{0,n}(x_1) & B_{1,n}(x_1) & \\dots & B_{n,n}(x_1) \\\\\nB_{0,n}(x_2) & B_{1,n}(x_2) & \\dots & B_{n,n}(x_2) \\\\\n\\vdots & \\vdots  & \\ddots& \\vdots  \\\\\nB_{0,n}(x_m) & B_{1,n}(x_m) & \\dots & B_{n,n}(x_m) \\\\\n\\end{pmatrix}\\]\n\nPolynomial \\(B_{i, n}\\) are n-degree polynomials. Therefore, their units are same.\nscipy.stats.binom.pmf(i, n, x) gives the binomial coefficient.\n\n\nfrom scipy.stats import binom\n\nX = np.array([0.0, 0.5, 0.7])\n\ndef bernvander(x, deg):\n    return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n\nX_poly = bernvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [0.  0.5 0.7]\n\nX_poly:\n\n\narray([[1.   , 0.   , 0.   , 0.   ],\n       [0.125, 0.375, 0.375, 0.125],\n       [0.027, 0.189, 0.441, 0.343]])\n\n\n\n\n\nRidge Regression\n\nLinear regression with penalty on the weights - ensures that the coefficients \\((\\alpha_0, \\alpha_1, \\alpha_2, ..., \\alpha_n)\\) do not grow too large.\nRidge coefficients, \\(\\alpha\\) determines the amount of shrinkage:\n\n\\(\\alpha = 0\\): no shrinkage\n\\(\\alpha = \\infty\\): all coefficients are zero\n\nNote: you will study this in upcoming lectures.\n\n\n\n# True function\ndef true_func(x):\n  return np.sin(8 * np.pi * x) / np.exp(x) + x\n\n\nm = 30\nsigma = 0.1\n\n# generate\nnp.random.seed(42)\nX = np.random.rand(m)\ny = true_func(X) + sigma * np.random.randn(m)\n\n\nplt_xs = np.linspace(0, 1, 1000)\nplt.scatter(X, y)\nplt.plot(plt_xs, true_func(plt_xs), 'blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1. Simple polynomial regression\n\nn = 50\nX_poly = poly.polyvander(X, deg=n)\nX_poly.shape\n\n(30, 51)\n\n\n\nX_poly[0,:]\n\narray([1.00000000e+00, 3.74540119e-01, 1.40280301e-01, 5.25406005e-02,\n       1.96785627e-02, 7.37041123e-03, 2.76051470e-03, 1.03392350e-03,\n       3.87245832e-04, 1.45039100e-04, 5.43229617e-05, 2.03461285e-05,\n       7.62044140e-06, 2.85416103e-06, 1.06899781e-06, 4.00382567e-07,\n       1.49959334e-07, 5.61657868e-08, 2.10363405e-08, 7.87895346e-09,\n       2.95098417e-09, 1.10526196e-09, 4.13964946e-10, 1.55046480e-10,\n       5.80711271e-11, 2.17499668e-11, 8.14623516e-12, 3.05109189e-12,\n       1.14275632e-12, 4.28008087e-13, 1.60306200e-13, 6.00411031e-14,\n       2.24878019e-14, 8.42258399e-15, 3.15459561e-15, 1.18152261e-15,\n       4.42527621e-16, 1.65744348e-16, 6.20779076e-17, 2.32506669e-17,\n       8.70830755e-18, 3.26161054e-18, 1.22160400e-18, 4.57539708e-19,\n       1.71366976e-19, 6.41838077e-20, 2.40394110e-20, 9.00372384e-21,\n       3.37225580e-21, 1.26304509e-21, 4.73061057e-22])\n\n\n\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_poly, y)\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression(fit_intercept=False)\n\n\n\nplt.scatter(X, y)                                    # plot the samples\nplt.plot(plt_xs, true_func(plt_xs), 'blue', label='True')                          # plot the true function\nplt.plot(plt_xs, model.predict(poly.polyvander(plt_xs, deg=n)), 'r', label=\"Predicted\") # plot the fit model\nplt.ylim([-5, 5])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRidge regression with polynomial basis\n\n# Fit a linear model\ndef fit_and_plot(vander, n, alpha):\n  model = Ridge(fit_intercept=False, alpha=alpha)\n  model.fit(vander(X, deg=n), y)\n\n  plt.scatter(X, y)                           # plot the samples\n  plt.plot(plt_xs, true_func(plt_xs), 'blue')                 # plot the true function\n  plt.plot(plt_xs, model.predict(vander(plt_xs, deg=n)), 'r') # plot the fit model\n  plt.ylim([-5, 5])\n  plt.show()\n\n\nfit_and_plot(poly.polyvander, n=50, alpha=1e-7)\n\n\n\n\n\n\n\n\n\n\n2. Chebyshev basis\n\ndef scaled_chebvander(x, deg):\n  return cheb.chebvander(2 * x - 1, deg=deg)\n\n\nfit_and_plot(scaled_chebvander, n=50, alpha=1)\n\n\n\n\n\n\n\n\n\nfit_and_plot(scaled_chebvander, n=50, alpha=10)\n\n\n\n\n\n\n\n\n\n\n3. Legendre basis\n\ndef scaled_legvander(x, deg):\n  return leg.legvander(2 * x - 1, deg=deg)\n\n\nfit_and_plot(scaled_legvander, n=50, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n4. Bernstein basis\n\ndef bernvander(x, deg):\n    return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n\n\nfit_and_plot(bernvander, n=50, alpha=0)\n\nc:\\Users\\ryees\\anaconda3\\envs\\pml\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:255: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\nfit_and_plot(bernvander, n=50, alpha=5e-4)\n\n\n\n\n\n\n\n\n\nfit_and_plot(bernvander, n=100, alpha=5e-4)"
  },
  {
    "objectID": "notebooks/entropy.html",
    "href": "notebooks/entropy.html",
    "title": "Decision Trees Entropy",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\nfrom scipy.special import xlogy\n\n# Function to calculate entropy\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Generate data\nx_values = np.linspace(0.000, 1.0, 100)  # Avoid log(0) in the calculation\ny_values = entropy(x_values)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: divide by zero encountered in log2\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: invalid value encountered in multiply\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\n\ny_values\n\narray([       nan, 0.08146203, 0.14257333, 0.19590927, 0.24414164,\n       0.28853851, 0.32984607, 0.36855678, 0.40502013, 0.43949699,\n       0.47218938, 0.50325833, 0.53283506, 0.56102849, 0.58793037,\n       0.61361902, 0.63816195, 0.66161791, 0.68403844, 0.70546904,\n       0.72595015, 0.74551784, 0.76420451, 0.78203929, 0.79904852,\n       0.81525608, 0.83068364, 0.84535094, 0.85927598, 0.87247521,\n       0.88496364, 0.89675502, 0.90786192, 0.91829583, 0.92806728,\n       0.93718586, 0.9456603 , 0.95349858, 0.9607079 , 0.96729478,\n       0.97326507, 0.97862399, 0.98337619, 0.98752571, 0.99107606,\n       0.99403021, 0.99639062, 0.99815923, 0.9993375 , 0.9999264 ,\n       0.9999264 , 0.9993375 , 0.99815923, 0.99639062, 0.99403021,\n       0.99107606, 0.98752571, 0.98337619, 0.97862399, 0.97326507,\n       0.96729478, 0.9607079 , 0.95349858, 0.9456603 , 0.93718586,\n       0.92806728, 0.91829583, 0.90786192, 0.89675502, 0.88496364,\n       0.87247521, 0.85927598, 0.84535094, 0.83068364, 0.81525608,\n       0.79904852, 0.78203929, 0.76420451, 0.74551784, 0.72595015,\n       0.70546904, 0.68403844, 0.66161791, 0.63816195, 0.61361902,\n       0.58793037, 0.56102849, 0.53283506, 0.50325833, 0.47218938,\n       0.43949699, 0.40502013, 0.36855678, 0.32984607, 0.28853851,\n       0.24414164, 0.19590927, 0.14257333, 0.08146203,        nan])\n\n\n\n# Replace NaN values with 0\ny_values = np.nan_to_num(y_values, nan=0.0)\n\n\nlatexify(columns=2)\n\n\nplt.plot(x_values, y_values, color='black')\n\n# Set labels and title\nplt.xlabel('$P(+)$')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. $P(+)$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/entropy.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Function to calculate entropy with numerical stability\ndef entropy_numerically_stable(p):\n    return (-xlogy(p, p) - xlogy(1 - p, 1 - p))/np.log(2)\n\ny_values = entropy_numerically_stable(x_values)\n\n\nplt.plot(x_values, y_values)\n\n\n\n\n\n\n\n\nHow does xlogy handle the corner case?\n\nxlogy??\n\nCall signature:  xlogy(*args, **kwargs)\nType:            ufunc\nString form:     &lt;ufunc 'xlogy'&gt;\nFile:            ~/miniconda3/lib/python3.9/site-packages/numpy/__init__.py\nDocstring:      \nxlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nxlogy(x, y, out=None)\n\nCompute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n\nParameters\n----------\nx : array_like\n    Multiplier\ny : array_like\n    Argument\nout : ndarray, optional\n    Optional output array for the function results\n\nReturns\n-------\nz : scalar or ndarray\n    Computed x*log(y)\n\nNotes\n-----\nThe log function used in the computation is the natural log.\n\n.. versionadded:: 0.13.0\n\nExamples\n--------\nWe can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n.. math::\n    L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i))\n\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.special import xlogy\n&gt;&gt;&gt; y = np.array([0, 1, 0, 1, 1, 0])\n&gt;&gt;&gt; y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\n&gt;&gt;&gt; n = len(y)\n&gt;&gt;&gt; loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\n&gt;&gt;&gt; loss /= n\n&gt;&gt;&gt; loss\n0.29597052165495025\n\nA lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilties are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.\nClass docstring:\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\n**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays."
  },
  {
    "objectID": "notebooks/dummy-baselines.html",
    "href": "notebooks/dummy-baselines.html",
    "title": "Comparison of Sophisticated vs Dummy Baseline ML Algorithms for Imbalanced Datasets",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nfrom latexify import latexify\n# retina\n%config InlineBackend.figure_format = 'retina'\n\nClassification\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Create an imbalanced dataset with two features\nX, y = make_blobs(\n    n_samples=[4500,500],\n    n_features=2,  # Use only two features\n    cluster_std=[4.0,4.0],\n    random_state=42\n)\n\nprint(len(y))\nprint(len(y[y==1]))\nprint(len(y[y==0]))\n\n5000\n500\n4500\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"Train set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\n\nTrain set shape: (4000, 2) (4000,)\nTest set shape: (1000, 2) (1000,)\n\n\n\ncolors = ['blue' if label == 0 else 'red' for label in y_train]\nlatexify(fig_width=7, fig_height=5)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=colors, alpha=0.8)\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.title('Training Data')\n\nText(0.5, 1.0, 'Training Data')\n\n\n\n\n\n\n\n\n\n\nprint(len(y_train[y_train==1]))\nprint(len(y_train[y_train==0]))\n\n393\n3607\n\n\n\nprint(len(y_test[y_test==1]))\nprint(len(y_test[y_test==0]))\n\n107\n893\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)\n\ny_pred = rf_classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Print the accuracy and F1 score\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1)\n\nAccuracy: 0.945\nF1 Score: 0.7208121827411167\n\n\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_classifier = DummyClassifier(strategy='stratified')\ndummy_classifier.fit(X_train, y_train)\n\ny_pred_dummy = dummy_classifier.predict(X_test)\naccuracy_dummy = accuracy_score(y_test, y_pred_dummy)\nf1_dummy = f1_score(y_test, y_pred_dummy)\n\n# Print the accuracy and F1 score for the dummy classifier\nprint(\"Dummy Classifier Accuracy:\", accuracy_dummy)\nprint(\"Dummy Classifier F1 Score:\", f1_dummy)\n\nDummy Classifier Accuracy: 0.81\nDummy Classifier F1 Score: 0.07766990291262137\n\n\nRegression\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.dummy import DummyRegressor\n\n# Generate synthetic regression dataset with noise\nnp.random.seed(42)\nX = np.linspace(0, 1, 500).reshape(-1, 1)\nslope = 1.2\ny_true = slope * X.squeeze()\ny = y_true + np.random.normal(scale=0.5, size=len(X))\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"Train set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\n\nTrain set shape: (400, 1) (400,)\nTest set shape: (100, 1) (100,)\n\n\n\n# Scatter plot of the training data\nplt.scatter(X_train, y_train, color='red', alpha=0.8, label='Actual')\nplt.plot(X, y_true, color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.legend()\nplt.title('Training Data')\n\nText(0.5, 1.0, 'Training Data')\n\n\n\n\n\n\n\n\n\n\n# RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators=10, random_state=42)\nrf_regressor.fit(X_train, y_train)\n\ny_pred_rf = rf_regressor.predict(X_test)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\n\n# Print the Mean Squared Error for the RandomForestRegressor\nprint(\"Random Forest Regressor MSE:\", mse_rf)\n\nRandom Forest Regressor MSE: 0.3446175878909235\n\n\n\nplt.scatter(X_test, y_test, color='red', alpha=0.8, label='Actual')\nplt.scatter(X_test, y_pred_rf, color='black', alpha=0.8, label='Predicted')\nplt.plot(X_test, slope*X_test.squeeze(), color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('Predictions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# DummyRegressor\ndummy_regressor = DummyRegressor(strategy='mean')\ndummy_regressor.fit(X_train, y_train)\n\ny_pred_dummy = dummy_regressor.predict(X_test)\nmse_dummy = mean_squared_error(y_test, y_pred_dummy)\n\n# Print the Mean Squared Error for the DummyRegressor\nprint(\"Dummy Regressor MSE:\", mse_dummy)\n\nDummy Regressor MSE: 0.39550009552721027\n\n\n\nplt.scatter(X_test, y_test, color='red', alpha=0.8, label='Actual')\nplt.scatter(X_test, y_pred_dummy, color='black', alpha=0.8, label='Predicted')\nplt.plot(X_test, slope*X_test.squeeze(), color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('Predictions')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/SFS_and_BFS.html",
    "href": "notebooks/SFS_and_BFS.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score as acc\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.datasets import fetch_california_housing\n\n\n\nfrom mlxtend.feature_selection import Sequential\n\n\n\n# Read data\ndata =  fetch_california_housing()\nX = data['data']\ny = data['target']\n\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.25,\n    random_state=42)\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\nprint('Training dataset shape:', X_train.shape, y_train.shape)\nprint('Testing dataset shape:', X_test.shape, y_test.shape)\n\nTraining dataset shape: (15480, 8) (15480,)\nTesting dataset shape: (5160, 8) (5160,)\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsfs1 = sfs(clf,\n           k_features=6,\n           forward=True,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sfs1.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.1s finished\n\n[2020-01-18 12:14:41] Features: 1/6 -- score: -0.9799855743872914[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.4s finished\n\n[2020-01-18 12:14:42] Features: 2/6 -- score: -0.6329184295861372[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.6s finished\n\n[2020-01-18 12:14:44] Features: 3/6 -- score: -0.6522227062026185[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:45] Features: 4/6 -- score: -0.6627208539646012[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:47] Features: 5/6 -- score: -0.6800772168838566[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.7s finished\n\n[2020-01-18 12:14:49] Features: 6/6 -- score: -0.6988981779449276\n\n\n\nfeat_cols = list(sfs1.k_feature_idx_)\n\n# data['features']\n\n\nnp.array(data['feature_names'])[feat_cols]\n\narray(['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'Latitude',\n       'Longitude'], dtype='&lt;U10')\n\n\n\ndata['feature_names']\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsbs = sfs(clf,\n           k_features=1,\n           forward=False,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sbs.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    5.2s finished\n\n[2020-01-18 12:26:50] Features: 7/1 -- score: -0.7097202737310716[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    3.9s finished\n\n[2020-01-18 12:26:54] Features: 6/1 -- score: -0.6985180206966245[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.8s finished\n\n[2020-01-18 12:26:57] Features: 5/1 -- score: -0.6766309576585353[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n\n[2020-01-18 12:26:58] Features: 4/1 -- score: -0.6683220592138266[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.2s finished\n\n[2020-01-18 12:27:00] Features: 3/1 -- score: -0.6613854217987167[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n\n[2020-01-18 12:27:00] Features: 2/1 -- score: -0.6320510743499647[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n\n[2020-01-18 12:27:00] Features: 1/1 -- score: -0.9799855743872914"
  },
  {
    "objectID": "notebooks/siren.html",
    "href": "notebooks/siren.html",
    "title": "Sirens v/s Linear Regression",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Remove all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set env CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\ntry:\n    from einops import rearrange\nexcept ImportError:\n    %pip install einops\n    from einops import rearrange\n\nModuleNotFoundError: No module named 'torch'\n\n\n\nif os.path.exists('dog.jpg'):\n    print('dog.jpg exists')\nelse:\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg\n\ndog.jpg exists\n\n\n\n# Read in a image from torchvision\nimg = torchvision.io.read_image(\"dog.jpg\")\nprint(img.shape)\n\ntorch.Size([3, 1365, 2048])\n\n\n\nplt.imshow(rearrange(img, 'c h w -&gt; h w c').numpy())\n\n\n\n\n\n\n\n\n\nfrom sklearn import preprocessing\n\nscaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\nscaler_img\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nimg_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\nimg_scaled.shape\n\nimg_scaled = torch.tensor(img_scaled)\n\n\nimg_scaled = img_scaled.to(device)\nimg_scaled\n\ntensor([[[0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         [0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         [0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         ...,\n         [0.4745, 0.4745, 0.4784,  ..., 0.3804, 0.3765, 0.3765],\n         [0.4745, 0.4745, 0.4784,  ..., 0.3804, 0.3804, 0.3765],\n         [0.4745, 0.4745, 0.4784,  ..., 0.3843, 0.3804, 0.3804]],\n\n        [[0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         [0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         [0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         ...,\n         [0.4039, 0.4039, 0.4078,  ..., 0.3216, 0.3176, 0.3176],\n         [0.4039, 0.4039, 0.4078,  ..., 0.3216, 0.3216, 0.3176],\n         [0.4039, 0.4039, 0.4078,  ..., 0.3255, 0.3216, 0.3216]],\n\n        [[0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         [0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         [0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         ...,\n         [0.1451, 0.1451, 0.1490,  ..., 0.1686, 0.1647, 0.1647],\n         [0.1451, 0.1451, 0.1490,  ..., 0.1686, 0.1686, 0.1647],\n         [0.1451, 0.1451, 0.1490,  ..., 0.1725, 0.1686, 0.1686]]],\n       device='cuda:0', dtype=torch.float64)\n\n\n\ncrop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 800, 300, 300)\ncrop.shape\n\ntorch.Size([3, 300, 300])\n\n\n\nplt.imshow(rearrange(crop, 'c h w -&gt; h w c').cpu().numpy())\n\n\n\n\n\n\n\n\n\ncrop = crop.to(device)\n\n\n# Get the dimensions of the image tensor\nnum_channels, height, width = crop.shape\nprint(num_channels, height, width)\n\n3 300 300\n\n\n\nnum_channels, height, width = 2, 3, 4\n\n    \n# Create a 2D grid of (x,y) coordinates\nw_coords = torch.arange(width).repeat(height, 1)\nh_coords = torch.arange(height).repeat(width, 1).t()\nw_coords = w_coords.reshape(-1)\nh_coords = h_coords.reshape(-1)\n\n# Combine the x and y coordinates into a single tensor\nX = torch.stack([h_coords, w_coords], dim=1).float()\n\n\nX.shape\n\ntorch.Size([12, 2])\n\n\n\ndef create_coordinate_map(img):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n    \n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n    \n    num_channels, height, width = img.shape\n    \n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(width).repeat(height, 1)\n    h_coords = torch.arange(height).repeat(width, 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -&gt; (h w) c').float()\n    return X, Y\n\n\ndog_X, dog_Y = create_coordinate_map(crop)\n\ndog_X.shape, dog_Y.shape\n\n(torch.Size([90000, 2]), torch.Size([90000, 3]))\n\n\n\n# MinMaxScaler from -1 to 1\nscaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n\n# Scale the X coordinates\ndog_X_scaled = scaler_X.transform(dog_X.cpu())\n\n# Move the scaled X coordinates to the GPU\ndog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n\n# Set to dtype float32\ndog_X_scaled = dog_X_scaled.float()\n\n\nclass LinearModel(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearModel, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, x):\n        return self.linear(x)\n    \n\n\nnet = LinearModel(2, 3)\nnet.to(device)\n\nLinearModel(\n  (linear): Linear(in_features=2, out_features=3, bias=True)\n)\n\n\n\ndef train(net, lr, X, Y, epochs, verbose=True):\n    \"\"\"\n    net: torch.nn.Module\n    lr: float\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = net(X)\n        \n        \n        loss = criterion(outputs, Y)\n        loss.backward()\n        optimizer.step()\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n    return loss.item()\n\n\ntrain(net, 0.01, dog_X_scaled, dog_Y, 1000)\n\nEpoch 0 loss: 0.504670\nEpoch 100 loss: 0.046783\nEpoch 200 loss: 0.036839\nEpoch 300 loss: 0.036823\nEpoch 400 loss: 0.036823\nEpoch 500 loss: 0.036823\nEpoch 600 loss: 0.036823\nEpoch 700 loss: 0.036823\nEpoch 800 loss: 0.036823\nEpoch 900 loss: 0.036823\n\n\n0.03682254999876022\n\n\n\ndef plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n    \"\"\"\n    net: torch.nn.Module\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    num_channels, height, width = original_img.shape\n    net.eval()\n    with torch.no_grad():\n        outputs = net(X)\n        outputs = outputs.reshape(height, width, num_channels)\n        #outputs = outputs.permute(1, 2, 0)\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n\n    ax0.imshow(outputs.cpu())\n    ax0.set_title(\"Reconstructed Image\")\n    \n\n    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n    ax1.set_title(\"Original Image\")\n    \n    for a in [ax0, ax1]:\n        a.axis(\"off\")\n\n\n    fig.suptitle(title, y=0.9)\n    plt.tight_layout()\n\n\nplot_reconstructed_and_original_image(crop, net, dog_X_scaled, title=\"Reconstructed Image\")\n\n\n\n\n\n\n\n\n\n# Use polynomial features of degree \"d\"\n\ndef poly_features(X, degree):\n    \"\"\"\n    X: torch.Tensor of shape (num_samples, 2)\n    degree: int\n    \n    return: torch.Tensor of shape (num_samples, degree * (degree + 1) / 2)\n    \"\"\"\n    X1 = X[:, 0]\n    X2 = X[:, 1]\n    X1 = X1.unsqueeze(1)\n    X2 = X2.unsqueeze(1)\n    X = torch.cat([X1, X2], dim=1)\n    poly = preprocessing.PolynomialFeatures(degree=degree)\n    X = poly.fit_transform(X.cpu())\n    return torch.tensor(X, dtype=torch.float32).to(device)\n\n\ndog_X_scaled_poly = poly_features(dog_X_scaled, 50)\n\n\ndog_X_scaled_poly.dtype, dog_X_scaled_poly.shape, dog_Y.shape, dog_Y.dtype\n\n(torch.float32,\n torch.Size([90000, 1326]),\n torch.Size([90000, 3]),\n torch.float32)\n\n\n\nnet = LinearModel(dog_X_scaled_poly.shape[1], 3)\nnet.to(device)\n\ntrain(net, 0.005, dog_X_scaled_poly, dog_Y, 1500)\n\nEpoch 0 loss: 0.353235\nEpoch 100 loss: 0.028444\nEpoch 200 loss: 0.025136\nEpoch 300 loss: 0.024183\nEpoch 400 loss: 0.023526\nEpoch 500 loss: 0.023012\nEpoch 600 loss: 0.022591\nEpoch 700 loss: 0.022229\nEpoch 800 loss: 0.021912\nEpoch 900 loss: 0.021658\nEpoch 1000 loss: 0.021389\nEpoch 1100 loss: 0.021166\nEpoch 1200 loss: 0.020970\nEpoch 1300 loss: 0.020785\nEpoch 1400 loss: 0.020631\n\n\n0.020467281341552734\n\n\n\nplot_reconstructed_and_original_image(crop, net, dog_X_scaled_poly, title=\"Reconstructed Image with Polynomial Features\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n# create RFF features\ndef create_rff_features(X, num_features, sigma):\n    from sklearn.kernel_approximation import RBFSampler\n    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n    X = X.cpu().numpy()\n    X = rff.fit_transform(X)\n    return torch.tensor(X, dtype=torch.float32).to(device)\n\n\nX_rff = create_rff_features(dog_X_scaled, 37500, 0.008)\n\n\nX_rff.shape\n\ntorch.Size([90000, 37500])\n\n\n\nnet = LinearModel(X_rff.shape[1], 3)\nnet.to(device)\n\ntrain(net, 0.005, X_rff, dog_Y, 2500)\n\nEpoch 0 loss: 0.375324\nEpoch 100 loss: 0.047630\nEpoch 200 loss: 0.009158\nEpoch 300 loss: 0.003941\nEpoch 400 loss: 0.002057\nEpoch 500 loss: 0.001118\nEpoch 600 loss: 0.000640\nEpoch 700 loss: 0.000404\nEpoch 800 loss: 0.000292\nEpoch 900 loss: 0.000241\nEpoch 1000 loss: 0.000218\nEpoch 1100 loss: 0.000208\nEpoch 1200 loss: 0.000204\nEpoch 1300 loss: 0.000201\nEpoch 1400 loss: 0.000200\nEpoch 1500 loss: 0.000199\nEpoch 1600 loss: 0.000198\nEpoch 1700 loss: 0.000197\nEpoch 1800 loss: 0.000196\nEpoch 1900 loss: 0.000195\nEpoch 2000 loss: 0.000195\nEpoch 2100 loss: 0.000194\nEpoch 2200 loss: 0.000194\nEpoch 2300 loss: 0.000193\nEpoch 2400 loss: 0.000193\n\n\n0.00019248582248110324\n\n\n\nplot_reconstructed_and_original_image(crop, net, X_rff, title=\"Reconstructed Image with RFF Features\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\nw = 100\nscale=2\ntorch.arange(0, w, 1/scale)\n\ntensor([ 0.0000,  0.5000,  1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,\n         4.0000,  4.5000,  5.0000,  5.5000,  6.0000,  6.5000,  7.0000,  7.5000,\n         8.0000,  8.5000,  9.0000,  9.5000, 10.0000, 10.5000, 11.0000, 11.5000,\n        12.0000, 12.5000, 13.0000, 13.5000, 14.0000, 14.5000, 15.0000, 15.5000,\n        16.0000, 16.5000, 17.0000, 17.5000, 18.0000, 18.5000, 19.0000, 19.5000,\n        20.0000, 20.5000, 21.0000, 21.5000, 22.0000, 22.5000, 23.0000, 23.5000,\n        24.0000, 24.5000, 25.0000, 25.5000, 26.0000, 26.5000, 27.0000, 27.5000,\n        28.0000, 28.5000, 29.0000, 29.5000, 30.0000, 30.5000, 31.0000, 31.5000,\n        32.0000, 32.5000, 33.0000, 33.5000, 34.0000, 34.5000, 35.0000, 35.5000,\n        36.0000, 36.5000, 37.0000, 37.5000, 38.0000, 38.5000, 39.0000, 39.5000,\n        40.0000, 40.5000, 41.0000, 41.5000, 42.0000, 42.5000, 43.0000, 43.5000,\n        44.0000, 44.5000, 45.0000, 45.5000, 46.0000, 46.5000, 47.0000, 47.5000,\n        48.0000, 48.5000, 49.0000, 49.5000, 50.0000, 50.5000, 51.0000, 51.5000,\n        52.0000, 52.5000, 53.0000, 53.5000, 54.0000, 54.5000, 55.0000, 55.5000,\n        56.0000, 56.5000, 57.0000, 57.5000, 58.0000, 58.5000, 59.0000, 59.5000,\n        60.0000, 60.5000, 61.0000, 61.5000, 62.0000, 62.5000, 63.0000, 63.5000,\n        64.0000, 64.5000, 65.0000, 65.5000, 66.0000, 66.5000, 67.0000, 67.5000,\n        68.0000, 68.5000, 69.0000, 69.5000, 70.0000, 70.5000, 71.0000, 71.5000,\n        72.0000, 72.5000, 73.0000, 73.5000, 74.0000, 74.5000, 75.0000, 75.5000,\n        76.0000, 76.5000, 77.0000, 77.5000, 78.0000, 78.5000, 79.0000, 79.5000,\n        80.0000, 80.5000, 81.0000, 81.5000, 82.0000, 82.5000, 83.0000, 83.5000,\n        84.0000, 84.5000, 85.0000, 85.5000, 86.0000, 86.5000, 87.0000, 87.5000,\n        88.0000, 88.5000, 89.0000, 89.5000, 90.0000, 90.5000, 91.0000, 91.5000,\n        92.0000, 92.5000, 93.0000, 93.5000, 94.0000, 94.5000, 95.0000, 95.5000,\n        96.0000, 96.5000, 97.0000, 97.5000, 98.0000, 98.5000, 99.0000, 99.5000])\n\n\n\ndef create_coordinate_map(img, scale=1):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n    \n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n    \n    num_channels, height, width = img.shape\n    \n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(0, width,  1/scale).repeat(int(height*scale), 1)\n    h_coords = torch.arange(0, height, 1/scale).repeat(int(width*scale), 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -&gt; (h w) c').float()\n    return X, Y\n\n\ncreate_coordinate_map(crop, scale=2)[0].shape\n\ntorch.Size([360000, 2])\n\n\n\ncreate_coordinate_map(crop, scale=1)[0].shape\n\ntorch.Size([90000, 2])"
  },
  {
    "objectID": "notebooks/names.html",
    "href": "notebooks/names.html",
    "title": "Generating names using MLPs",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfrom pprint import pprint\n\n\ntorch.__version__\n\n'2.0.1'\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice\n\ndevice(type='cpu')\n\n\n\n# Get some names from https://github.com/MASTREX/List-of-Indian-Names\n\n\n!wget https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv -O names-long.csv\n\n--2024-03-07 11:57:26--  https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 85538 (84K) [text/plain]\nSaving to: names-long.csv\n\nnames-long.csv      100%[===================&gt;]  83.53K  --.-KB/s    in 0.08s   \n\n2024-03-07 11:57:26 (1.08 MB/s) - names-long.csv saved [85538/85538]\n\n\n\n\n!head names-long.csv\n\n,Name\n0,aabid\n1,aabida\n2,aachal\n3,aadesh\n4,aadil\n5,aadish\n6,aaditya\n7,aaenab\n8,aafreen\n\n\n\n!tail names-long.csv\n\n6476,zeshan\n6477,zhini\n6478,ziarul\n6479,zile\n6480,zina\n6481,zishan\n6482,ziyabul\n6483,zoya\n6484,zuhaib\n6485,zuveb\n\n\n\nwords = pd.read_csv('names-long.csv')[\"Name\"]\nwords = words.str.lower()\nwords = words.str.strip()\nwords = words.str.replace(\" \", \"\")\n\nwords = words[words.str.len() &gt; 2]\nwords = words[words.str.len() &lt; 10]\n\n# Randomly shuffle the words\nwords = words.sample(frac=1).reset_index(drop=True)\nwords = words.tolist()\n\n# Remove words having non alphabets\nwords = [word for word in words if word.isalpha()]\nwords[:10]\n\n['diti',\n 'bajrang',\n 'sanjya',\n 'nain',\n 'mahrul',\n 'shib',\n 'jashgul',\n 'zeba',\n 'jaishmin',\n 'somil']\n\n\n\nlen(words)\n\n6184\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\npprint(itos)\n\n{0: '.',\n 1: 'a',\n 2: 'b',\n 3: 'c',\n 4: 'd',\n 5: 'e',\n 6: 'f',\n 7: 'g',\n 8: 'h',\n 9: 'i',\n 10: 'j',\n 11: 'k',\n 12: 'l',\n 13: 'm',\n 14: 'n',\n 15: 'o',\n 16: 'p',\n 17: 'q',\n 18: 'r',\n 19: 's',\n 20: 't',\n 21: 'u',\n 22: 'v',\n 23: 'w',\n 24: 'x',\n 25: 'y',\n 26: 'z'}\n\n\n\nblock_size = 5 # context length: how many characters do we take to predict the next one?\nX, Y = [], []\nfor w in words[:]:\n  \n  #print(w)\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n    context = context[1:] + [ix] # crop and append\n  \n# Move data to GPU\n\nX = torch.tensor(X).to(device)\nY = torch.tensor(Y).to(device)\n\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; t\n..dit ---&gt; i\n.diti ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; r\n.bajr ---&gt; a\nbajra ---&gt; n\najran ---&gt; g\njrang ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; y\nsanjy ---&gt; a\nanjya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; n\n.nain ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; r\n.mahr ---&gt; u\nmahru ---&gt; l\nahrul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; b\n.shib ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; h\n.jash ---&gt; g\njashg ---&gt; u\nashgu ---&gt; l\nshgul ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; b\n..zeb ---&gt; a\n.zeba ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; s\n.jais ---&gt; h\njaish ---&gt; m\naishm ---&gt; i\nishmi ---&gt; n\nshmin ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; i\n.somi ---&gt; l\nsomil ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; o\nhario ---&gt; m\nariom ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; u\n.faru ---&gt; k\nfaruk ---&gt; h\narukh ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; p\n.gurp ---&gt; r\ngurpr ---&gt; e\nurpre ---&gt; e\nrpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; h\n..ruh ---&gt; h\n.ruhh ---&gt; e\nruhhe ---&gt; n\nuhhen ---&gt; a\nhhena ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; n\n..kon ---&gt; i\n.koni ---&gt; k\nkonik ---&gt; a\nonika ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; s\ngiris ---&gt; h\nirish ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; a\nmitha ---&gt; l\nithal ---&gt; e\nthale ---&gt; s\nhales ---&gt; h\nalesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; j\n.kamj ---&gt; e\nkamje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; y\n..piy ---&gt; a\n.piya ---&gt; r\npiyar ---&gt; i\niyari ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; a\n.dama ---&gt; n\ndaman ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; l\nrupal ---&gt; i\nupali ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; p\n.gurp ---&gt; r\ngurpr ---&gt; i\nurpri ---&gt; t\nrprit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; d\nsamad ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; e\nmohme ---&gt; d\nohmed ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; u\n.beeu ---&gt; t\nbeeut ---&gt; y\neeuty ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; i\n.aaki ---&gt; b\naakib ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; a\n..aha ---&gt; m\n.aham ---&gt; a\nahama ---&gt; d\nhamad ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; u\n.hemu ---&gt; n\nhemun ---&gt; a\nemuna ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; k\n.mook ---&gt; a\nmooka ---&gt; n\nookan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; e\nkalpe ---&gt; s\nalpes ---&gt; h\nlpesh ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; u\nshabu ---&gt; d\nhabud ---&gt; d\nabudd ---&gt; i\nbuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; i\n.vaki ---&gt; l\nvakil ---&gt; a\nakila ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; f\nlshaf ---&gt; a\nshafa ---&gt; .\n..... ---&gt; o\n....o ---&gt; n\n...on ---&gt; g\n..ong ---&gt; i\n.ongi ---&gt; n\nongin ---&gt; y\nnginy ---&gt; e\nginye ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; e\nvende ---&gt; r\nender ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; n\n.teen ---&gt; u\nteenu ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; r\n.bhor ---&gt; e\nbhore ---&gt; l\nhorel ---&gt; a\norela ---&gt; l\nrelal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; s\n..sos ---&gt; a\n.sosa ---&gt; r\nsosar ---&gt; i\nosari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; f\n.shaf ---&gt; i\nshafi ---&gt; b\nhafib ---&gt; u\nafibu ---&gt; l\nfibul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; j\n..shj ---&gt; a\n.shja ---&gt; d\nshjad ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; s\nharis ---&gt; h\narish ---&gt; e\nrishe ---&gt; n\nishen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; n\nsaran ---&gt; t\narant ---&gt; h\nranth ---&gt; e\nanthe ---&gt; m\nnthem ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; h\nruksh ---&gt; a\nuksha ---&gt; n\nkshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; u\nrabhu ---&gt; .\n..... ---&gt; e\n....e ---&gt; d\n...ed ---&gt; r\n..edr ---&gt; i\n.edri ---&gt; s\nedris ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; e\n.kame ---&gt; s\nkames ---&gt; h\namesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; c\n.sakc ---&gt; h\nsakch ---&gt; a\nakcha ---&gt; n\nkchan ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; i\n.dali ---&gt; p\ndalip ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; r\n..adr ---&gt; a\n.adra ---&gt; s\nadras ---&gt; h\ndrash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; i\nanjai ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; e\n.aaye ---&gt; s\naayes ---&gt; h\nayesh ---&gt; a\nyesha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; a\nprama ---&gt; t\nramat ---&gt; m\namatm ---&gt; a\nmatma ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; u\nanupu ---&gt; m\nnupum ---&gt; a\nupuma ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; i\n.gopi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; b\naseeb ---&gt; a\nseeba ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; u\nsharu ---&gt; k\nharuk ---&gt; h\narukh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; s\n.murs ---&gt; h\nmursh ---&gt; i\nurshi ---&gt; d\nrshid ---&gt; a\nshida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; v\nsanav ---&gt; v\nanavv ---&gt; a\nnavva ---&gt; r\navvar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; n\n..ron ---&gt; a\n.rona ---&gt; k\nronak ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; r\n.nazr ---&gt; i\nnazri ---&gt; n\nazrin ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; n\n.mahn ---&gt; a\nmahna ---&gt; t\nahnat ---&gt; h\nhnath ---&gt; a\nnatha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; n\nhawan ---&gt; a\nawana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; k\njanak ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; d\n.bhad ---&gt; d\nbhadd ---&gt; a\nhadda ---&gt; l\naddal ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; a\n.bega ---&gt; m\nbegam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; a\nparta ---&gt; b\nartab ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; i\n.mehi ---&gt; b\nmehib ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; j\n.benj ---&gt; i\nbenji ---&gt; r\nenjir ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; a\n.vasa ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; v\n..gov ---&gt; i\n.govi ---&gt; n\ngovin ---&gt; d\novind ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; d\n..jed ---&gt; u\n.jedu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; e\n.nage ---&gt; n\nnagen ---&gt; d\nagend ---&gt; r\ngendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; i\namari ---&gt; n\nmarin ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; j\n.dalj ---&gt; e\ndalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; a\nausha ---&gt; r\nushar ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; t\nemant ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; w\n.dhaw ---&gt; a\ndhawa ---&gt; l\nhawal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; f\n.mahf ---&gt; u\nmahfu ---&gt; j\nahfuj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; t\nudhat ---&gt; a\ndhata ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; i\nhushi ---&gt; l\nushil ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; c\n..inc ---&gt; e\n.ince ---&gt; e\nincee ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; a\n.mada ---&gt; m\nmadam ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; i\nramji ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; j\n..roj ---&gt; m\n.rojm ---&gt; e\nrojme ---&gt; r\nojmer ---&gt; i\njmeri ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; l\n.tril ---&gt; o\ntrilo ---&gt; c\nriloc ---&gt; h\niloch ---&gt; a\nlocha ---&gt; n\nochan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; r\nsamer ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; s\n..yos ---&gt; h\n.yosh ---&gt; o\nyosho ---&gt; d\noshod ---&gt; a\nshoda ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; t\n.atit ---&gt; a\natita ---&gt; j\ntitaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; w\n.kanw ---&gt; a\nkanwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; m\n.bism ---&gt; i\nbismi ---&gt; l\nismil ---&gt; l\nsmill ---&gt; a\nmilla ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; i\n.jahi ---&gt; r\njahir ---&gt; u\nahiru ---&gt; l\nhirul ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; n\ndhann ---&gt; i\nhanni ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; l\n.manl ---&gt; i\nmanli ---&gt; s\nanlis ---&gt; a\nnlisa ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; c\n..jac ---&gt; k\n.jack ---&gt; y\njacky ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; y\n..jiy ---&gt; a\n.jiya ---&gt; u\njiyau ---&gt; l\niyaul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; y\nrabiy ---&gt; a\nabiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; l\n..anl ---&gt; i\n.anli ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; b\nkarmb ---&gt; i\narmbi ---&gt; r\nrmbir ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; c\n.nenc ---&gt; y\nnency ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; n\n..avn ---&gt; i\n.avni ---&gt; s\navnis ---&gt; h\nvnish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; u\n.sagu ---&gt; f\nsaguf ---&gt; t\naguft ---&gt; a\ngufta ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; r\n.heer ---&gt; a\nheera ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; i\nhishi ---&gt; e\nishie ---&gt; k\nshiek ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; t\n.ramt ---&gt; e\nramte ---&gt; k\namtek ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; b\n.balb ---&gt; e\nbalbe ---&gt; e\nalbee ---&gt; r\nlbeer ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; a\nbhara ---&gt; t\nharat ---&gt; l\naratl ---&gt; a\nratla ---&gt; l\natlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; u\n.biru ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; e\n.vine ---&gt; t\nvinet ---&gt; a\nineta ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; b\nsukhb ---&gt; e\nukhbe ---&gt; e\nkhbee ---&gt; r\nhbeer ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; a\nausha ---&gt; l\nushal ---&gt; y\nshaly ---&gt; a\nhalya ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; t\n..ket ---&gt; a\n.keta ---&gt; n\nketan ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; w\n.pusw ---&gt; a\npuswa ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; n\nvijen ---&gt; d\nijend ---&gt; a\njenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; a\nhasha ---&gt; i\nashai ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; l\n.meel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; w\n.rizw ---&gt; a\nrizwa ---&gt; a\nizwaa ---&gt; n\nzwaan ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; i\nlakhi ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; a\nindra ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; h\nhusbh ---&gt; u\nusbhu ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; d\n.abid ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; j\nhailj ---&gt; a\nailja ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; p\n.ranp ---&gt; a\nranpa ---&gt; l\nanpal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; p\n.ganp ---&gt; a\nganpa ---&gt; t\nanpat ---&gt; i\nnpati ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; b\nsukhb ---&gt; i\nukhbi ---&gt; r\nkhbir ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; l\nbabul ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; a\n.musa ---&gt; r\nmusar ---&gt; r\nusarr ---&gt; a\nsarra ---&gt; t\narrat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; e\nratne ---&gt; s\natnes ---&gt; w\ntnesw ---&gt; a\nneswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; c\n.dalc ---&gt; h\ndalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; r\nnazir ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; v\n..luv ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; h\narash ---&gt; u\nrashu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; k\namrik ---&gt; .\n..... ---&gt; g\n....g ---&gt; j\n...gj ---&gt; e\n..gje ---&gt; n\n.gjen ---&gt; d\ngjend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; a\n.bija ---&gt; n\nbijan ---&gt; d\nijand ---&gt; e\njande ---&gt; r\nander ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; g\n..asg ---&gt; a\n.asga ---&gt; r\nasgar ---&gt; i\nsgari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; j\nsharj ---&gt; p\nharjp ---&gt; r\narjpr ---&gt; i\nrjpri ---&gt; t\njprit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; b\n..aab ---&gt; i\n.aabi ---&gt; d\naabid ---&gt; a\nabida ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; d\n.hand ---&gt; u\nhandu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; b\n.jagb ---&gt; i\njagbi ---&gt; r\nagbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; l\n.raml ---&gt; i\nramli ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; a\n.vima ---&gt; l\nvimal ---&gt; a\nimala ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; e\n.shre ---&gt; e\nshree ---&gt; e\nhreee ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; k\n.devk ---&gt; i\ndevki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; a\njamaa ---&gt; l\namaal ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; p\nnderp ---&gt; a\nderpa ---&gt; l\nerpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; l\n.bhul ---&gt; i\nbhuli ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; d\n.vard ---&gt; a\nvarda ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; i\n.jani ---&gt; d\njanid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; o\nantro ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; r\n..fur ---&gt; k\n.furk ---&gt; h\nfurkh ---&gt; a\nurkha ---&gt; n\nrkhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; a\nramka ---&gt; r\namkar ---&gt; a\nmkara ---&gt; n\nkaran ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; r\nlakhr ---&gt; a\nakhra ---&gt; j\nkhraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; p\n.jasp ---&gt; a\njaspa ---&gt; l\naspal ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; k\n.tink ---&gt; u\ntinku ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; o\n.niro ---&gt; s\nniros ---&gt; h\nirosh ---&gt; a\nrosha ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; j\n.khaj ---&gt; a\nkhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; n\nkiran ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; a\nbinda ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; m\n..dum ---&gt; n\n.dumn ---&gt; i\ndumni ---&gt; k\numnik ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; f\n.shaf ---&gt; i\nshafi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; l\n..sel ---&gt; a\n.sela ---&gt; r\nselar ---&gt; s\nelars ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; u\n.begu ---&gt; m\nbegum ---&gt; p\negump ---&gt; u\ngumpu ---&gt; r\numpur ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; a\n.kisa ---&gt; n\nkisan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; t\n.kant ---&gt; a\nkanta ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; m\n.birm ---&gt; a\nbirma ---&gt; d\nirmad ---&gt; e\nrmade ---&gt; v\nmadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; j\n....j ---&gt; g\n...jg ---&gt; d\n..jgd ---&gt; i\n.jgdi ---&gt; s\njgdis ---&gt; h\ngdish ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; b\n..akb ---&gt; a\n.akba ---&gt; r\nakbar ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; j\n.dalj ---&gt; i\ndalji ---&gt; t\naljit ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; a\n.jaya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; l\n.harl ---&gt; e\nharle ---&gt; e\narlee ---&gt; n\nrleen ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; s\nlakhs ---&gt; m\nakhsm ---&gt; i\nkhsmi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; a\n.muda ---&gt; s\nmudas ---&gt; a\nudasa ---&gt; r\ndasar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; t\n.ratt ---&gt; a\nratta ---&gt; n\nattan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; i\nmithi ---&gt; l\nithil ---&gt; e\nthile ---&gt; s\nhiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; r\n.mukr ---&gt; a\nmukra ---&gt; m\nukram ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; n\ntaran ---&gt; n\narann ---&gt; u\nrannu ---&gt; m\nannum ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; l\n.kosl ---&gt; y\nkosly ---&gt; a\noslya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; t\n..sot ---&gt; a\n.sota ---&gt; j\nsotaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; a\nsarda ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; u\n.ashu ---&gt; t\nashut ---&gt; o\nshuto ---&gt; s\nhutos ---&gt; h\nutosh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; v\natyav ---&gt; a\ntyava ---&gt; t\nyavat ---&gt; i\navati ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; p\nbabup ---&gt; u\nabupu ---&gt; r\nbupur ---&gt; i\nupuri ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; h\ntulsh ---&gt; a\nulsha ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; n\n.junn ---&gt; a\njunna ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; p\n..nup ---&gt; u\n.nupu ---&gt; r\nnupur ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; s\n.anis ---&gt; h\nanish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; e\n....e ---&gt; s\n...es ---&gt; r\n..esr ---&gt; a\n.esra ---&gt; i\nesrai ---&gt; l\nsrail ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; i\nsohai ---&gt; b\nohaib ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; o\nhhano ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; d\n.maad ---&gt; h\nmaadh ---&gt; u\naadhu ---&gt; r\nadhur ---&gt; i\ndhuri ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; w\n.harw ---&gt; i\nharwi ---&gt; n\narwin ---&gt; d\nrwind ---&gt; r\nwindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; n\n..gon ---&gt; a\n.gona ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; v\n..mev ---&gt; a\n.meva ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; l\n.phol ---&gt; w\npholw ---&gt; a\nholwa ---&gt; t\nolwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; k\nabhak ---&gt; i\nbhaki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; i\n.javi ---&gt; d\njavid ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; i\n.risi ---&gt; r\nrisir ---&gt; a\nisira ---&gt; j\nsiraj ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; r\n.deer ---&gt; e\ndeere ---&gt; n\neeren ---&gt; d\nerend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; n\n.navn ---&gt; e\nnavne ---&gt; e\navnee ---&gt; t\nvneet ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; u\n.ansu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; t\n.shet ---&gt; a\nsheta ---&gt; n\nhetan ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; j\n.barj ---&gt; e\nbarje ---&gt; s\narjes ---&gt; h\nrjesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; r\nhabir ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; p\n.veep ---&gt; a\nveepa ---&gt; l\neepal ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; e\n.pune ---&gt; e\npunee ---&gt; t\nuneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; e\n.sule ---&gt; n\nsulen ---&gt; d\nulend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; e\n..awe ---&gt; d\n.awed ---&gt; h\nawedh ---&gt; e\nwedhe ---&gt; s\nedhes ---&gt; h\ndhesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; e\n..ale ---&gt; m\n.alem ---&gt; a\nalema ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; z\nshahz ---&gt; a\nhahza ---&gt; d\nahzad ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; h\nbhudh ---&gt; i\nhudhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; e\n.vike ---&gt; s\nvikes ---&gt; h\nikesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; k\n..amk ---&gt; i\n.amki ---&gt; t\namkit ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; e\nsidhe ---&gt; s\nidhes ---&gt; w\ndhesw ---&gt; a\nheswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; y\nprity ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; p\n.jagp ---&gt; a\njagpa ---&gt; t\nagpat ---&gt; i\ngpati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; d\n.sajd ---&gt; a\nsajda ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; n\nroshn ---&gt; i\noshni ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; h\n.math ---&gt; u\nmathu ---&gt; r\nathur ---&gt; a\nthura ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; r\n.patr ---&gt; a\npatra ---&gt; s\natras ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; c\n.bacc ---&gt; h\nbacch ---&gt; u\nacchu ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; t\n.lalt ---&gt; i\nlalti ---&gt; a\naltia ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; i\n.basi ---&gt; r\nbasir ---&gt; a\nasira ---&gt; n\nsiran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; n\nsahin ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; l\n..sel ---&gt; v\n.selv ---&gt; a\nselva ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; s\nantos ---&gt; h\nntosh ---&gt; i\ntoshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; h\n.saah ---&gt; i\nsaahi ---&gt; l\naahil ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; t\n..vit ---&gt; h\n.vith ---&gt; a\nvitha ---&gt; l\nithal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; o\ngango ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; i\n.muki ---&gt; s\nmukis ---&gt; h\nukish ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; y\nampay ---&gt; a\nmpaya ---&gt; r\npayar ---&gt; i\nayari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; a\nshama ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; t\n..git ---&gt; a\n.gita ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; d\n.devd ---&gt; h\ndevdh ---&gt; a\nevdha ---&gt; r\nvdhar ---&gt; i\ndhari ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; w\n.kulw ---&gt; a\nkulwa ---&gt; n\nulwan ---&gt; t\nlwant ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; m\n.jhum ---&gt; k\njhumk ---&gt; i\nhumki ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; v\n..gov ---&gt; i\n.govi ---&gt; n\ngovin ---&gt; d\novind ---&gt; a\nvinda ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; a\nrabha ---&gt; s\nabhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; z\n.sehz ---&gt; a\nsehza ---&gt; d\nehzad ---&gt; a\nhzada ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; a\n.nira ---&gt; n\nniran ---&gt; j\niranj ---&gt; a\nranja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; b\nveerb ---&gt; h\neerbh ---&gt; a\nerbha ---&gt; n\nrbhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; c\n..ric ---&gt; h\n.rich ---&gt; a\nricha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; m\nsamim ---&gt; a\namima ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; d\njamad ---&gt; a\namada ---&gt; r\nmadar ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; a\n.fira ---&gt; s\nfiras ---&gt; a\nirasa ---&gt; t\nrasat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; b\n.mahb ---&gt; i\nmahbi ---&gt; r\nahbir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; b\nshabb ---&gt; i\nhabbi ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; h\n..aph ---&gt; s\n.aphs ---&gt; a\naphsa ---&gt; n\nphsan ---&gt; a\nhsana ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; h\n.rekh ---&gt; a\nrekha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; i\nmbiri ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; a\n..ada ---&gt; l\n.adal ---&gt; a\nadala ---&gt; t\ndalat ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; i\n..isi ---&gt; k\n.isik ---&gt; a\nisika ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; a\n..bea ---&gt; u\n.beau ---&gt; t\nbeaut ---&gt; y\neauty ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; k\n.ruck ---&gt; m\nruckm ---&gt; a\nuckma ---&gt; n\nckman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; l\n.doll ---&gt; y\ndolly ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; t\n..yat ---&gt; h\n.yath ---&gt; a\nyatha ---&gt; r\nathar ---&gt; t\nthart ---&gt; h\nharth ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; k\nmohak ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; a\nrisha ---&gt; b\nishab ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; r\n..asr ---&gt; a\n.asra ---&gt; n\nasran ---&gt; i\nsrani ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; j\n..aaj ---&gt; i\n.aaji ---&gt; v\naajiv ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; a\nramka ---&gt; l\namkal ---&gt; i\nmkali ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; n\n.deen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; e\nramse ---&gt; m\namsem ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; p\n.dalp ---&gt; a\ndalpa ---&gt; t\nalpat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; e\nrakhe ---&gt; e\nakhee ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; r\nashar ---&gt; a\nshara ---&gt; n\nharan ---&gt; i\narani ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; n\n..jen ---&gt; a\n.jena ---&gt; b\njenab ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; i\n.anji ---&gt; l\nanjil ---&gt; a\nnjila ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; s\n.nees ---&gt; a\nneesa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; i\n.rasi ---&gt; d\nrasid ---&gt; a\nasida ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; m\n.bhim ---&gt; a\nbhima ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; g\n.jhag ---&gt; d\njhagd ---&gt; u\nhagdu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; u\nmariu ---&gt; m\narium ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; y\n.mary ---&gt; a\nmarya ---&gt; m\naryam ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; a\n..oma ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; n\nsohan ---&gt; l\nohanl ---&gt; a\nhanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; u\nmanju ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; t\n.bant ---&gt; y\nbanty ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; u\nnasru ---&gt; d\nasrud ---&gt; e\nsrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; v\n.rijv ---&gt; a\nrijva ---&gt; n\nijvan ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; m\n.bram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; r\nshrir ---&gt; a\nhrira ---&gt; m\nriram ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; c\n..joc ---&gt; k\n.jock ---&gt; y\njocky ---&gt; i\nockyi ---&gt; p\nckyip ---&gt; a\nkyipa ---&gt; i\nyipai ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; i\n.faki ---&gt; r\nfakir ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; a\n.pata ---&gt; s\npatas ---&gt; o\nataso ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; c\n..mic ---&gt; h\n.mich ---&gt; a\nmicha ---&gt; e\nichae ---&gt; l\nchael ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; i\n.biri ---&gt; j\nbirij ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; a\n.mama ---&gt; n\nmaman ---&gt; b\namanb ---&gt; a\nmanba ---&gt; i\nanbai ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; k\n..pak ---&gt; h\n.pakh ---&gt; a\npakha ---&gt; l\nakhal ---&gt; i\nkhali ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; j\n..aaj ---&gt; a\n.aaja ---&gt; d\naajad ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; t\nsurat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; u\n.sadu ---&gt; r\nsadur ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; a\n..pha ---&gt; r\n.phar ---&gt; j\npharj ---&gt; a\nharja ---&gt; n\narjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; n\nranjn ---&gt; a\nanjna ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; b\n.shub ---&gt; a\nshuba ---&gt; n\nhuban ---&gt; k\nubank ---&gt; a\nbanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; m\nrahim ---&gt; u\nahimu ---&gt; n\nhimun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; m\n.saim ---&gt; a\nsaima ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; d\n..avd ---&gt; e\n.avde ---&gt; s\navdes ---&gt; h\nvdesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; e\nbhure ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; k\ndivak ---&gt; a\nivaka ---&gt; r\nvakar ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; s\n.alis ---&gt; h\nalish ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; b\n..mab ---&gt; i\n.mabi ---&gt; y\nmabiy ---&gt; a\nabiya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; a\nharba ---&gt; s\narbas ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; p\n..bup ---&gt; a\n.bupa ---&gt; l\nbupal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; v\n..ruv ---&gt; i\n.ruvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; s\nkalus ---&gt; i\nalusi ---&gt; n\nlusin ---&gt; g\nusing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; a\n.nira ---&gt; j\nniraj ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; a\nramda ---&gt; s\namdas ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; k\nbalak ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; n\nhakun ---&gt; t\nakunt ---&gt; l\nkuntl ---&gt; a\nuntla ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; b\n.vaib ---&gt; h\nvaibh ---&gt; a\naibha ---&gt; v\nibhav ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; o\n..mho ---&gt; s\n.mhos ---&gt; i\nmhosi ---&gt; n\nhosin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; i\n.suri ---&gt; n\nsurin ---&gt; d\nurind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; a\nndera ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; i\nashmi ---&gt; n\nshmin ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; s\n.nans ---&gt; h\nnansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; k\neenak ---&gt; s\nenaks ---&gt; h\nnaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; i\nramni ---&gt; w\namniw ---&gt; a\nmniwa ---&gt; s\nniwas ---&gt; j\niwasj ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; a\nvasha ---&gt; n\nashan ---&gt; a\nshana ---&gt; v\nhanav ---&gt; i\nanavi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; d\n.somd ---&gt; a\nsomda ---&gt; t\nomdat ---&gt; h\nmdath ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; i\n.akhi ---&gt; l\nakhil ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; e\n.huse ---&gt; n\nhusen ---&gt; i\nuseni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; a\n.saga ---&gt; n\nsagan ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; o\n.fulo ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; m\n.khem ---&gt; c\nkhemc ---&gt; h\nhemch ---&gt; a\nemcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; l\n..asl ---&gt; a\n.asla ---&gt; m\naslam ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; o\n.nilo ---&gt; f\nnilof ---&gt; a\nilofa ---&gt; r\nlofar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; a\nhyama ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; m\nmohmm ---&gt; a\nohmma ---&gt; d\nhmmad ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; a\nprasa ---&gt; n\nrasan ---&gt; t\nasant ---&gt; a\nsanta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; i\n.fuli ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; s\nulfas ---&gt; a\nlfasa ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; t\nbhart ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; t\nshant ---&gt; y\nhanty ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; g\n.poog ---&gt; a\npooga ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; l\nrohil ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; i\nhishi ---&gt; k\nishik ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; d\n..bid ---&gt; u\n.bidu ---&gt; r\nbidur ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; r\n.char ---&gt; n\ncharn ---&gt; j\nharnj ---&gt; e\narnje ---&gt; e\nrnjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; e\n.akhe ---&gt; e\nakhee ---&gt; l\nkheel ---&gt; e\nheele ---&gt; s\neeles ---&gt; h\nelesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; u\n.dipu ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; n\n..chn ---&gt; a\n.chna ---&gt; d\nchnad ---&gt; a\nhnada ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; u\n.mamu ---&gt; n\nmamun ---&gt; i\namuni ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; e\n..ile ---&gt; m\n.ilem ---&gt; a\nilema ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; i\n.hazi ---&gt; r\nhazir ---&gt; a\nazira ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; i\n.doli ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; z\neenaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; l\n..isl ---&gt; a\n.isla ---&gt; m\nislam ---&gt; u\nslamu ---&gt; d\nlamud ---&gt; i\namudi ---&gt; n\nmudin ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; h\nvansh ---&gt; i\nanshi ---&gt; k\nnshik ---&gt; a\nshika ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; b\ngulab ---&gt; s\nulabs ---&gt; a\nlabsa ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; a\nmansa ---&gt; v\nansav ---&gt; i\nnsavi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; t\n.alit ---&gt; a\nalita ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; o\n.dolo ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; y\nsoniy ---&gt; a\noniya ---&gt; .\n..... ---&gt; p\n....p ---&gt; p\n...pp ---&gt; h\n..pph ---&gt; o\n.ppho ---&gt; l\npphol ---&gt; a\nphola ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; p\ndhanp ---&gt; a\nhanpa ---&gt; t\nanpat ---&gt; i\nnpati ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; i\npremi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; a\n.sata ---&gt; n\nsatan ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; s\n.abis ---&gt; h\nabish ---&gt; a\nbisha ---&gt; k\nishak ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; b\n.manb ---&gt; h\nmanbh ---&gt; a\nanbha ---&gt; r\nnbhar ---&gt; i\nbhari ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; n\n.basn ---&gt; t\nbasnt ---&gt; i\nasnti ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; i\n.taji ---&gt; n\ntajin ---&gt; d\najind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; i\n..udi ---&gt; t\n.udit ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; s\n..prs ---&gt; h\n.prsh ---&gt; a\nprsha ---&gt; n\nrshan ---&gt; t\nshant ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; d\nbabud ---&gt; d\nabudd ---&gt; e\nbudde ---&gt; n\nudden ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; b\nkushb ---&gt; u\nushbu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; m\n.narm ---&gt; a\nnarma ---&gt; d\narmad ---&gt; a\nrmada ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; n\n.amen ---&gt; a\namena ---&gt; .\n..... ---&gt; e\n....e ---&gt; m\n...em ---&gt; i\n..emi ---&gt; l\n.emil ---&gt; i\nemili ---&gt; a\nmilia ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; s\n.nees ---&gt; h\nneesh ---&gt; u\neeshu ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; t\n..tit ---&gt; u\n.titu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; r\nashir ---&gt; a\nshira ---&gt; m\nhiram ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; c\ntarac ---&gt; h\narach ---&gt; a\nracha ---&gt; n\nachan ---&gt; d\nchand ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; l\n.navl ---&gt; e\nnavle ---&gt; e\navlee ---&gt; n\nvleen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; h\n.ramh ---&gt; e\nramhe ---&gt; t\namhet ---&gt; u\nmhetu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; h\n..tah ---&gt; i\n.tahi ---&gt; r\ntahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; p\n..arp ---&gt; i\n.arpi ---&gt; t\narpit ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; i\n.zahi ---&gt; r\nzahir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; r\n.nazr ---&gt; a\nnazra ---&gt; n\nazran ---&gt; a\nzrana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; a\nkarma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; u\nmantu ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; y\nlaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; d\nsahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; k\nsadik ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; r\n..imr ---&gt; a\n.imra ---&gt; n\nimran ---&gt; a\nmrana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; r\nsawar ---&gt; i\nawari ---&gt; y\nwariy ---&gt; a\nariya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; b\nsahib ---&gt; a\nahiba ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; n\narchn ---&gt; a\nrchna ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; y\n.sony ---&gt; i\nsonyi ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; a\nhoola ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; y\nhhany ---&gt; a\nhanya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; p\n.manp ---&gt; r\nmanpr ---&gt; e\nanpre ---&gt; e\nnpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; n\n.navn ---&gt; i\nnavni ---&gt; t\navnit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; b\nsarab ---&gt; j\narabj ---&gt; e\nrabje ---&gt; e\nabjee ---&gt; t\nbjeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; i\n.rami ---&gt; l\nramil ---&gt; a\namila ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; a\n..pha ---&gt; k\n.phak ---&gt; i\nphaki ---&gt; r\nhakir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; n\nnandn ---&gt; i\nandni ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; s\n..zes ---&gt; h\n.zesh ---&gt; a\nzesha ---&gt; n\neshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; s\n..aws ---&gt; h\n.awsh ---&gt; i\nawshi ---&gt; n\nwshin ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; n\n..zan ---&gt; m\n.zanm ---&gt; i\nzanmi ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; k\n..nak ---&gt; u\n.naku ---&gt; l\nnakul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; e\n.sahe ---&gt; n\nsahen ---&gt; a\nahena ---&gt; j\nhenaj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; h\n.bush ---&gt; r\nbushr ---&gt; a\nushra ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; o\n.anoo ---&gt; j\nanooj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; v\nshanv ---&gt; a\nhanva ---&gt; j\nanvaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; a\nnajra ---&gt; n\najran ---&gt; a\njrana ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; v\nmanav ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; m\n.neem ---&gt; a\nneema ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; e\nhande ---&gt; r\nander ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; d\n.navd ---&gt; e\nnavde ---&gt; e\navdee ---&gt; p\nvdeep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; a\nchama ---&gt; n\nhaman ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; s\n.muds ---&gt; i\nmudsi ---&gt; r\nudsir ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; r\n..mor ---&gt; a\n.mora ---&gt; l\nmoral ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; u\npancu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; v\n.jaiv ---&gt; a\njaiva ---&gt; n\naivan ---&gt; t\nivant ---&gt; i\nvanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; i\n.pryi ---&gt; n\npryin ---&gt; k\nryink ---&gt; a\nyinka ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; u\nbhavu ---&gt; k\nhavuk ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; t\nmanit ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; l\n.viml ---&gt; e\nvimle ---&gt; s\nimles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; a\nmuska ---&gt; n\nuskan ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; v\neshav ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; e\n.vede ---&gt; h\nvedeh ---&gt; i\nedehi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; c\nbachc ---&gt; h\nachch ---&gt; a\nchcha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; g\n.narg ---&gt; i\nnargi ---&gt; s\nargis ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; n\neshan ---&gt; t\nshant ---&gt; i\nhanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; d\n.pand ---&gt; u\npandu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; v\nmahav ---&gt; i\nahavi ---&gt; r\nhavir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; o\n.najo ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; i\nramai ---&gt; y\namaiy ---&gt; a\nmaiya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; n\nkaran ---&gt; v\naranv ---&gt; e\nranve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; o\n.sato ---&gt; s\nsatos ---&gt; h\natosh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; e\n.name ---&gt; e\nnamee ---&gt; t\nameet ---&gt; a\nmeeta ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; u\n.latu ---&gt; r\nlatur ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; u\nnathu ---&gt; r\nathur ---&gt; a\nthura ---&gt; m\nhuram ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; r\neghar ---&gt; a\nghara ---&gt; m\nharam ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; h\n.bush ---&gt; a\nbusha ---&gt; r\nushar ---&gt; a\nshara ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; o\nukhso ---&gt; n\nkhson ---&gt; a\nhsona ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; k\n.dark ---&gt; a\ndarka ---&gt; s\narkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; a\n.jafa ---&gt; r\njafar ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; w\n.balw ---&gt; a\nbalwa ---&gt; n\nalwan ---&gt; t\nlwant ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; h\ndaksh ---&gt; y\nakshy ---&gt; a\nkshya ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; l\n.amil ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; s\nvijes ---&gt; h\nijesh ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; a\n..ika ---&gt; r\n.ikar ---&gt; a\nikara ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; b\n.sarb ---&gt; a\nsarba ---&gt; r\narbar ---&gt; i\nrbari ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; e\n.sake ---&gt; e\nsakee ---&gt; n\nakeen ---&gt; a\nkeena ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; r\naakar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; t\nsumit ---&gt; r\numitr ---&gt; a\nmitra ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; v\n.perv ---&gt; i\npervi ---&gt; n\nervin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; d\n..mid ---&gt; d\n.midd ---&gt; a\nmidda ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; a\nimama ---&gt; n\nmaman ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; a\n.aara ---&gt; d\naarad ---&gt; h\naradh ---&gt; a\nradha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; y\n.sury ---&gt; a\nsurya ---&gt; n\nuryan ---&gt; a\nryana ---&gt; t\nyanat ---&gt; h\nanath ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; b\nurajb ---&gt; h\nrajbh ---&gt; a\najbha ---&gt; n\njbhan ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; a\naksha ---&gt; y\nkshay ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; t\nchint ---&gt; a\nhinta ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; a\nharba ---&gt; i\narbai ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; h\nshobh ---&gt; a\nhobha ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; a\n..afa ---&gt; r\n.afar ---&gt; i\nafari ---&gt; n\nfarin ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; i\nbhavi ---&gt; s\nhavis ---&gt; a\navisa ---&gt; y\nvisay ---&gt; a\nisaya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; n\nviren ---&gt; d\nirend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; v\naramv ---&gt; e\nramve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; n\nlshan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; a\n..naa ---&gt; j\n.naaj ---&gt; i\nnaaji ---&gt; m\naajim ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; a\nshana ---&gt; r\nhanar ---&gt; a\nanara ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; j\n.shaj ---&gt; i\nshaji ---&gt; d\nhajid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; b\n..nib ---&gt; h\n.nibh ---&gt; a\nnibha ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; k\n.vikk ---&gt; i\nvikki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; u\n.ranu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; a\n.shoa ---&gt; i\nshoai ---&gt; b\nhoaib ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; r\n.satr ---&gt; u\nsatru ---&gt; d\natrud ---&gt; a\ntruda ---&gt; n\nrudan ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; s\n.akas ---&gt; h\nakash ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; d\n..yud ---&gt; h\n.yudh ---&gt; b\nyudhb ---&gt; i\nudhbi ---&gt; r\ndhbir ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; e\n.nile ---&gt; s\nniles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; y\n..gay ---&gt; t\n.gayt ---&gt; r\ngaytr ---&gt; i\naytri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; g\n.ramg ---&gt; o\nramgo ---&gt; p\namgop ---&gt; a\nmgopa ---&gt; l\ngopal ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; s\ndevas ---&gt; h\nevash ---&gt; i\nvashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; u\nchotu ---&gt; r\nhotur ---&gt; a\notura ---&gt; m\nturam ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; g\n.surg ---&gt; y\nsurgy ---&gt; a\nurgya ---&gt; n\nrgyan ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; a\nrisha ---&gt; n\nishan ---&gt; a\nshana ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; i\n.tali ---&gt; m\ntalim ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; v\n.tanv ---&gt; i\ntanvi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; d\nashid ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; i\n.nari ---&gt; n\nnarin ---&gt; d\narind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; t\n.pret ---&gt; i\npreti ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; e\n.mune ---&gt; e\nmunee ---&gt; r\nuneer ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; e\ngyane ---&gt; n\nyanen ---&gt; d\nanend ---&gt; e\nnende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; h\nhanah ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; a\nchata ---&gt; r\nhatar ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; v\n..omv ---&gt; e\n.omve ---&gt; e\nomvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; l\n.swal ---&gt; i\nswali ---&gt; y\nwaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; u\nbachu ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; h\nvansh ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; e\n.swee ---&gt; t\nsweet ---&gt; y\nweety ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; v\n.danv ---&gt; e\ndanve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; t\n.sult ---&gt; a\nsulta ---&gt; n\nultan ---&gt; .\n..... ---&gt; s\n....s ---&gt; t\n...st ---&gt; e\n..ste ---&gt; p\n.step ---&gt; h\nsteph ---&gt; e\ntephe ---&gt; n\nephen ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; n\nrishn ---&gt; a\nishna ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; k\n.balk ---&gt; i\nbalki ---&gt; s\nalkis ---&gt; h\nlkish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; s\n..jos ---&gt; h\n.josh ---&gt; n\njoshn ---&gt; a\noshna ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; t\n.mitt ---&gt; h\nmitth ---&gt; u\nitthu ---&gt; n\ntthun ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; d\n.gurd ---&gt; a\ngurda ---&gt; y\nurday ---&gt; a\nrdaya ---&gt; l\ndayal ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; a\n.hira ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; y\n..kay ---&gt; u\n.kayu ---&gt; m\nkayum ---&gt; .\n..... ---&gt; e\n....e ---&gt; s\n...es ---&gt; h\n..esh ---&gt; a\n.esha ---&gt; v\neshav ---&gt; a\nshava ---&gt; r\nhavar ---&gt; y\navary ---&gt; a\nvarya ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; a\n.risa ---&gt; b\nrisab ---&gt; h\nisabh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; e\nrajne ---&gt; e\najnee ---&gt; s\njnees ---&gt; h\nneesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; k\npushk ---&gt; a\nushka ---&gt; r\nshkar ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; t\n..jet ---&gt; e\n.jete ---&gt; n\njeten ---&gt; d\netend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; i\n.puni ---&gt; t\npunit ---&gt; a\nunita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; a\nchama ---&gt; n\nhaman ---&gt; l\namanl ---&gt; a\nmanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; b\n..hab ---&gt; i\n.habi ---&gt; b\nhabib ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; p\n.rajp ---&gt; a\nrajpa ---&gt; l\najpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; j\n.rinj ---&gt; u\nrinju ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; a\nharma ---&gt; n\narman ---&gt; i\nrmani ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; l\n.kall ---&gt; o\nkallo ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; r\n.subr ---&gt; a\nsubra ---&gt; t\nubrat ---&gt; i\nbrati ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; n\n..sen ---&gt; t\n.sent ---&gt; h\nsenth ---&gt; i\nenthi ---&gt; a\nnthia ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; n\nkarin ---&gt; a\narina ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; l\n.hitl ---&gt; a\nhitla ---&gt; r\nitlar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; h\nrameh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; a\n.dila ---&gt; w\ndilaw ---&gt; a\nilawa ---&gt; r\nlawar ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; u\nneelu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; a\n.vica ---&gt; k\nvicak ---&gt; h\nicakh ---&gt; a\ncakha ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; d\n.jayd ---&gt; a\njayda ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; r\n.rajr ---&gt; a\nrajra ---&gt; n\najran ---&gt; i\njrani ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; i\n.rubi ---&gt; y\nrubiy ---&gt; a\nubiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; f\n.sarf ---&gt; r\nsarfr ---&gt; a\narfra ---&gt; j\nrfraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; e\n.rafe ---&gt; d\nrafed ---&gt; d\nafedd ---&gt; i\nfeddi ---&gt; n\neddin ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; n\namrin ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; s\n..pas ---&gt; a\n.pasa ---&gt; n\npasan ---&gt; j\nasanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; s\n.taus ---&gt; i\ntausi ---&gt; n\nausin ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; s\n..ims ---&gt; a\n.imsa ---&gt; a\nimsaa ---&gt; n\nmsaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; s\nhupes ---&gt; h\nupesh ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; b\ngurub ---&gt; a\nuruba ---&gt; k\nrubak ---&gt; s\nubaks ---&gt; h\nbaksh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; k\n.halk ---&gt; i\nhalki ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; a\n.suba ---&gt; n\nsuban ---&gt; i\nubani ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; r\n..ikr ---&gt; a\n.ikra ---&gt; r\nikrar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; a\nnasia ---&gt; r\nasiar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; w\namesw ---&gt; a\nmeswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; n\naveen ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; a\n..nea ---&gt; h\n.neah ---&gt; a\nneaha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; d\nravid ---&gt; u\navidu ---&gt; t\nvidut ---&gt; t\nidutt ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; a\nsunda ---&gt; r\nundar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; n\nsayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; u\n.hanu ---&gt; m\nhanum ---&gt; a\nanuma ---&gt; n\nnuman ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; i\nrajni ---&gt; s\najnis ---&gt; h\njnish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; i\nsushi ---&gt; l\nushil ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; s\n.mees ---&gt; h\nmeesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; w\nahnaw ---&gt; a\nhnawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; a\nparta ---&gt; p\nartap ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; u\n.niru ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; p\ndhurp ---&gt; a\nhurpa ---&gt; t\nurpat ---&gt; i\nrpati ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; d\nhahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; h\nmanoh ---&gt; a\nanoha ---&gt; r\nnohar ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; b\n.sehb ---&gt; o\nsehbo ---&gt; o\nehboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; a\npuspa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; k\nnayak ---&gt; a\nayaka ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; k\n.devk ---&gt; r\ndevkr ---&gt; a\nevkra ---&gt; n\nvkran ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; u\n..fau ---&gt; j\n.fauj ---&gt; i\nfauji ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; a\n..kra ---&gt; s\n.kras ---&gt; h\nkrash ---&gt; n\nrashn ---&gt; a\nashna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; a\n.susa ---&gt; n\nsusan ---&gt; t\nusant ---&gt; o\nsanto ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; b\nnasib ---&gt; u\nasibu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; a\n.naha ---&gt; r\nnahar ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; m\n..mem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; s\nshris ---&gt; h\nhrish ---&gt; t\nrisht ---&gt; i\nishti ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; y\n.ajay ---&gt; p\najayp ---&gt; a\njaypa ---&gt; l\naypal ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; v\nlilav ---&gt; a\nilava ---&gt; t\nlavat ---&gt; i\navati ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; e\n.dave ---&gt; n\ndaven ---&gt; d\navend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; o\nrinko ---&gt; o\ninkoo ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; n\n.renn ---&gt; u\nrennu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; r\nshabr ---&gt; e\nhabre ---&gt; e\nabree ---&gt; n\nbreen ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; f\n.gurf ---&gt; a\ngurfa ---&gt; a\nurfaa ---&gt; n\nrfaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; u\nsamsu ---&gt; d\namsud ---&gt; d\nmsudd ---&gt; i\nsuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; k\nomprk ---&gt; a\nmprka ---&gt; s\nprkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; h\n..mih ---&gt; a\n.miha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; e\nramve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; m\n.susm ---&gt; a\nsusma ---&gt; t\nusmat ---&gt; a\nsmata ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; h\n.sheh ---&gt; n\nshehn ---&gt; a\nhehna ---&gt; z\nehnaz ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; r\n.musr ---&gt; a\nmusra ---&gt; t\nusrat ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; i\n.husi ---&gt; y\nhusiy ---&gt; a\nusiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; r\nsukhr ---&gt; a\nukhra ---&gt; m\nkhram ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; i\n.mohi ---&gt; n\nmohin ---&gt; i\nohini ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; w\n.tabw ---&gt; s\ntabws ---&gt; u\nabwsu ---&gt; m\nbwsum ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; y\n.divy ---&gt; a\ndivya ---&gt; n\nivyan ---&gt; s\nvyans ---&gt; h\nyansh ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; p\n..nip ---&gt; u\n.nipu ---&gt; n\nnipun ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; b\n.mahb ---&gt; u\nmahbu ---&gt; b\nahbub ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; n\nsalin ---&gt; i\nalini ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; i\n.disi ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; s\n..gos ---&gt; i\n.gosi ---&gt; a\ngosia ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; i\n.niki ---&gt; t\nnikit ---&gt; a\nikita ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; a\nhusha ---&gt; b\nushab ---&gt; u\nshabu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; e\n.sahe ---&gt; e\nsahee ---&gt; n\naheen ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; y\n..piy ---&gt; u\n.piyu ---&gt; s\npiyus ---&gt; h\niyush ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; a\nparsa ---&gt; n\narsan ---&gt; t\nrsant ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; e\n.jule ---&gt; e\njulee ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; a\n.safa ---&gt; l\nsafal ---&gt; i\nafali ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; k\n.reek ---&gt; h\nreekh ---&gt; a\neekha ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; k\nsheik ---&gt; h\nheikh ---&gt; s\neikhs ---&gt; a\nikhsa ---&gt; i\nkhsai ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; t\n.kirt ---&gt; i\nkirti ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; i\n.puni ---&gt; t\npunit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; r\nshivr ---&gt; a\nhivra ---&gt; j\nivraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; h\n..joh ---&gt; o\n.joho ---&gt; l\njohol ---&gt; a\nohola ---&gt; l\nholal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; y\nbhagy ---&gt; a\nhagya ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; e\n..mhe ---&gt; g\n.mheg ---&gt; a\nmhega ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; j\n.nurj ---&gt; a\nnurja ---&gt; h\nurjah ---&gt; a\nrjaha ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; a\nharma ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; e\nsande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; a\nambha ---&gt; j\nmbhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; r\nvendr ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; a\n.kawa ---&gt; l\nkawal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; i\n.somi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; l\nshill ---&gt; y\nhilly ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; o\nsaroo ---&gt; p\naroop ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; e\n..ale ---&gt; e\n.alee ---&gt; s\nalees ---&gt; h\nleesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; e\n.tape ---&gt; n\ntapen ---&gt; d\napend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; r\nnandr ---&gt; a\nandra ---&gt; m\nndram ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; f\n..hif ---&gt; j\n.hifj ---&gt; u\nhifju ---&gt; l\nifjul ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; n\n..yun ---&gt; i\n.yuni ---&gt; s\nyunis ---&gt; h\nunish ---&gt; s\nnishs ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; w\n..suw ---&gt; a\n.suwa ---&gt; d\nsuwad ---&gt; h\nuwadh ---&gt; i\nwadhi ---&gt; n\nadhin ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; i\nrinki ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; e\n.zahe ---&gt; e\nzahee ---&gt; r\naheer ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; e\nmanve ---&gt; n\nanven ---&gt; d\nnvend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; a\n.suka ---&gt; n\nsukan ---&gt; y\nukany ---&gt; a\nkanya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; a\nranja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; n\n..jen ---&gt; i\n.jeni ---&gt; f\njenif ---&gt; e\nenife ---&gt; r\nnifer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; p\nubhap ---&gt; a\nbhapa ---&gt; l\nhapal ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; k\neepak ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; i\ndhani ---&gt; y\nhaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; v\n..urv ---&gt; a\n.urva ---&gt; s\nurvas ---&gt; h\nrvash ---&gt; i\nvashi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; h\n.mosh ---&gt; a\nmosha ---&gt; d\noshad ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; i\n.anni ---&gt; e\nannie ---&gt; l\nnniel ---&gt; a\nniela ---&gt; l\nielal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; n\nsohan ---&gt; p\nohanp ---&gt; a\nhanpa ---&gt; l\nanpal ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; t\n..akt ---&gt; h\n.akth ---&gt; a\naktha ---&gt; r\nkthar ---&gt; i\nthari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; i\nhashi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; s\n.tabs ---&gt; u\ntabsu ---&gt; m\nabsum ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; i\n.fazi ---&gt; n\nfazin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; k\n..jak ---&gt; i\n.jaki ---&gt; r\njakir ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; e\n.bije ---&gt; n\nbijen ---&gt; d\nijend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; d\n.shid ---&gt; h\nshidh ---&gt; a\nhidha ---&gt; r\nidhar ---&gt; t\ndhart ---&gt; h\nharth ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; h\nvandh ---&gt; a\nandha ---&gt; n\nndhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; m\nrashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; m\nlakhm ---&gt; i\nakhmi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; j\n.nirj ---&gt; l\nnirjl ---&gt; a\nirjla ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; w\nkalaw ---&gt; a\nalawa ---&gt; t\nlawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; l\naveel ---&gt; a\nveela ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; m\n.rahm ---&gt; a\nrahma ---&gt; t\nahmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; e\n.suhe ---&gt; l\nsuhel ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; i\n.dami ---&gt; n\ndamin ---&gt; i\namini ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; t\n.meet ---&gt; u\nmeetu ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; r\nchitr ---&gt; a\nhitra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; v\n.mahv ---&gt; i\nmahvi ---&gt; s\nahvis ---&gt; h\nhvish ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; d\n.prad ---&gt; e\nprade ---&gt; e\nradee ---&gt; p\nadeep ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; p\n.alap ---&gt; n\nalapn ---&gt; a\nlapna ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; a\n..kaa ---&gt; m\n.kaam ---&gt; i\nkaami ---&gt; n\naamin ---&gt; i\namini ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; m\n.sazm ---&gt; i\nsazmi ---&gt; n\nazmin ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; t\n.kart ---&gt; i\nkarti ---&gt; k\nartik ---&gt; a\nrtika ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; a\n.kesa ---&gt; v\nkesav ---&gt; i\nesavi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; l\n.dhul ---&gt; i\ndhuli ---&gt; n\nhulin ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; i\ndeepi ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; t\n.pint ---&gt; t\npintt ---&gt; u\ninttu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; i\nramki ---&gt; s\namkis ---&gt; h\nmkish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; t\n..net ---&gt; r\n.netr ---&gt; a\nnetra ---&gt; p\netrap ---&gt; a\ntrapa ---&gt; l\nrapal ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; i\n.yasi ---&gt; n\nyasin ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; i\n..twi ---&gt; n\n.twin ---&gt; k\ntwink ---&gt; i\nwinki ---&gt; l\ninkil ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; u\n.renu ---&gt; k\nrenuk ---&gt; a\nenuka ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; n\n.karn ---&gt; e\nkarne ---&gt; s\narnes ---&gt; h\nrnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; h\nsushh ---&gt; m\nushhm ---&gt; i\nshhmi ---&gt; t\nhhmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; s\n..sis ---&gt; p\n.sisp ---&gt; a\nsispa ---&gt; l\nispal ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; e\n..sne ---&gt; h\n.sneh ---&gt; a\nsneha ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; d\n..rud ---&gt; h\n.rudh ---&gt; r\nrudhr ---&gt; a\nudhra ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; j\n.girj ---&gt; a\ngirja ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; r\nmohar ---&gt; d\nohard ---&gt; i\nhardi ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; h\ndilsh ---&gt; a\nilsha ---&gt; d\nlshad ---&gt; i\nshadi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; u\nmangu ---&gt; b\nangub ---&gt; a\nnguba ---&gt; i\ngubai ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; d\n.gord ---&gt; h\ngordh ---&gt; a\nordha ---&gt; n\nrdhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; a\nresha ---&gt; m\nesham ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; m\n.rukm ---&gt; a\nrukma ---&gt; n\nukman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; u\n.aaru ---&gt; s\naarus ---&gt; h\narush ---&gt; i\nrushi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; m\n.dham ---&gt; e\ndhame ---&gt; n\nhamen ---&gt; d\namend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; m\nbrijm ---&gt; o\nrijmo ---&gt; h\nijmoh ---&gt; a\njmoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; b\n.dilb ---&gt; a\ndilba ---&gt; r\nilbar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; r\n.beer ---&gt; a\nbeera ---&gt; m\neeram ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; e\nkunde ---&gt; n\nunden ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; a\nsanna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; h\n..fah ---&gt; i\n.fahi ---&gt; j\nfahij ---&gt; a\nahija ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; m\n.jaim ---&gt; a\njaima ---&gt; l\naimal ---&gt; a\nimala ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; f\n..kif ---&gt; a\n.kifa ---&gt; y\nkifay ---&gt; a\nifaya ---&gt; t\nfayat ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; k\n.bhak ---&gt; u\nbhaku ---&gt; n\nhakun ---&gt; i\nakuni ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; l\nompal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; i\n..hai ---&gt; l\n.hail ---&gt; e\nhaile ---&gt; n\nailen ---&gt; a\nilena ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; d\n..nid ---&gt; h\n.nidh ---&gt; i\nnidhi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; i\n.keli ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; t\n.bunt ---&gt; y\nbunty ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; s\nshams ---&gt; h\nhamsh ---&gt; a\namsha ---&gt; d\nmshad ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; v\nkarmv ---&gt; e\narmve ---&gt; e\nrmvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; f\n.sarf ---&gt; a\nsarfa ---&gt; r\narfar ---&gt; a\nrfara ---&gt; j\nfaraj ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; a\nveera ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; m\n.anam ---&gt; i\nanami ---&gt; k\nnamik ---&gt; a\namika ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; r\nsabir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; r\n.mair ---&gt; y\nmairy ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; a\n.rosa ---&gt; n\nrosan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; y\nnaray ---&gt; a\naraya ---&gt; n\nrayan ---&gt; i\nayani ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; i\n.vini ---&gt; t\nvinit ---&gt; a\ninita ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; i\n..moi ---&gt; n\n.moin ---&gt; u\nmoinu ---&gt; d\noinud ---&gt; d\ninudd ---&gt; i\nnuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; l\n.mahl ---&gt; i\nmahli ---&gt; k\nahlik ---&gt; k\nhlikk ---&gt; a\nlikka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; i\nramdi ---&gt; n\namdin ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; r\nompar ---&gt; k\nmpark ---&gt; e\nparke ---&gt; s\narkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; d\n.arad ---&gt; h\naradh ---&gt; a\nradha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; y\nraniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; a\n.dura ---&gt; g\ndurag ---&gt; a\nuraga ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; i\npyari ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; j\nabhij ---&gt; e\nbhije ---&gt; e\nhijee ---&gt; t\nijeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; e\n.kale ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; o\nkisho ---&gt; r\nishor ---&gt; e\nshore ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; e\n..ade ---&gt; r\n.ader ---&gt; s\naders ---&gt; e\nderse ---&gt; n\nersen ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; l\n.rupl ---&gt; a\nrupla ---&gt; l\nuplal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; o\n.subo ---&gt; d\nsubod ---&gt; h\nubodh ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; g\n..jog ---&gt; i\n.jogi ---&gt; n\njogin ---&gt; d\nogind ---&gt; e\nginde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; a\nhasha ---&gt; n\nashan ---&gt; k\nshank ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; s\n.anas ---&gt; h\nanash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; r\n.samr ---&gt; i\nsamri ---&gt; n\namrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; e\n.swee ---&gt; t\nsweet ---&gt; i\nweeti ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; a\nbisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; l\n.saal ---&gt; u\nsaalu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; e\n.sude ---&gt; s\nsudes ---&gt; h\nudesh ---&gt; w\ndeshw ---&gt; e\neshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; s\nravis ---&gt; h\navish ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; h\n..wah ---&gt; i\n.wahi ---&gt; d\nwahid ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; i\nanshi ---&gt; y\nnshiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; e\nshale ---&gt; n\nhalen ---&gt; d\nalend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; a\ndisha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; u\n.goru ---&gt; a\ngorua ---&gt; v\noruav ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; m\n.bhum ---&gt; i\nbhumi ---&gt; k\nhumik ---&gt; a\numika ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; n\n.nayn ---&gt; a\nnayna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; j\n.sajj ---&gt; a\nsajja ---&gt; n\najjan ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; t\n.mont ---&gt; u\nmontu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; h\nramdh ---&gt; i\namdhi ---&gt; n\nmdhin ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; c\n..kac ---&gt; h\n.kach ---&gt; r\nkachr ---&gt; i\nachri ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; k\n..tek ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; e\n.rahe ---&gt; e\nrahee ---&gt; s\nahees ---&gt; h\nheesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; i\nsubhi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; i\n.yasi ---&gt; b\nyasib ---&gt; .\n..... ---&gt; e\n....e ---&gt; s\n...es ---&gt; h\n..esh ---&gt; w\n.eshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; i\nhwari ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; d\n.arad ---&gt; d\naradd ---&gt; h\nraddh ---&gt; n\naddhn ---&gt; a\nddhna ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; r\n.char ---&gt; a\nchara ---&gt; n\nharan ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; k\n..ajk ---&gt; a\n.ajka ---&gt; t\najkat ---&gt; i\njkati ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; o\nambho ---&gt; o\nmbhoo ---&gt; l\nbhool ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; i\n.momi ---&gt; n\nmomin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; r\nsakir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; a\nshara ---&gt; d\nharad ---&gt; h\naradh ---&gt; h\nradhh ---&gt; a\nadhha ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; i\nmansi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; l\nsheel ---&gt; u\nheelu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; e\n..ake ---&gt; e\n.akee ---&gt; l\nakeel ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; p\n.rimp ---&gt; i\nrimpi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; v\n.seev ---&gt; a\nseeva ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; m\n.bhim ---&gt; s\nbhims ---&gt; e\nhimse ---&gt; n\nimsen ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; i\naditi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; k\n..bik ---&gt; k\n.bikk ---&gt; i\nbikki ---&gt; .\n..... ---&gt; t\n....t ---&gt; h\n...th ---&gt; a\n..tha ---&gt; r\n.thar ---&gt; u\ntharu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; k\n.rakk ---&gt; i\nrakki ---&gt; b\nakkib ---&gt; h\nkkibh ---&gt; u\nkibhu ---&gt; l\nibhul ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; l\nanjal ---&gt; y\nnjaly ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; w\n.rajw ---&gt; a\nrajwa ---&gt; n\najwan ---&gt; t\njwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; j\n..arj ---&gt; u\n.arju ---&gt; n\narjun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; e\n.save ---&gt; t\nsavet ---&gt; a\naveta ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; y\n..ziy ---&gt; a\n.ziya ---&gt; b\nziyab ---&gt; u\niyabu ---&gt; l\nyabul ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; t\n..pit ---&gt; i\n.piti ---&gt; k\npitik ---&gt; a\nitika ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; o\n.sano ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; v\n.kauv ---&gt; a\nkauva ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; n\n.asan ---&gt; t\nasant ---&gt; i\nsanti ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; i\n.riti ---&gt; k\nritik ---&gt; a\nitika ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; b\n.ranb ---&gt; i\nranbi ---&gt; r\nanbir ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; t\nchint ---&gt; u\nhintu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; c\n..sac ---&gt; h\n.sach ---&gt; i\nsachi ---&gt; n\nachin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; n\nsanjn ---&gt; a\nanjna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; i\n.fari ---&gt; d\nfarid ---&gt; a\narida ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; u\nrajku ---&gt; m\najkum ---&gt; a\njkuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; s\n..aps ---&gt; a\n.apsa ---&gt; n\napsan ---&gt; a\npsana ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; a\nmunta ---&gt; j\nuntaj ---&gt; a\nntaja ---&gt; r\ntajar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; a\nalmaa ---&gt; n\nlmaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; e\nhande ---&gt; s\nandes ---&gt; h\nndesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; a\n.rata ---&gt; n\nratan ---&gt; i\natani ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; w\n..isw ---&gt; a\n.iswa ---&gt; r\niswar ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; r\n..her ---&gt; a\n.hera ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; k\n.tilk ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; y\n..piy ---&gt; u\n.piyu ---&gt; s\npiyus ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; h\n.maah ---&gt; i\nmaahi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; o\n.salo ---&gt; n\nsalon ---&gt; i\naloni ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; i\n.bani ---&gt; t\nbanit ---&gt; a\nanita ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; l\n..mol ---&gt; u\n.molu ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; t\n..irt ---&gt; u\n.irtu ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; h\n..udh ---&gt; a\n.udha ---&gt; m\nudham ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; b\n.sarb ---&gt; j\nsarbj ---&gt; e\narbje ---&gt; e\nrbjee ---&gt; t\nbjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; s\n.asis ---&gt; h\nasish ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; e\n.sile ---&gt; n\nsilen ---&gt; d\nilend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; n\nchunn ---&gt; i\nhunni ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; d\n..had ---&gt; a\n.hada ---&gt; r\nhadar ---&gt; a\nadara ---&gt; l\ndaral ---&gt; i\narali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; s\nubhas ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; e\nsamee ---&gt; r\nameer ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; a\ndisha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; i\nrajni ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; m\n.farm ---&gt; a\nfarma ---&gt; a\narmaa ---&gt; n\nrmaan ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; z\n..waz ---&gt; i\n.wazi ---&gt; r\nwazir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; r\n.sair ---&gt; a\nsaira ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; s\n.saks ---&gt; i\nsaksi ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; h\n..azh ---&gt; a\n.azha ---&gt; r\nazhar ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; k\n..brk ---&gt; h\n.brkh ---&gt; a\nbrkha ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; l\n.jhal ---&gt; l\njhall ---&gt; a\nhalla ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; u\n.bhau ---&gt; k\nbhauk ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; a\nsusha ---&gt; n\nushan ---&gt; t\nshant ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; a\n..iba ---&gt; r\n.ibar ---&gt; h\nibarh ---&gt; i\nbarhi ---&gt; m\narhim ---&gt; .\n..... ---&gt; y\n....y ---&gt; i\n...yi ---&gt; n\n..yin ---&gt; i\n.yini ---&gt; t\nyinit ---&gt; a\ninita ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; w\n..new ---&gt; a\n.newa ---&gt; l\nnewal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; v\n.rajv ---&gt; i\nrajvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; n\nkaman ---&gt; a\namana ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; a\nukhsa ---&gt; r\nkhsar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; n\nrachn ---&gt; a\nachna ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; e\n.saye ---&gt; r\nsayer ---&gt; i\nayeri ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; d\n.lald ---&gt; h\nlaldh ---&gt; a\naldha ---&gt; r\nldhar ---&gt; i\ndhari ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; p\nbrijp ---&gt; a\nrijpa ---&gt; l\nijpal ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; s\n.aans ---&gt; u\naansu ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; r\nmehar ---&gt; b\neharb ---&gt; a\nharba ---&gt; n\narban ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; c\n..luc ---&gt; k\n.luck ---&gt; i\nlucki ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; t\n.akht ---&gt; a\nakhta ---&gt; r\nkhtar ---&gt; i\nhtari ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; r\n.sadr ---&gt; e\nsadre ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; i\nratni ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; c\n.dakc ---&gt; h\ndakch ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; v\n..lov ---&gt; e\n.love ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; t\n.mint ---&gt; u\nmintu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; h\n.bash ---&gt; a\nbasha ---&gt; n\nashan ---&gt; t\nshant ---&gt; i\nhanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; e\n.save ---&gt; r\nsaver ---&gt; o\navero ---&gt; o\nveroo ---&gt; n\neroon ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; n\n.razn ---&gt; i\nrazni ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; e\nkanhe ---&gt; y\nanhey ---&gt; a\nnheya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; f\n.praf ---&gt; h\nprafh ---&gt; o\nrafho ---&gt; o\nafhoo ---&gt; l\nfhool ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; u\n..piu ---&gt; e\n.piue ---&gt; s\npiues ---&gt; h\niuesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; h\n.bhah ---&gt; a\nbhaha ---&gt; d\nhahad ---&gt; u\nahadu ---&gt; r\nhadur ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; l\nanjal ---&gt; i\nnjali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; e\n.sule ---&gt; m\nsulem ---&gt; a\nulema ---&gt; n\nleman ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; i\n.viri ---&gt; y\nviriy ---&gt; a\niriya ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; w\n..tiw ---&gt; a\n.tiwa ---&gt; n\ntiwan ---&gt; k\niwank ---&gt; l\nwankl ---&gt; e\nankle ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; a\ndilsa ---&gt; h\nilsah ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; h\n.mush ---&gt; i\nmushi ---&gt; r\nushir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; k\nashik ---&gt; a\nshika ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; m\n..kim ---&gt; a\n.kima ---&gt; t\nkimat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; n\n.main ---&gt; a\nmaina ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; r\netanr ---&gt; a\ntanra ---&gt; m\nanram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; d\naramd ---&gt; e\nramde ---&gt; e\namdee ---&gt; p\nmdeep ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; n\n..isn ---&gt; e\n.isne ---&gt; s\nisnes ---&gt; h\nsnesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; c\n..ric ---&gt; h\n.rich ---&gt; a\nricha ---&gt; r\nichar ---&gt; a\nchara ---&gt; j\nharaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; e\n.safe ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; y\n.aany ---&gt; a\naanya ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; t\n..got ---&gt; a\n.gota ---&gt; m\ngotam ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; p\n..bip ---&gt; i\n.bipi ---&gt; n\nbipin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; n\nrabin ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; h\n..neh ---&gt; a\n.neha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; p\n.chap ---&gt; p\nchapp ---&gt; l\nhappl ---&gt; a\nappla ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; a\n.nura ---&gt; i\nnurai ---&gt; s\nurais ---&gt; h\nraish ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; i\n.jagi ---&gt; r\njagir ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; r\n..fur ---&gt; k\n.furk ---&gt; a\nfurka ---&gt; n\nurkan ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; y\n.boby ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; a\n.juga ---&gt; n\njugan ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; b\n..rob ---&gt; e\n.robe ---&gt; r\nrober ---&gt; t\nobert ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; e\n.suke ---&gt; s\nsukes ---&gt; h\nukesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; u\ndhanu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; a\n.kana ---&gt; h\nkanah ---&gt; i\nanahi ---&gt; y\nnahiy ---&gt; a\nahiya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; e\n.naze ---&gt; e\nnazee ---&gt; m\nazeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; m\nhivam ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; p\n..grp ---&gt; r\n.grpr ---&gt; e\ngrpre ---&gt; e\nrpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; s\niyans ---&gt; h\nyansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; o\n.sugo ---&gt; d\nsugod ---&gt; h\nugodh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; f\n..kaf ---&gt; i\n.kafi ---&gt; a\nkafia ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; i\nsheki ---&gt; b\nhekib ---&gt; a\nekiba ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; x\nminax ---&gt; i\ninaxi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; h\n.parh ---&gt; a\nparha ---&gt; l\narhal ---&gt; a\nrhala ---&gt; d\nhalad ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; t\n.gaut ---&gt; a\ngauta ---&gt; m\nautam ---&gt; .\n..... ---&gt; w\n....w ---&gt; i\n...wi ---&gt; r\n..wir ---&gt; e\n.wire ---&gt; s\nwires ---&gt; h\niresh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; a\nmanda ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; y\nsunay ---&gt; n\nunayn ---&gt; a\nnayna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; m\n.dhrm ---&gt; v\ndhrmv ---&gt; e\nhrmve ---&gt; e\nrmvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; i\n.aati ---&gt; k\naatik ---&gt; u\natiku ---&gt; n\ntikun ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; l\narshl ---&gt; a\nrshla ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; w\nshasw ---&gt; a\nhaswa ---&gt; t\naswat ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; t\n..att ---&gt; a\n.atta ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; r\n.megr ---&gt; a\nmegra ---&gt; a\negraa ---&gt; j\ngraaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; h\noorjh ---&gt; a\norjha ---&gt; n\nrjhan ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; w\nfoolw ---&gt; a\noolwa ---&gt; t\nolwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; h\n..srh ---&gt; e\n.srhe ---&gt; e\nsrhee ---&gt; r\nrheer ---&gt; a\nheera ---&gt; m\neeram ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; s\n.amis ---&gt; h\namish ---&gt; a\nmisha ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; j\n.anuj ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; r\n..isr ---&gt; a\n.isra ---&gt; r\nisrar ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; y\ndimpy ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; b\n..vib ---&gt; h\n.vibh ---&gt; a\nvibha ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; a\n.asma ---&gt; t\nasmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; d\nsamad ---&gt; h\namadh ---&gt; a\nmadha ---&gt; n\nadhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; i\n.rafi ---&gt; q\nrafiq ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; u\nramdu ---&gt; t\namdut ---&gt; t\nmdutt ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; l\n.tasl ---&gt; i\ntasli ---&gt; m\naslim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; u\nrajku ---&gt; m\najkum ---&gt; a\njkuma ---&gt; r\nkumar ---&gt; i\numari ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; a\nhanwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; o\npramo ---&gt; d\nramod ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; d\n..jad ---&gt; u\n.jadu ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; m\n..jum ---&gt; m\n.jumm ---&gt; a\njumma ---&gt; n\numman ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; e\n..ome ---&gt; n\n.omen ---&gt; d\nomend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; a\n.haja ---&gt; r\nhajar ---&gt; i\najari ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; r\n.jair ---&gt; a\njaira ---&gt; m\nairam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; n\nparsn ---&gt; a\narsna ---&gt; t\nrsnat ---&gt; h\nsnath ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; a\nveera ---&gt; n\neeran ---&gt; d\nerand ---&gt; e\nrande ---&gt; r\nander ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; i\n.ragi ---&gt; n\nragin ---&gt; i\nagini ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; k\n.fark ---&gt; u\nfarku ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; a\n.pata ---&gt; v\npatav ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; u\nnazmu ---&gt; s\nazmus ---&gt; l\nzmusl ---&gt; a\nmusla ---&gt; m\nuslam ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; e\n.suje ---&gt; n\nsujen ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; y\n.anay ---&gt; a\nanaya ---&gt; t\nnayat ---&gt; h\nayath ---&gt; a\nyatha ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; n\nvijen ---&gt; d\nijend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; m\nahnam ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; b\n..reb ---&gt; e\n.rebe ---&gt; c\nrebec ---&gt; c\nebecc ---&gt; a\nbecca ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; f\ngulaf ---&gt; s\nulafs ---&gt; h\nlafsh ---&gt; a\nafsha ---&gt; n\nfshan ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; j\n..puj ---&gt; j\n.pujj ---&gt; a\npujja ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; e\nhande ---&gt; n\nanden ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; a\n.ruba ---&gt; l\nrubal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; m\n.mahm ---&gt; u\nmahmu ---&gt; n\nahmun ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; e\n.tane ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; o\nyasho ---&gt; d\nashod ---&gt; h\nshodh ---&gt; a\nhodha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; q\nsariq ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; w\n.ishw ---&gt; a\nishwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; t\n.malt ---&gt; i\nmalti ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; d\n.abid ---&gt; a\nabida ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; r\n.bajr ---&gt; a\nbajra ---&gt; n\najran ---&gt; g\njrang ---&gt; i\nrangi ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; a\n.nosa ---&gt; r\nnosar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; w\n.ragw ---&gt; e\nragwe ---&gt; n\nagwen ---&gt; d\ngwend ---&gt; e\nwende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; d\n.sadd ---&gt; a\nsadda ---&gt; m\naddam ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; p\n.rupp ---&gt; a\nruppa ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; t\n.swet ---&gt; a\nsweta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; d\nsayad ---&gt; a\nayada ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; m\n.gurm ---&gt; e\ngurme ---&gt; e\nurmee ---&gt; t\nrmeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; k\nashik ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; e\nsabre ---&gt; e\nabree ---&gt; n\nbreen ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; y\npappy ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; o\n..pro ---&gt; m\n.prom ---&gt; i\npromi ---&gt; l\nromil ---&gt; a\nomila ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; u\n.angu ---&gt; r\nangur ---&gt; i\nnguri ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; t\n..alt ---&gt; a\n.alta ---&gt; b\naltab ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; n\nagwan ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; j\n..sej ---&gt; a\n.seja ---&gt; n\nsejan ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; h\narash ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; e\n.jame ---&gt; e\njamee ---&gt; r\nameer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; e\nrajbe ---&gt; e\najbee ---&gt; r\njbeer ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; r\n.angr ---&gt; e\nangre ---&gt; g\nngreg ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; i\n..koi ---&gt; l\n.koil ---&gt; u\nkoilu ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; o\n.afro ---&gt; z\nafroz ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; o\n..joo ---&gt; h\n.jooh ---&gt; i\njoohi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; a\n.mura ---&gt; r\nmurar ---&gt; i\nurari ---&gt; l\nraril ---&gt; a\narila ---&gt; l\nrilal ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; n\n..adn ---&gt; a\n.adna ---&gt; n\nadnan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; n\n.sabn ---&gt; u\nsabnu ---&gt; r\nabnur ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; i\npinki ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; r\nsamar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; t\n.shit ---&gt; a\nshita ---&gt; l\nhital ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; u\nbittu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; a\n.raha ---&gt; t\nrahat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; j\n.marj ---&gt; i\nmarji ---&gt; n\narjin ---&gt; a\nrjina ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; s\n..des ---&gt; h\n.desh ---&gt; r\ndeshr ---&gt; a\neshra ---&gt; j\nshraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; w\n.jaiw ---&gt; a\njaiwa ---&gt; n\naiwan ---&gt; t\niwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; t\n.firt ---&gt; u\nfirtu ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; b\n..rob ---&gt; i\n.robi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; t\n..met ---&gt; i\n.meti ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; u\nimamu ---&gt; d\nmamud ---&gt; e\namude ---&gt; e\nmudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; y\n..diy ---&gt; a\n.diya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; u\n.manu ---&gt; v\nmanuv ---&gt; a\nanuva ---&gt; r\nnuvar ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; h\n.afsh ---&gt; a\nafsha ---&gt; r\nfshar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; n\n.sabn ---&gt; a\nsabna ---&gt; m\nabnam ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; n\n..amn ---&gt; a\n.amna ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; a\n.tina ---&gt; .\n..... ---&gt; e\n....e ---&gt; b\n...eb ---&gt; a\n..eba ---&gt; n\n.eban ---&gt; e\nebane ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; n\n.hann ---&gt; y\nhanny ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; d\n..sed ---&gt; a\n.seda ---&gt; r\nsedar ---&gt; a\nedara ---&gt; t\ndarat ---&gt; h\narath ---&gt; .\n..... ---&gt; w\n....w ---&gt; i\n...wi ---&gt; l\n..wil ---&gt; i\n.wili ---&gt; y\nwiliy ---&gt; a\niliya ---&gt; m\nliyam ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; v\n..siv ---&gt; a\n.siva ---&gt; m\nsivam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; a\nahaja ---&gt; d\nhajad ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; w\n.nirw ---&gt; a\nnirwa ---&gt; t\nirwat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; i\nparvi ---&gt; n\narvin ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; k\n..tak ---&gt; s\n.taks ---&gt; h\ntaksh ---&gt; i\nakshi ---&gt; l\nkshil ---&gt; a\nshila ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; t\n.satt ---&gt; a\nsatta ---&gt; r\nattar ---&gt; a\nttara ---&gt; m\ntaram ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; i\n.mami ---&gt; n\nmamin ---&gt; i\namini ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; k\nbhark ---&gt; h\nharkh ---&gt; a\narkha ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; n\nkishn ---&gt; a\nishna ---&gt; r\nshnar ---&gt; a\nhnara ---&gt; m\nnaram ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; e\n.vine ---&gt; y\nviney ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; a\n.kawa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; h\nraish ---&gt; m\naishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; h\ndilsh ---&gt; a\nilsha ---&gt; n\nlshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; j\nriyaj ---&gt; u\niyaju ---&gt; d\nyajud ---&gt; i\najudi ---&gt; n\njudin ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; m\n..bim ---&gt; l\n.biml ---&gt; e\nbimle ---&gt; s\nimles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; n\n.kesn ---&gt; a\nkesna ---&gt; t\nesnat ---&gt; a\nsnata ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; s\nmanis ---&gt; h\nanish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; k\n.anok ---&gt; h\nanokh ---&gt; a\nnokha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; n\nbadan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; t\nhakut ---&gt; a\nakuta ---&gt; l\nkutal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; j\n.birj ---&gt; u\nbirju ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; t\nlalit ---&gt; a\nalita ---&gt; l\nlital ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; a\n.dula ---&gt; r\ndular ---&gt; i\nulari ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; h\n.sikh ---&gt; a\nsikha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; u\nrajku ---&gt; m\najkum ---&gt; a\njkuma ---&gt; r\nkumar ---&gt; m\numarm ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; e\nratne ---&gt; s\natnes ---&gt; h\ntnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; a\n.suba ---&gt; t\nsubat ---&gt; o\nubato ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; r\n.mehr ---&gt; a\nmehra ---&gt; j\nehraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; y\n.rosy ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; .\n..... ---&gt; z\n....z ---&gt; u\n...zu ---&gt; h\n..zuh ---&gt; a\n.zuha ---&gt; i\nzuhai ---&gt; b\nuhaib ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; l\nnshul ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; d\ndevid ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; r\n.shor ---&gt; a\nshora ---&gt; v\nhorav ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; a\n.dana ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; i\npriti ---&gt; b\nritib ---&gt; h\nitibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; a\n.shra ---&gt; w\nshraw ---&gt; a\nhrawa ---&gt; n\nrawan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; o\nmango ---&gt; l\nangol ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; s\n.roos ---&gt; h\nroosh ---&gt; i\nooshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; a\nsabra ---&gt; j\nabraj ---&gt; e\nbraje ---&gt; e\nrajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; a\nrasha ---&gt; b\nashab ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; u\nhusbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; m\nashim ---&gt; i\nshimi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; t\n.bhot ---&gt; r\nbhotr ---&gt; a\nhotra ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; i\n.moti ---&gt; l\nmotil ---&gt; a\notila ---&gt; l\ntilal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; h\namesh ---&gt; w\nmeshw ---&gt; e\neshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; i\n.zahi ---&gt; d\nzahid ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; r\nharpr ---&gt; i\narpri ---&gt; t\nrprit ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; a\n.maya ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; c\nmoolc ---&gt; h\noolch ---&gt; a\nolcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; r\nbabur ---&gt; a\nabura ---&gt; m\nburam ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; a\nbhola ---&gt; r\nholar ---&gt; a\nolara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; h\n..keh ---&gt; k\n.kehk ---&gt; a\nkehka ---&gt; s\nehkas ---&gt; a\nhkasa ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; s\n..als ---&gt; e\n.alse ---&gt; e\nalsee ---&gt; p\nlseep ---&gt; a\nseepa ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; v\nhyamv ---&gt; e\nyamve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; f\n.mehf ---&gt; o\nmehfo ---&gt; o\nehfoo ---&gt; j\nhfooj ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; e\n.amie ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; j\n..amj ---&gt; a\n.amja ---&gt; t\namjat ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; n\n.reen ---&gt; a\nreena ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; p\n..irp ---&gt; h\n.irph ---&gt; a\nirpha ---&gt; n\nrphan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; t\nsawat ---&gt; r\nawatr ---&gt; i\nwatri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; a\nprama ---&gt; l\nramal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; t\nsheet ---&gt; a\nheeta ---&gt; l\neetal ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; i\nvashi ---&gt; l\nashil ---&gt; a\nshila ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; t\nasmit ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; n\n.akan ---&gt; s\nakans ---&gt; h\nkansh ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; i\n.davi ---&gt; n\ndavin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; i\n.sati ---&gt; s\nsatis ---&gt; h\natish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; n\n.supn ---&gt; a\nsupna ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; h\nhursh ---&gt; e\nurshe ---&gt; e\nrshee ---&gt; d\nsheed ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; a\nrhana ---&gt; z\nhanaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; n\nishan ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; u\n.alau ---&gt; d\nalaud ---&gt; i\nlaudi ---&gt; n\naudin ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; t\n.joyt ---&gt; i\njoyti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; c\npremc ---&gt; h\nremch ---&gt; a\nemcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; a\nijaya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; l\n.tasl ---&gt; e\ntasle ---&gt; e\naslee ---&gt; m\nsleem ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; e\ngyane ---&gt; n\nyanen ---&gt; d\nanend ---&gt; r\nnendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; d\nrazid ---&gt; a\nazida ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; e\n.vere ---&gt; n\nveren ---&gt; d\nerend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; s\n.ashs ---&gt; i\nashsi ---&gt; s\nshsis ---&gt; h\nhsish ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; m\n.jism ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; y\n..dey ---&gt; j\n.deyj ---&gt; i\ndeyji ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; a\nkhusa ---&gt; b\nhusab ---&gt; o\nusabo ---&gt; o\nsaboo ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; k\n.ruck ---&gt; s\nrucks ---&gt; a\nucksa ---&gt; n\ncksan ---&gt; a\nksana ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; o\n..koo ---&gt; k\n.kook ---&gt; i\nkooki ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; k\n.beek ---&gt; a\nbeeka ---&gt; r\neekar ---&gt; .\n..... ---&gt; f\n....f ---&gt; r\n...fr ---&gt; r\n..frr ---&gt; a\n.frra ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; a\n.basa ---&gt; n\nbasan ---&gt; t\nasant ---&gt; i\nsanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; i\nanshi ---&gt; k\nnshik ---&gt; a\nshika ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; n\n.mehn ---&gt; a\nmehna ---&gt; j\nehnaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; l\n.anil ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; l\nrupal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; i\n.babi ---&gt; t\nbabit ---&gt; a\nabita ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; u\n.rinu ---&gt; k\nrinuk ---&gt; a\ninuka ---&gt; n\nnukan ---&gt; w\nukanw ---&gt; r\nkanwr ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; j\naramj ---&gt; i\nramji ---&gt; t\namjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; u\n.suku ---&gt; m\nsukum ---&gt; a\nukuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; m\n.farm ---&gt; a\nfarma ---&gt; n\narman ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; l\n.ball ---&gt; u\nballu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; w\n..gaw ---&gt; r\n.gawr ---&gt; i\ngawri ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; m\nbalam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; t\n.shat ---&gt; i\nshati ---&gt; s\nhatis ---&gt; h\natish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; i\n.kavi ---&gt; l\nkavil ---&gt; a\navila ---&gt; s\nvilas ---&gt; h\nilash ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; e\n..twe ---&gt; n\n.twen ---&gt; k\ntwenk ---&gt; l\nwenkl ---&gt; e\nenkle ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; a\n.amra ---&gt; t\namrat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; n\nsaben ---&gt; o\nabeno ---&gt; o\nbenoo ---&gt; r\nenoor ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; s\n.bans ---&gt; i\nbansi ---&gt; l\nansil ---&gt; a\nnsila ---&gt; l\nsilal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; e\n.hase ---&gt; e\nhasee ---&gt; n\naseen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; i\nsandi ---&gt; p\nandip ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; r\nsagir ---&gt; a\nagira ---&gt; n\ngiran ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; n\nyogen ---&gt; d\nogend ---&gt; r\ngendr ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; b\n.roob ---&gt; i\nroobi ---&gt; .\n..... ---&gt; a\n....a ---&gt; e\n...ae ---&gt; s\n..aes ---&gt; h\n.aesh ---&gt; a\naesha ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; b\n..arb ---&gt; a\n.arba ---&gt; n\narban ---&gt; a\nrbana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; a\nradha ---&gt; b\nadhab ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; i\nchoti ---&gt; b\nhotib ---&gt; a\notiba ---&gt; i\ntibai ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; y\n.kany ---&gt; a\nkanya ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; a\nkunda ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; p\n..arp ---&gt; n\n.arpn ---&gt; a\narpna ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; j\n..vaj ---&gt; i\n.vaji ---&gt; d\nvajid ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; h\nhusbh ---&gt; o\nusbho ---&gt; o\nsbhoo ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; e\npyare ---&gt; l\nyarel ---&gt; a\narela ---&gt; l\nrelal ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; t\nchint ---&gt; a\nhinta ---&gt; n\nintan ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; a\n..aya ---&gt; s\n.ayas ---&gt; h\nayash ---&gt; a\nyasha ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; t\n..akt ---&gt; r\n.aktr ---&gt; i\naktri ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; t\n.dipt ---&gt; i\ndipti ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; s\n..jes ---&gt; m\n.jesm ---&gt; i\njesmi ---&gt; n\nesmin ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; i\n.sehi ---&gt; n\nsehin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; v\n.satv ---&gt; i\nsatvi ---&gt; d\natvid ---&gt; a\ntvida ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; o\n.chho ---&gt; t\nchhot ---&gt; e\nhhote ---&gt; l\nhotel ---&gt; a\notela ---&gt; l\ntelal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; u\n.matu ---&gt; l\nmatul ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; e\n.amre ---&gt; e\namree ---&gt; k\nmreek ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; h\n.sunh ---&gt; a\nsunha ---&gt; r\nunhar ---&gt; a\nnhara ---&gt; .\n..... ---&gt; t\n....t ---&gt; y\n...ty ---&gt; a\n..tya ---&gt; r\n.tyar ---&gt; a\ntyara ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; p\n.lavp ---&gt; r\nlavpr ---&gt; e\navpre ---&gt; e\nvpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; s\nrupes ---&gt; h\nupesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; a\n.ansa ---&gt; l\nansal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; a\n.nawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; a\n.sala ---&gt; u\nsalau ---&gt; d\nalaud ---&gt; d\nlaudd ---&gt; i\nauddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; k\ndipak ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; e\n.gane ---&gt; s\nganes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; a\n.lala ---&gt; n\nlalan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; n\n.sapn ---&gt; a\nsapna ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; i\nharmi ---&gt; t\narmit ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; i\nparti ---&gt; b\nartib ---&gt; h\nrtibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; i\nnasri ---&gt; n\nasrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; f\n..suf ---&gt; i\n.sufi ---&gt; y\nsufiy ---&gt; a\nufiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; k\n.anik ---&gt; e\nanike ---&gt; t\nniket ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; o\nshalo ---&gt; o\nhaloo ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; l\n.heml ---&gt; a\nhemla ---&gt; t\nemlat ---&gt; a\nmlata ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; u\n..abu ---&gt; l\n.abul ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; i\n.ishi ---&gt; t\nishit ---&gt; a\nshita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; g\n.birg ---&gt; e\nbirge ---&gt; s\nirges ---&gt; h\nrgesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; a\n..pha ---&gt; l\n.phal ---&gt; g\nphalg ---&gt; u\nhalgu ---&gt; n\nalgun ---&gt; i\nlguni ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; r\n..mir ---&gt; a\n.mira ---&gt; z\nmiraz ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; u\n..mau ---&gt; s\n.maus ---&gt; a\nmausa ---&gt; m\nausam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; a\nkaila ---&gt; s\nailas ---&gt; h\nilash ---&gt; .\n..... ---&gt; o\n....o ---&gt; s\n...os ---&gt; i\n..osi ---&gt; e\n.osie ---&gt; r\nosier ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; f\n.arif ---&gt; a\narifa ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; f\n..aff ---&gt; a\n.affa ---&gt; n\naffan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; i\n.fati ---&gt; m\nfatim ---&gt; a\natima ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; h\n.barh ---&gt; a\nbarha ---&gt; m\narham ---&gt; .\n..... ---&gt; k\n....k ---&gt; l\n...kl ---&gt; u\n..klu ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; h\n..neh ---&gt; a\n.neha ---&gt; l\nnehal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; a\n.anka ---&gt; l\nankal ---&gt; a\nnkala ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; a\nnanda ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; j\namalj ---&gt; e\nmalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; e\n.bhee ---&gt; m\nbheem ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; v\nharmv ---&gt; e\narmve ---&gt; e\nrmvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; z\nahnaz ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; r\nulfar ---&gt; a\nlfara ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; t\n.mont ---&gt; i\nmonti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; o\nsabbo ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; n\nsafin ---&gt; a\nafina ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; v\n.neev ---&gt; e\nneeve ---&gt; n\neeven ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; s\n..sis ---&gt; h\n.sish ---&gt; p\nsishp ---&gt; a\nishpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; s\n..nus ---&gt; r\n.nusr ---&gt; a\nnusra ---&gt; t\nusrat ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; n\n..ron ---&gt; y\n.rony ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; u\n.shku ---&gt; n\nshkun ---&gt; d\nhkund ---&gt; l\nkundl ---&gt; a\nundla ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; o\nmanto ---&gt; s\nantos ---&gt; h\nntosh ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; d\n.kuld ---&gt; e\nkulde ---&gt; e\nuldee ---&gt; p\nldeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; s\nsaras ---&gt; w\narasw ---&gt; a\nraswa ---&gt; t\naswat ---&gt; i\nswati ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; h\n..tah ---&gt; i\n.tahi ---&gt; r\ntahir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; m\n.jasm ---&gt; i\njasmi ---&gt; n\nasmin ---&gt; a\nsmina ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; a\nhrama ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; e\n..dhe ---&gt; e\n.dhee ---&gt; r\ndheer ---&gt; a\nheera ---&gt; j\neeraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; j\n..roj ---&gt; m\n.rojm ---&gt; e\nrojme ---&gt; n\nojmen ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; y\n.ruby ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; o\n.kano ---&gt; k\nkanok ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; a\n.jaga ---&gt; t\njagat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; t\nsavit ---&gt; a\navita ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; a\nramsa ---&gt; g\namsag ---&gt; a\nmsaga ---&gt; r\nsagar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; a\nsaida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; y\nsanny ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; g\nshahg ---&gt; u\nhahgu ---&gt; j\nahguj ---&gt; t\nhgujt ---&gt; a\ngujta ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; n\nhahin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; e\njasve ---&gt; e\nasvee ---&gt; n\nsveen ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; o\n.choo ---&gt; t\nchoot ---&gt; u\nhootu ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; h\n.ajah ---&gt; a\najaha ---&gt; r\njahar ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; a\n.naza ---&gt; n\nnazan ---&gt; i\nazani ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; o\nbitto ---&gt; o\nittoo ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; d\nmahad ---&gt; e\nahade ---&gt; v\nhadev ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; a\nbhara ---&gt; t\nharat ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; c\n.prac ---&gt; h\nprach ---&gt; i\nrachi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; a\naksha ---&gt; y\nkshay ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; u\n.taru ---&gt; n\ntarun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; y\nsandy ---&gt; a\nandya ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; y\n.gudy ---&gt; a\ngudya ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; a\ndisha ---&gt; d\nishad ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; d\n.bund ---&gt; a\nbunda ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; m\nsonam ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; e\n.tale ---&gt; e\ntalee ---&gt; m\naleem ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; r\nlilar ---&gt; a\nilara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; k\nkamak ---&gt; s\namaks ---&gt; h\nmaksh ---&gt; y\nakshy ---&gt; a\nkshya ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; a\n..ira ---&gt; m\n.iram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; j\n.pamj ---&gt; e\npamje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; j\n.bhoj ---&gt; p\nbhojp ---&gt; a\nhojpa ---&gt; l\nojpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; z\n..siz ---&gt; a\n.siza ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; a\nshila ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; w\n.bahw ---&gt; a\nbahwa ---&gt; n\nahwan ---&gt; a\nhwana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; u\n.ritu ---&gt; n\nritun ---&gt; j\nitunj ---&gt; a\ntunja ---&gt; y\nunjay ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; y\n.divy ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; t\n.kart ---&gt; i\nkarti ---&gt; k\nartik ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; n\n.uman ---&gt; g\numang ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; a\njeeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; a\nratna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; a\n.asma ---&gt; n\nasman ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; h\n.nash ---&gt; i\nnashi ---&gt; m\nashim ---&gt; a\nshima ---&gt; .\n..... ---&gt; w\n....w ---&gt; i\n...wi ---&gt; l\n..wil ---&gt; k\n.wilk ---&gt; i\nwilki ---&gt; s\nilkis ---&gt; h\nlkish ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; i\n.muni ---&gt; a\nmunia ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; s\n.tabs ---&gt; s\ntabss ---&gt; u\nabssu ---&gt; m\nbssum ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; n\n.amin ---&gt; a\namina ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; m\nrishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; a\nprata ---&gt; p\nratap ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; s\nepans ---&gt; h\npansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; b\n.adib ---&gt; a\nadiba ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; f\n.kaif ---&gt; i\nkaifi ---&gt; y\naifiy ---&gt; a\nifiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; s\n.adis ---&gt; a\nadisa ---&gt; n\ndisan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; w\n..shw ---&gt; e\n.shwe ---&gt; t\nshwet ---&gt; a\nhweta ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; i\nnarai ---&gt; n\narain ---&gt; i\nraini ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; o\n.vino ---&gt; d\nvinod ---&gt; a\ninoda ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; t\n..het ---&gt; a\n.heta ---&gt; l\nhetal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; d\ngurud ---&gt; u\nurudu ---&gt; t\nrudut ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; u\n..fau ---&gt; j\n.fauj ---&gt; d\nfaujd ---&gt; a\naujda ---&gt; r\nujdar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; u\n.muku ---&gt; n\nmukun ---&gt; d\nukund ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; a\nrazia ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; t\numant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; f\nsaraf ---&gt; r\narafr ---&gt; a\nrafra ---&gt; j\nafraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; a\n.rafa ---&gt; t\nrafat ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; i\n.aadi ---&gt; t\naadit ---&gt; y\nadity ---&gt; a\nditya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; s\n.raks ---&gt; h\nraksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; m\n..mum ---&gt; t\n.mumt ---&gt; a\nmumta ---&gt; z\numtaz ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; e\nhanwe ---&gt; r\nanwer ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; j\n.binj ---&gt; a\nbinja ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; b\n..dab ---&gt; u\n.dabu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; t\n.bist ---&gt; o\nbisto ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; c\n.bacc ---&gt; h\nbacch ---&gt; e\nacche ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; b\n..dib ---&gt; a\n.diba ---&gt; k\ndibak ---&gt; a\nibaka ---&gt; r\nbakar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; l\nsafil ---&gt; a\nafila ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; e\n..ate ---&gt; e\n.atee ---&gt; k\nateek ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; s\nprems ---&gt; i\nremsi ---&gt; n\nemsin ---&gt; g\nmsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; a\nmanta ---&gt; s\nantas ---&gt; h\nntash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; n\nrahin ---&gt; a\nahina ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; t\n.srit ---&gt; a\nsrita ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; h\n.sheh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; p\nhramp ---&gt; a\nrampa ---&gt; l\nampal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; g\nushag ---&gt; r\nshagr ---&gt; a\nhagra ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; r\n.devr ---&gt; a\ndevra ---&gt; j\nevraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; a\nukhsa ---&gt; n\nkhsan ---&gt; a\nhsana ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; j\n..aij ---&gt; a\n.aija ---&gt; y\naijay ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; j\n.munj ---&gt; i\nmunji ---&gt; r\nunjir ---&gt; a\nnjira ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; a\nhisha ---&gt; k\nishak ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; c\nharic ---&gt; h\narich ---&gt; a\nricha ---&gt; n\nichan ---&gt; d\nchand ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; w\n.sanw ---&gt; a\nsanwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; d\n.saud ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; j\n.araj ---&gt; u\naraju ---&gt; n\nrajun ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; i\n..zai ---&gt; n\n.zain ---&gt; a\nzaina ---&gt; b\nainab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; i\nsanni ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; a\nvinda ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; s\n..yus ---&gt; u\n.yusu ---&gt; f\nyusuf ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; c\n.nisc ---&gt; h\nnisch ---&gt; a\nischa ---&gt; y\nschay ---&gt; a\nchaya ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; s\n.khas ---&gt; b\nkhasb ---&gt; h\nhasbh ---&gt; o\nasbho ---&gt; o\nsbhoo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; k\n..hak ---&gt; a\n.haka ---&gt; m\nhakam ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; r\n.jhar ---&gt; n\njharn ---&gt; a\nharna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; k\n.sukk ---&gt; h\nsukkh ---&gt; u\nukkhu ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; h\n..bhh ---&gt; a\n.bhha ---&gt; t\nbhhat ---&gt; u\nhhatu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; d\n.jayd ---&gt; u\njaydu ---&gt; r\naydur ---&gt; g\nydurg ---&gt; a\ndurga ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; e\njende ---&gt; r\nender ---&gt; i\nnderi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; d\natyad ---&gt; e\ntyade ---&gt; v\nyadev ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; l\nishal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; p\n.jaip ---&gt; a\njaipa ---&gt; l\naipal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; s\n.bhas ---&gt; k\nbhask ---&gt; e\nhaske ---&gt; r\nasker ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; j\n.faij ---&gt; i\nfaiji ---&gt; n\naijin ---&gt; a\nijina ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; s\n..ves ---&gt; a\n.vesa ---&gt; l\nvesal ---&gt; i\nesali ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; s\n.kais ---&gt; h\nkaish ---&gt; a\naisha ---&gt; v\nishav ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; d\n..awd ---&gt; e\n.awde ---&gt; s\nawdes ---&gt; h\nwdesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; t\n.sart ---&gt; a\nsarta ---&gt; j\nartaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; t\n.kast ---&gt; o\nkasto ---&gt; o\nastoo ---&gt; r\nstoor ---&gt; i\ntoori ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; d\n.sayd ---&gt; a\nsayda ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; r\n..iqr ---&gt; a\n.iqra ---&gt; m\niqram ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; n\n..chn ---&gt; d\n.chnd ---&gt; r\nchndr ---&gt; a\nhndra ---&gt; k\nndrak ---&gt; l\ndrakl ---&gt; a\nrakla ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; o\n.shyo ---&gt; r\nshyor ---&gt; a\nhyora ---&gt; m\nyoram ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; a\nmusta ---&gt; q\nustaq ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; i\nnathi ---&gt; y\nathiy ---&gt; a\nthiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; a\n..sna ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; i\n.nabi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; r\n.meer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; u\n.jayu ---&gt; t\njayut ---&gt; i\nayuti ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; m\n..pom ---&gt; a\n.poma ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; m\n.rasm ---&gt; i\nrasmi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; y\nkanhy ---&gt; a\nanhya ---&gt; l\nnhyal ---&gt; a\nhyala ---&gt; l\nyalal ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; v\n..rev ---&gt; t\n.revt ---&gt; i\nrevti ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; d\n.chid ---&gt; d\nchidd ---&gt; u\nhiddu ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; i\n.dali ---&gt; m\ndalim ---&gt; a\nalima ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; i\n.muli ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; l\n.tasl ---&gt; i\ntasli ---&gt; m\naslim ---&gt; a\nslima ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; t\n.shit ---&gt; t\nshitt ---&gt; a\nhitta ---&gt; l\nittal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; r\n.jamr ---&gt; u\njamru ---&gt; j\namruj ---&gt; a\nmruja ---&gt; h\nrujah ---&gt; a\nujaha ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; p\n.birp ---&gt; a\nbirpa ---&gt; l\nirpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; h\n..neh ---&gt; a\n.neha ---&gt; r\nnehar ---&gt; i\nehari ---&gt; k\nharik ---&gt; a\narika ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; r\n..ber ---&gt; a\n.bera ---&gt; g\nberag ---&gt; i\neragi ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; t\nghant ---&gt; o\nhanto ---&gt; l\nantol ---&gt; i\nntoli ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; s\nnitis ---&gt; h\nitish ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; .\n..... ---&gt; a\n....a ---&gt; c\n...ac ---&gt; h\n..ach ---&gt; i\n.achi ---&gt; n\nachin ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; j\nnderj ---&gt; i\nderji ---&gt; t\nerjit ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; k\nlilak ---&gt; i\nilaki ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; a\nndera ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; b\n.mehb ---&gt; o\nmehbo ---&gt; b\nehbob ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; a\n.gora ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; i\n.momi ---&gt; t\nmomit ---&gt; a\nomita ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; s\n.taps ---&gt; y\ntapsy ---&gt; a\napsya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; l\n.bhul ---&gt; a\nbhula ---&gt; e\nhulae ---&gt; e\nulaee ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; v\naramv ---&gt; i\nramvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; j\ntanuj ---&gt; a\nanuja ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; j\namarj ---&gt; i\nmarji ---&gt; t\narjit ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; u\naashu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; a\nsabba ---&gt; r\nabbar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; n\n.jain ---&gt; u\njainu ---&gt; b\nainub ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; j\n..snj ---&gt; a\n.snja ---&gt; n\nsnjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; a\neepaa ---&gt; k\nepaak ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; h\n.rekh ---&gt; w\nrekhw ---&gt; a\nekhwa ---&gt; n\nkhwan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; t\n.mitt ---&gt; h\nmitth ---&gt; u\nitthu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; a\n.arsa ---&gt; d\narsad ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; a\nmanga ---&gt; l\nangal ---&gt; a\nngala ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; m\n..lim ---&gt; c\n.limc ---&gt; a\nlimca ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; z\n..kaz ---&gt; a\n.kaza ---&gt; l\nkazal ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; l\n..all ---&gt; a\n.alla ---&gt; r\nallar ---&gt; a\nllara ---&gt; j\nlaraj ---&gt; i\naraji ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; a\nshada ---&gt; n\nhadan ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; l\nsuhal ---&gt; i\nuhali ---&gt; y\nhaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; e\nrubee ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; r\n..asr ---&gt; u\n.asru ---&gt; d\nasrud ---&gt; d\nsrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; n\n..dun ---&gt; d\n.dund ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; h\n..vah ---&gt; a\n.vaha ---&gt; b\nvahab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; u\n.saku ---&gt; n\nsakun ---&gt; t\nakunt ---&gt; a\nkunta ---&gt; l\nuntal ---&gt; a\nntala ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; i\nhishi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; i\nranji ---&gt; t\nanjit ---&gt; a\nnjita ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; r\nanjar ---&gt; a\nnjara ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; s\n.nars ---&gt; a\nnarsa ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; l\n.ladl ---&gt; i\nladli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; c\n.ramc ---&gt; h\nramch ---&gt; a\namcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; a\nbhura ---&gt; l\nhural ---&gt; a\nurala ---&gt; l\nralal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; j\n.harj ---&gt; i\nharji ---&gt; t\narjit ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; v\n..liv ---&gt; e\n.live ---&gt; r\nliver ---&gt; i\niveri ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; h\n.afsh ---&gt; a\nafsha ---&gt; n\nfshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; p\n..sip ---&gt; r\n.sipr ---&gt; a\nsipra ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; b\n.munb ---&gt; u\nmunbu ---&gt; r\nunbur ---&gt; a\nnbura ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; i\nnoori ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; r\n.sawr ---&gt; n\nsawrn ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; l\n..aal ---&gt; a\n.aala ---&gt; m\naalam ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; k\n.malk ---&gt; e\nmalke ---&gt; e\nalkee ---&gt; t\nlkeet ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; n\n..kin ---&gt; i\n.kini ---&gt; y\nkiniy ---&gt; a\niniya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; a\n.rafa ---&gt; l\nrafal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; n\n.sohn ---&gt; a\nsohna ---&gt; l\nohnal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; w\nshanw ---&gt; a\nhanwa ---&gt; z\nanwaz ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; e\nravee ---&gt; n\naveen ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; d\n.kuld ---&gt; e\nkulde ---&gt; e\nuldee ---&gt; l\nldeel ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; y\nsamiy ---&gt; a\namiya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; v\naghuv ---&gt; i\nghuvi ---&gt; r\nhuvir ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; e\n.muje ---&gt; e\nmujee ---&gt; b\nujeeb ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; h\nmuskh ---&gt; a\nuskha ---&gt; n\nskhan ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; n\ndipan ---&gt; s\nipans ---&gt; u\npansu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; j\nrabhj ---&gt; o\nabhjo ---&gt; t\nbhjot ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; h\nsavih ---&gt; a\naviha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; e\nprame ---&gt; e\nramee ---&gt; l\nameel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; a\n.jata ---&gt; n\njatan ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; s\n..das ---&gt; h\n.dash ---&gt; r\ndashr ---&gt; a\nashra ---&gt; t\nshrat ---&gt; h\nhrath ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; w\n.rizw ---&gt; a\nrizwa ---&gt; n\nizwan ---&gt; a\nzwana ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; h\n.mukh ---&gt; t\nmukht ---&gt; i\nukhti ---&gt; y\nkhtiy ---&gt; a\nhtiya ---&gt; r\ntiyar ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; v\n.tinv ---&gt; k\ntinvk ---&gt; a\ninvka ---&gt; l\nnvkal ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; t\n..jet ---&gt; a\n.jeta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; n\n.satn ---&gt; o\nsatno ---&gt; s\natnos ---&gt; h\ntnosh ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; r\nandrr ---&gt; a\nndrra ---&gt; m\ndrram ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; a\narsha ---&gt; d\nrshad ---&gt; .\n..... ---&gt; d\n....d ---&gt; r\n...dr ---&gt; u\n..dru ---&gt; g\n.drug ---&gt; a\ndruga ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; z\nsaroz ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; n\n.parn ---&gt; i\nparni ---&gt; t\narnit ---&gt; i\nrniti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; z\nhanaz ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; d\naveed ---&gt; a\nveeda ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; j\ngirij ---&gt; a\nirija ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; p\n..dep ---&gt; a\n.depa ---&gt; n\ndepan ---&gt; d\nepand ---&gt; e\npande ---&gt; r\nander ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; a\n.vira ---&gt; j\nviraj ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; p\n.pamp ---&gt; a\npampa ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; a\n.lata ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; e\n.amee ---&gt; r\nameer ---&gt; a\nmeera ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; u\nchitu ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; k\n..tuk ---&gt; a\n.tuka ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; t\n.neet ---&gt; u\nneetu ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; d\n..vad ---&gt; h\n.vadh ---&gt; i\nvadhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; s\n.vars ---&gt; a\nvarsa ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; i\n.jami ---&gt; r\njamir ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; j\n.pooj ---&gt; a\npooja ---&gt; d\noojad ---&gt; e\nojade ---&gt; v\njadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; v\n..giv ---&gt; i\n.givi ---&gt; n\ngivin ---&gt; d\nivind ---&gt; a\nvinda ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; a\nkaila ---&gt; s\nailas ---&gt; h\nilash ---&gt; i\nlashi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; p\nurajp ---&gt; a\nrajpa ---&gt; l\najpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; j\n.braj ---&gt; e\nbraje ---&gt; n\nrajen ---&gt; d\najend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; s\n.ghas ---&gt; e\nghase ---&gt; e\nhasee ---&gt; t\naseet ---&gt; a\nseeta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; a\nmanja ---&gt; r\nanjar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; m\nkushm ---&gt; i\nushmi ---&gt; t\nshmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; n\n.purn ---&gt; i\npurni ---&gt; m\nurnim ---&gt; a\nrnima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; n\nsantn ---&gt; a\nantna ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; u\nkeshu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; i\namiri ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; b\nshamb ---&gt; u\nhambu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; v\nrajiv ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; a\n.rina ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; j\n..rej ---&gt; a\n.reja ---&gt; u\nrejau ---&gt; l\nejaul ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; n\nmahen ---&gt; d\nahend ---&gt; e\nhende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; k\n.mank ---&gt; u\nmanku ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; r\n.beer ---&gt; u\nbeeru ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; u\nmansu ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; l\n..ikl ---&gt; a\n.ikla ---&gt; k\niklak ---&gt; h\nklakh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; n\n.jagn ---&gt; a\njagna ---&gt; t\nagnat ---&gt; h\ngnath ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; a\n.mona ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; s\n.ayus ---&gt; h\nayush ---&gt; i\nyushi ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; b\n..jeb ---&gt; a\n.jeba ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; i\n.gani ---&gt; t\nganit ---&gt; a\nanita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; v\nsatyv ---&gt; a\natyva ---&gt; t\ntyvat ---&gt; i\nyvati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; a\n.soba ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; v\nbhanv ---&gt; a\nhanva ---&gt; r\nanvar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; r\n.mudr ---&gt; i\nmudri ---&gt; k\nudrik ---&gt; a\ndrika ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; n\n..jon ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; r\nvikar ---&gt; m\nikarm ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; i\nbhagi ---&gt; p\nhagip ---&gt; u\nagipu ---&gt; r\ngipur ---&gt; i\nipuri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; y\n.pray ---&gt; a\npraya ---&gt; t\nrayat ---&gt; a\nayata ---&gt; n\nyatan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; n\n.ashn ---&gt; u\nashnu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; d\n..prd ---&gt; e\n.prde ---&gt; e\nprdee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; k\nhandk ---&gt; i\nandki ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; g\n.surg ---&gt; a\nsurga ---&gt; y\nurgay ---&gt; a\nrgaya ---&gt; n\ngayan ---&gt; i\nayani ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; w\nahnaw ---&gt; a\nhnawa ---&gt; j\nnawaj ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; h\n.vich ---&gt; i\nvichi ---&gt; t\nichit ---&gt; r\nchitr ---&gt; a\nhitra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; m\nurajm ---&gt; a\nrajma ---&gt; l\najmal ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; v\n..yuv ---&gt; r\n.yuvr ---&gt; a\nyuvra ---&gt; j\nuvraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; o\n.shro ---&gt; t\nshrot ---&gt; a\nhrota ---&gt; m\nrotam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; r\nsangr ---&gt; a\nangra ---&gt; m\nngram ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; e\nlaxme ---&gt; e\naxmee ---&gt; n\nxmeen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; a\n.jisa ---&gt; a\njisaa ---&gt; n\nisaan ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; p\n..pop ---&gt; i\n.popi ---&gt; n\npopin ---&gt; d\nopind ---&gt; e\npinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; k\n..zak ---&gt; i\n.zaki ---&gt; r\nzakir ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; e\n.kare ---&gt; s\nkares ---&gt; h\naresh ---&gt; a\nresha ---&gt; n\neshan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; u\nfaizu ---&gt; r\naizur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; q\nsafiq ---&gt; .\n..... ---&gt; a\n....a ---&gt; o\n...ao ---&gt; o\n..aoo ---&gt; s\n.aoos ---&gt; a\naoosa ---&gt; f\noosaf ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; j\n.sirj ---&gt; n\nsirjn ---&gt; a\nirjna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; l\n.ashl ---&gt; a\nashla ---&gt; m\nshlam ---&gt; .\n..... ---&gt; a\n....a ---&gt; x\n...ax ---&gt; a\n..axa ---&gt; t\n.axat ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; i\n.siri ---&gt; s\nsiris ---&gt; h\nirish ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; n\n.punn ---&gt; e\npunne ---&gt; t\nunnet ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; n\nulfan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; i\n.nahi ---&gt; d\nnahid ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; t\n..aft ---&gt; a\n.afta ---&gt; b\naftab ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; u\nmithu ---&gt; n\nithun ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; s\n.raks ---&gt; h\nraksh ---&gt; a\naksha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; e\n..sae ---&gt; s\n.saes ---&gt; h\nsaesh ---&gt; t\naesht ---&gt; a\neshta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; b\nsakib ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; t\nugant ---&gt; i\nganti ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; a\ndhira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; d\nsumed ---&gt; h\numedh ---&gt; a\nmedha ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; y\n.ajay ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; o\n.kuno ---&gt; d\nkunod ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; o\narmoo ---&gt; d\nrmood ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; d\n.jaid ---&gt; e\njaide ---&gt; e\naidee ---&gt; p\nideep ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; e\n.jare ---&gt; e\njaree ---&gt; n\nareen ---&gt; a\nreena ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; m\n..prm ---&gt; o\n.prmo ---&gt; d\nprmod ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; k\nnitik ---&gt; a\nitika ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; n\nvishn ---&gt; u\nishnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; t\nsabit ---&gt; a\nabita ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; i\n.haji ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; h\nshekh ---&gt; a\nhekha ---&gt; r\nekhar ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; i\n..joi ---&gt; t\n.joit ---&gt; y\njoity ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; t\n.bhat ---&gt; e\nbhate ---&gt; r\nhater ---&gt; i\nateri ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; o\nsubho ---&gt; d\nubhod ---&gt; h\nbhodh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; r\n.gajr ---&gt; a\ngajra ---&gt; j\najraj ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; l\n..bil ---&gt; k\n.bilk ---&gt; i\nbilki ---&gt; s\nilkis ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; n\nsuhan ---&gt; i\nuhani ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; b\n.misb ---&gt; a\nmisba ---&gt; h\nisbah ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; a\n.susa ---&gt; n\nsusan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; b\nchhab ---&gt; i\nhhabi ---&gt; l\nhabil ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; u\n..alu ---&gt; d\n.alud ---&gt; d\naludd ---&gt; i\nluddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; d\n.somd ---&gt; e\nsomde ---&gt; v\nomdev ---&gt; i\nmdevi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; u\nsaidu ---&gt; l\naidul ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; o\n..hoo ---&gt; r\n.hoor ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; h\nshadh ---&gt; i\nhadhi ---&gt; k\nadhik ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; a\nmunna ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; n\nbhawn ---&gt; a\nhawna ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; d\n..ked ---&gt; a\n.keda ---&gt; r\nkedar ---&gt; m\nedarm ---&gt; a\ndarma ---&gt; l\narmal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; e\n..ane ---&gt; e\n.anee ---&gt; t\naneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; t\n..snt ---&gt; o\n.snto ---&gt; s\nsntos ---&gt; h\nntosh ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; f\n.darf ---&gt; a\ndarfa ---&gt; s\narfas ---&gt; h\nrfash ---&gt; a\nfasha ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; i\n.aari ---&gt; f\naarif ---&gt; a\narifa ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; n\n.avin ---&gt; a\navina ---&gt; s\nvinas ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; v\n.jaiv ---&gt; i\njaivi ---&gt; n\naivin ---&gt; d\nivind ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; a\nashma ---&gt; n\nshman ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; n\nraman ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; a\n.roha ---&gt; a\nrohaa ---&gt; n\nohaan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; i\n.mami ---&gt; t\nmamit ---&gt; a\namita ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; k\n.milk ---&gt; h\nmilkh ---&gt; i\nilkhi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; o\n.lato ---&gt; o\nlatoo ---&gt; r\natoor ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; m\n..zam ---&gt; e\n.zame ---&gt; e\nzamee ---&gt; r\nameer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; f\nsarif ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; b\nshabb ---&gt; i\nhabbi ---&gt; r\nabbir ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; a\ntulsa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; r\nrampr ---&gt; a\nampra ---&gt; s\nmpras ---&gt; a\nprasa ---&gt; d\nrasad ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; m\namarm ---&gt; u\nmarmu ---&gt; l\narmul ---&gt; a\nrmula ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; s\ngulfs ---&gt; a\nulfsa ---&gt; l\nlfsal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; u\n.garu ---&gt; p\ngarup ---&gt; a\narupa ---&gt; l\nrupal ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; i\n.dari ---&gt; y\ndariy ---&gt; a\nariya ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; k\n.tulk ---&gt; i\ntulki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; v\naghuv ---&gt; e\nghuve ---&gt; e\nhuvee ---&gt; r\nuveer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; o\n.ramo ---&gt; t\nramot ---&gt; a\namota ---&gt; r\nmotar ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; a\n.rija ---&gt; k\nrijak ---&gt; p\nijakp ---&gt; a\njakpa ---&gt; l\nakpal ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; e\n.fare ---&gt; e\nfaree ---&gt; m\nareem ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; e\nrajne ---&gt; s\najnes ---&gt; h\njnesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; e\n.jube ---&gt; d\njubed ---&gt; a\nubeda ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; i\n.husi ---&gt; n\nhusin ---&gt; a\nusina ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; e\n.mune ---&gt; n\nmunen ---&gt; d\nunend ---&gt; e\nnende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; t\n.sawt ---&gt; r\nsawtr ---&gt; i\nawtri ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; i\nreeti ---&gt; m\neetim ---&gt; a\netima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; s\n.sars ---&gt; a\nsarsa ---&gt; w\narsaw ---&gt; a\nrsawa ---&gt; t\nsawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; i\n.afri ---&gt; n\nafrin ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; k\n.dilk ---&gt; h\ndilkh ---&gt; u\nilkhu ---&gt; s\nlkhus ---&gt; h\nkhush ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; s\n.dars ---&gt; h\ndarsh ---&gt; n\narshn ---&gt; a\nrshna ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; r\n..chr ---&gt; a\n.chra ---&gt; n\nchran ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; n\nkishn ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; l\n..bel ---&gt; o\n.belo ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; k\narunk ---&gt; u\nrunku ---&gt; m\nunkum ---&gt; a\nnkuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; u\n..iqu ---&gt; b\n.iqub ---&gt; a\niquba ---&gt; l\nqubal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; n\n.pann ---&gt; a\npanna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; j\n.jagj ---&gt; i\njagji ---&gt; w\nagjiw ---&gt; a\ngjiwa ---&gt; n\njiwan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; e\nnajre ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; u\n.paru ---&gt; l\nparul ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; a\n..soa ---&gt; m\n.soam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; l\n.sahl ---&gt; e\nsahle ---&gt; s\nahles ---&gt; h\nhlesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; e\n.hite ---&gt; s\nhites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; e\n.vake ---&gt; s\nvakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; i\npravi ---&gt; n\nravin ---&gt; d\navind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; v\n..sev ---&gt; a\n.seva ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; j\n.shaj ---&gt; i\nshaji ---&gt; y\nhajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; p\nharmp ---&gt; a\narmpa ---&gt; l\nrmpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; e\n.sare ---&gt; e\nsaree ---&gt; n\nareen ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; s\n.muds ---&gt; s\nmudss ---&gt; i\nudssi ---&gt; r\ndssir ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; u\n.aayu ---&gt; s\naayus ---&gt; h\nayush ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; s\n.muds ---&gt; a\nmudsa ---&gt; y\nudsay ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; k\n.sahk ---&gt; i\nsahki ---&gt; r\nahkir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; h\nahajh ---&gt; a\nhajha ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; r\n.simr ---&gt; e\nsimre ---&gt; n\nimren ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; o\n.vino ---&gt; s\nvinos ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; i\naditi ---&gt; y\nditiy ---&gt; a\nitiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; d\n.sahd ---&gt; a\nsahda ---&gt; b\nahdab ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; a\n.kana ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; i\n..khi ---&gt; m\n.khim ---&gt; a\nkhima ---&gt; n\nhiman ---&gt; a\nimana ---&gt; n\nmanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; n\neghan ---&gt; a\nghana ---&gt; t\nhanat ---&gt; h\nanath ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; m\nalmam ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; n\n.mitn ---&gt; u\nmitnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; a\n.susa ---&gt; n\nsusan ---&gt; t\nusant ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; m\n..sem ---&gt; a\n.sema ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; j\n.aarj ---&gt; u\naarju ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; w\nvishw ---&gt; j\nishwj ---&gt; e\nshwje ---&gt; e\nhwjee ---&gt; t\nwjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; b\n..abb ---&gt; a\n.abba ---&gt; l\nabbal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; y\nsamay ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; d\n.bald ---&gt; e\nbalde ---&gt; v\naldev ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; i\nandri ---&gt; k\nndrik ---&gt; a\ndrika ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; a\nrajia ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; j\n.rimj ---&gt; h\nrimjh ---&gt; i\nimjhi ---&gt; m\nmjhim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; b\n..jab ---&gt; b\n.jabb ---&gt; a\njabba ---&gt; r\nabbar ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; t\n..avt ---&gt; a\n.avta ---&gt; r\navtar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; o\n.mato ---&gt; k\nmatok ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; u\n..dau ---&gt; a\n.daua ---&gt; d\ndauad ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; b\n.mudb ---&gt; i\nmudbi ---&gt; r\nudbir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; d\nshard ---&gt; a\nharda ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; a\nshila ---&gt; p\nhilap ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; o\nhusbo ---&gt; o\nusboo ---&gt; .\n..... ---&gt; u\n....u ---&gt; s\n...us ---&gt; m\n..usm ---&gt; a\n.usma ---&gt; n\nusman ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; y\nabhay ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; m\nhamim ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; e\numare ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; i\n.mali ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; v\ndhurv ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; l\numanl ---&gt; t\nmanlt ---&gt; a\nanlta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; k\nsamik ---&gt; s\namiks ---&gt; h\nmiksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; r\n.babr ---&gt; a\nbabra ---&gt; m\nabram ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; m\nfoolm ---&gt; a\noolma ---&gt; y\nolmay ---&gt; a\nlmaya ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; i\n.dani ---&gt; s\ndanis ---&gt; t\nanist ---&gt; a\nnista ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; a\n.gyaa ---&gt; n\ngyaan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; h\n.nanh ---&gt; e\nnanhe ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; c\n.aanc ---&gt; h\naanch ---&gt; a\nancha ---&gt; l\nnchal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; e\nparte ---&gt; e\nartee ---&gt; k\nrteek ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; a\n.dula ---&gt; l\ndulal ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; o\n..deo ---&gt; u\n.deou ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; d\n..yad ---&gt; h\n.yadh ---&gt; o\nyadho ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; s\n.kirs ---&gt; h\nkirsh ---&gt; n\nirshn ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; s\n.rajs ---&gt; h\nrajsh ---&gt; r\najshr ---&gt; e\njshre ---&gt; e\nshree ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; e\nchote ---&gt; l\nhotel ---&gt; a\notela ---&gt; l\ntelal ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; n\n..din ---&gt; e\n.dine ---&gt; e\ndinee ---&gt; s\ninees ---&gt; h\nneesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; a\n.aana ---&gt; n\naanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; l\n.devl ---&gt; i\ndevli ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; a\nsabba ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; t\npritt ---&gt; a\nritta ---&gt; m\nittam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; u\n.banu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; l\n..gal ---&gt; i\n.gali ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; k\n..tek ---&gt; c\n.tekc ---&gt; h\ntekch ---&gt; a\nekcha ---&gt; n\nkchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; t\n..art ---&gt; h\n.arth ---&gt; i\narthi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; t\nrajit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; t\nshant ---&gt; a\nhanta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; i\n.mali ---&gt; k\nmalik ---&gt; a\nalika ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; m\ngulam ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; a\nhanwa ---&gt; r\nanwar ---&gt; i\nnwari ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; m\n.gurm ---&gt; i\ngurmi ---&gt; t\nurmit ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; b\n..arb ---&gt; a\n.arba ---&gt; j\narbaj ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; t\nishat ---&gt; k\nshatk ---&gt; a\nhatka ---&gt; r\natkar ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; u\n.jitu ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; t\neenat ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; l\ndevil ---&gt; a\nevila ---&gt; l\nvilal ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; b\n.nurb ---&gt; i\nnurbi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; u\nbishu ---&gt; n\nishun ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; b\n..jeb ---&gt; i\n.jebi ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; k\n.anik ---&gt; e\nanike ---&gt; s\nnikes ---&gt; h\nikesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; l\n.leel ---&gt; u\nleelu ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; o\n.kalo ---&gt; o\nkaloo ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; m\n..umm ---&gt; e\n.umme ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; m\n.faim ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; d\nashad ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; n\n.sayn ---&gt; a\nsayna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; i\n.jati ---&gt; n\njatin ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; a\n..aya ---&gt; n\n.ayan ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; l\nohanl ---&gt; a\nhanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; g\n.shag ---&gt; u\nshagu ---&gt; f\nhaguf ---&gt; t\naguft ---&gt; a\ngufta ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; r\neghar ---&gt; a\nghara ---&gt; j\nharaj ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; u\nvashu ---&gt; d\nashud ---&gt; e\nshude ---&gt; v\nhudev ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; u\n..mou ---&gt; s\n.mous ---&gt; i\nmousi ---&gt; n\nousin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; b\n.anub ---&gt; h\nanubh ---&gt; a\nnubha ---&gt; v\nubhav ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; r\n.saur ---&gt; a\nsaura ---&gt; b\naurab ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; u\n.mahu ---&gt; d\nmahud ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; u\n..kou ---&gt; s\n.kous ---&gt; h\nkoush ---&gt; i\noushi ---&gt; k\nushik ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; k\n.park ---&gt; a\nparka ---&gt; s\narkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; n\n..ajn ---&gt; a\n.ajna ---&gt; b\najnab ---&gt; i\njnabi ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; s\ngiris ---&gt; h\nirish ---&gt; i\nrishi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; p\n.dhap ---&gt; u\ndhapu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; d\n.mohd ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; e\n.jube ---&gt; r\njuber ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; i\n.nagi ---&gt; a\nnagia ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; r\nbudhr ---&gt; a\nudhra ---&gt; m\ndhram ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; m\n.khum ---&gt; l\nkhuml ---&gt; o\nhumlo ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; z\n.rimz ---&gt; i\nrimzi ---&gt; m\nimzim ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; m\n..rum ---&gt; a\n.ruma ---&gt; l\nrumal ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; j\n.farj ---&gt; a\nfarja ---&gt; n\narjan ---&gt; d\nrjand ---&gt; .\n..... ---&gt; a\n....a ---&gt; e\n...ae ---&gt; j\n..aej ---&gt; a\n.aeja ---&gt; z\naejaz ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; u\n.manu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; u\n..mau ---&gt; s\n.maus ---&gt; h\nmaush ---&gt; i\naushi ---&gt; d\nushid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; t\nsarit ---&gt; a\narita ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; e\n.amre ---&gt; e\namree ---&gt; t\nmreet ---&gt; a\nreeta ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; v\n.gorv ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; e\nsamee ---&gt; n\nameen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; j\n.nirj ---&gt; a\nnirja ---&gt; l\nirjal ---&gt; a\nrjala ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; a\njenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; g\n..dig ---&gt; a\n.diga ---&gt; m\ndigam ---&gt; b\nigamb ---&gt; e\ngambe ---&gt; r\namber ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; z\n..aaz ---&gt; a\n.aaza ---&gt; d\naazad ---&gt; i\nazadi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; b\naghub ---&gt; e\nghube ---&gt; e\nhubee ---&gt; r\nubeer ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; a\n.hara ---&gt; r\nharar ---&gt; a\narara ---&gt; t\nrarat ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; d\n..rid ---&gt; h\n.ridh ---&gt; i\nridhi ---&gt; m\nidhim ---&gt; a\ndhima ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; u\nkushu ---&gt; m\nushum ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; e\npyare ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; n\nsawan ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; f\n..irf ---&gt; a\n.irfa ---&gt; n\nirfan ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; b\n..deb ---&gt; a\n.deba ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; r\n..mer ---&gt; c\n.merc ---&gt; y\nmercy ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; w\n..jiw ---&gt; a\n.jiwa ---&gt; n\njiwan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; m\n.chum ---&gt; k\nchumk ---&gt; i\nhumki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; a\n.jaya ---&gt; n\njayan ---&gt; t\nayant ---&gt; i\nyanti ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; s\n.vars ---&gt; h\nvarsh ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; m\n..azm ---&gt; i\n.azmi ---&gt; r\nazmir ---&gt; a\nzmira ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; f\n.ramf ---&gt; a\nramfa ---&gt; r\namfar ---&gt; e\nmfare ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; n\nukhsn ---&gt; a\nkhsna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; h\nsandh ---&gt; y\nandhy ---&gt; a\nndhya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; l\n.mall ---&gt; i\nmalli ---&gt; k\nallik ---&gt; a\nllika ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; k\n.punk ---&gt; a\npunka ---&gt; j\nunkaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; z\n..afz ---&gt; a\n.afza ---&gt; j\nafzaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; r\n.sahr ---&gt; u\nsahru ---&gt; l\nahrul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; h\n.shoh ---&gt; a\nshoha ---&gt; n\nhohan ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; d\n.avid ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; u\ndhiru ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; s\n.umas ---&gt; a\numasa ---&gt; n\nmasan ---&gt; k\nasank ---&gt; a\nsanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; n\n.shun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; e\nangee ---&gt; n\nngeen ---&gt; a\ngeena ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; t\n..gat ---&gt; t\n.gatt ---&gt; u\ngattu ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; a\nushba ---&gt; r\nshbar ---&gt; i\nhbari ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; i\n.bobi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; k\n..jak ---&gt; h\n.jakh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; a\n.mura ---&gt; r\nmurar ---&gt; i\nurari ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; i\n.duli ---&gt; c\ndulic ---&gt; h\nulich ---&gt; a\nlicha ---&gt; n\nichan ---&gt; d\nchand ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; m\n.najm ---&gt; a\nnajma ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; i\n.lavi ---&gt; s\nlavis ---&gt; h\navish ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; l\n.viml ---&gt; a\nvimla ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; h\nvansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; r\n.sour ---&gt; a\nsoura ---&gt; b\nourab ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; u\n.lalu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; s\n.aays ---&gt; h\naaysh ---&gt; a\naysha ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; s\n..des ---&gt; h\n.desh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; i\nmandi ---&gt; r\nandir ---&gt; a\nndira ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; t\n.kunt ---&gt; e\nkunte ---&gt; s\nuntes ---&gt; h\nntesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; r\nshahr ---&gt; i\nhahri ---&gt; s\nahris ---&gt; h\nhrish ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; a\n.husa ---&gt; n\nhusan ---&gt; a\nusana ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; i\n..aji ---&gt; t\n.ajit ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; o\n.bhoo ---&gt; r\nbhoor ---&gt; i\nhoori ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; h\nshekh ---&gt; e\nhekhe ---&gt; r\nekher ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; y\nsamay ---&gt; d\namayd ---&gt; d\nmaydd ---&gt; i\nayddi ---&gt; n\nyddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; e\nramde ---&gt; v\namdev ---&gt; i\nmdevi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; n\n..den ---&gt; e\n.dene ---&gt; s\ndenes ---&gt; h\nenesh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; r\n.poor ---&gt; n\npoorn ---&gt; i\noorni ---&gt; m\nornim ---&gt; a\nrnima ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; i\n.vaki ---&gt; l\nvakil ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; m\n.yasm ---&gt; i\nyasmi ---&gt; n\nasmin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; o\n.anoo ---&gt; p\nanoop ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; o\n.bhoo ---&gt; p\nbhoop ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; n\nsahan ---&gt; a\nahana ---&gt; j\nhanaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; s\n.akas ---&gt; h\nakash ---&gt; y\nkashy ---&gt; a\nashya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; i\n.kali ---&gt; s\nkalis ---&gt; h\nalish ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; d\n.band ---&gt; u\nbandu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; i\nparmi ---&gt; n\narmin ---&gt; d\nrmind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; k\n.pusk ---&gt; a\npuska ---&gt; r\nuskar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; z\n..aaz ---&gt; a\n.aaza ---&gt; d\naazad ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; e\nkamle ---&gt; s\namles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; a\nnazma ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; r\nhishr ---&gt; a\nishra ---&gt; m\nshram ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; t\n.meet ---&gt; h\nmeeth ---&gt; u\neethu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; f\n..guf ---&gt; r\n.gufr ---&gt; a\ngufra ---&gt; n\nufran ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; v\n.bhiv ---&gt; a\nbhiva ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; o\n..amo ---&gt; t\n.amot ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; n\nkiran ---&gt; n\nirann ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; r\nhahir ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; a\n.kosa ---&gt; r\nkosar ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; e\n.sohe ---&gt; l\nsohel ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; l\n.naul ---&gt; a\nnaula ---&gt; k\naulak ---&gt; h\nulakh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; r\naamir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; n\nsajin ---&gt; i\najini ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; i\n..aji ---&gt; m\n.ajim ---&gt; a\najima ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; o\nmando ---&gt; t\nandot ---&gt; h\nndoth ---&gt; i\ndothi ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; d\n.gend ---&gt; u\ngendu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; n\n.nann ---&gt; u\nnannu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; h\n.mosh ---&gt; i\nmoshi ---&gt; n\noshin ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; m\n.fatm ---&gt; a\nfatma ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; g\nsuhag ---&gt; i\nuhagi ---&gt; n\nhagin ---&gt; i\nagini ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; g\n..dig ---&gt; v\n.digv ---&gt; i\ndigvi ---&gt; j\nigvij ---&gt; a\ngvija ---&gt; y\nvijay ---&gt; .\n..... ---&gt; o\n....o ---&gt; j\n...oj ---&gt; a\n..oja ---&gt; s\n.ojas ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; i\n.basi ---&gt; b\nbasib ---&gt; u\nasibu ---&gt; l\nsibul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; l\n.shul ---&gt; a\nshula ---&gt; l\nhulal ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; o\npramo ---&gt; o\nramoo ---&gt; d\namood ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; k\n.kumk ---&gt; u\nkumku ---&gt; m\numkum ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; e\nshoke ---&gt; s\nhokes ---&gt; h\nokesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; a\nbacha ---&gt; n\nachan ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; n\n..tun ---&gt; n\n.tunn ---&gt; i\ntunni ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; s\ndevas ---&gt; i\nevasi ---&gt; s\nvasis ---&gt; h\nasish ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; d\n.gird ---&gt; h\ngirdh ---&gt; a\nirdha ---&gt; r\nrdhar ---&gt; i\ndhari ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; m\n.lalm ---&gt; u\nlalmu ---&gt; n\nalmun ---&gt; i\nlmuni ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; e\nvishe ---&gt; s\nishes ---&gt; h\nshesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; k\nprink ---&gt; a\nrinka ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; o\n.mamo ---&gt; i\nmamoi ---&gt; t\namoit ---&gt; a\nmoita ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; a\n.naja ---&gt; r\nnajar ---&gt; a\najara ---&gt; n\njaran ---&gt; a\narana ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; d\n..abd ---&gt; u\n.abdu ---&gt; l\nabdul ---&gt; l\nbdull ---&gt; a\ndulla ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; a\n..eka ---&gt; t\n.ekat ---&gt; a\nekata ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; p\n.samp ---&gt; a\nsampa ---&gt; t\nampat ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; w\n.balw ---&gt; a\nbalwa ---&gt; n\nalwan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; u\nrinku ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; a\n.rena ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; e\n.hane ---&gt; e\nhanee ---&gt; f\naneef ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; u\nushbu ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; a\nrisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; a\nsanaa ---&gt; l\nanaal ---&gt; i\nnaali ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; b\n..hab ---&gt; i\n.habi ---&gt; b\nhabib ---&gt; a\nabiba ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; s\n.bans ---&gt; i\nbansi ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; a\numara ---&gt; g\nmarag ---&gt; e\narage ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; a\n..sna ---&gt; h\n.snah ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; e\nravee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; c\n.gurc ---&gt; h\ngurch ---&gt; a\nurcha ---&gt; r\nrchar ---&gt; a\nchara ---&gt; n\nharan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; i\nraisi ---&gt; n\naisin ---&gt; g\nising ---&gt; h\nsingh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; j\n.sarj ---&gt; u\nsarju ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; i\njasvi ---&gt; n\nasvin ---&gt; d\nsvind ---&gt; i\nvindi ---&gt; r\nindir ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; b\n..lib ---&gt; i\n.libi ---&gt; n\nlibin ---&gt; .\n..... ---&gt; d\n....d ---&gt; y\n...dy ---&gt; a\n..dya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; m\nanchm ---&gt; a\nnchma ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; u\nsamsu ---&gt; n\namsun ---&gt; g\nmsung ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; r\ntendr ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; d\nsamad ---&gt; h\namadh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; i\n.kasi ---&gt; s\nkasis ---&gt; h\nasish ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; g\n..jog ---&gt; e\n.joge ---&gt; n\njogen ---&gt; d\nogend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; b\nrajib ---&gt; u\najibu ---&gt; l\njibul ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; a\nmunna ---&gt; d\nunnad ---&gt; e\nnnade ---&gt; v\nnadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; s\n.suks ---&gt; h\nsuksh ---&gt; a\nuksha ---&gt; n\nkshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; r\n..azr ---&gt; u\n.azru ---&gt; d\nazrud ---&gt; d\nzrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; r\nidhar ---&gt; t\ndhart ---&gt; h\nharth ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; v\n.shav ---&gt; i\nshavi ---&gt; n\nhavin ---&gt; g\naving ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; s\n..fas ---&gt; r\n.fasr ---&gt; u\nfasru ---&gt; n\nasrun ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; i\n..bii ---&gt; t\n.biit ---&gt; u\nbiitu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; d\namand ---&gt; e\nmande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; k\n.halk ---&gt; u\nhalku ---&gt; j\nalkuj ---&gt; i\nlkuji ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; v\nharmv ---&gt; i\narmvi ---&gt; r\nrmvir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; m\nnasim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; a\nnanda ---&gt; n\nandan ---&gt; i\nndani ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; l\ndipal ---&gt; i\nipali ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; n\nvijen ---&gt; d\nijend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; s\n.muks ---&gt; n\nmuksn ---&gt; d\nuksnd ---&gt; i\nksndi ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; u\n.indu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; d\n.rajd ---&gt; e\nrajde ---&gt; v\najdev ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; r\ntendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; n\n.tarn ---&gt; u\ntarnu ---&gt; m\narnum ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; k\nmustk ---&gt; i\nustki ---&gt; .\n..... ---&gt; e\n....e ---&gt; l\n...el ---&gt; d\n..eld ---&gt; r\n.eldr ---&gt; o\neldro ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; i\n.aadi ---&gt; l\naadil ---&gt; .\n..... ---&gt; b\n....b ---&gt; s\n...bs ---&gt; r\n..bsr ---&gt; a\n.bsra ---&gt; m\nbsram ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; d\n.bagd ---&gt; a\nbagda ---&gt; i\nagdai ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; m\n.anam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; l\nushal ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; i\nmunni ---&gt; a\nunnia ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; a\nnanka ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; r\n.chir ---&gt; a\nchira ---&gt; g\nhirag ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; l\n..gil ---&gt; b\n.gilb ---&gt; a\ngilba ---&gt; h\nilbah ---&gt; a\nlbaha ---&gt; r\nbahar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; a\n.muka ---&gt; t\nmukat ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; h\n.prah ---&gt; l\nprahl ---&gt; a\nrahla ---&gt; d\nahlad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; j\naramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; t\n..dat ---&gt; a\n.data ---&gt; t\ndatat ---&gt; e\natate ---&gt; r\ntater ---&gt; y\natery ---&gt; a\nterya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; n\n.harn ---&gt; e\nharne ---&gt; e\narnee ---&gt; t\nrneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; r\n.sahr ---&gt; u\nsahru ---&gt; k\nahruk ---&gt; h\nhrukh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; u\nnasru ---&gt; l\nasrul ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; z\n..muz ---&gt; a\n.muza ---&gt; m\nmuzam ---&gt; i\nuzami ---&gt; i\nzamii ---&gt; l\namiil ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; d\n.jaid ---&gt; e\njaide ---&gt; v\naidev ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; n\ntaran ---&gt; j\naranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; n\nhahin ---&gt; a\nahina ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; i\n.ruki ---&gt; y\nrukiy ---&gt; a\nukiya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; r\nbabar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; h\namesh ---&gt; w\nmeshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; d\n.bund ---&gt; h\nbundh ---&gt; u\nundhu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; e\n.rave ---&gt; e\nravee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; b\n.surb ---&gt; h\nsurbh ---&gt; i\nurbhi ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; a\n..uja ---&gt; m\n.ujam ---&gt; a\nujama ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; u\n.haru ---&gt; n\nharun ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; a\n.prya ---&gt; n\npryan ---&gt; k\nryank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; j\n..waj ---&gt; u\n.waju ---&gt; d\nwajud ---&gt; d\najudd ---&gt; i\njuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; n\nrajan ---&gt; t\najant ---&gt; i\njanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; n\n.sain ---&gt; k\nsaink ---&gt; y\nainky ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; w\n.janw ---&gt; i\njanwi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; b\nambab ---&gt; u\nmbabu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; v\nsukhv ---&gt; i\nukhvi ---&gt; r\nkhvir ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; d\ngyand ---&gt; e\nyande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; j\n.virj ---&gt; i\nvirji ---&gt; n\nirjin ---&gt; i\nrjini ---&gt; y\njiniy ---&gt; a\niniya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; u\nnasru ---&gt; d\nasrud ---&gt; d\nsrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; j\n..ruj ---&gt; i\n.ruji ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; e\n.hame ---&gt; n\nhamen ---&gt; t\nament ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; k\n.sank ---&gt; a\nsanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; i\nbholi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; k\nravik ---&gt; a\navika ---&gt; n\nvikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; t\n.aast ---&gt; o\naasto ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; i\nguddi ---&gt; y\nuddiy ---&gt; a\nddiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; a\nsusha ---&gt; m\nusham ---&gt; a\nshama ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; d\n.perd ---&gt; e\nperde ---&gt; e\nerdee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; w\n..raw ---&gt; i\n.rawi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; m\njanam ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; r\n.harr ---&gt; y\nharry ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; a\nroopa ---&gt; m\noopam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; i\n.kani ---&gt; k\nkanik ---&gt; a\nanika ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; j\nbiraj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; m\narism ---&gt; a\nrisma ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; s\n..mes ---&gt; a\n.mesa ---&gt; k\nmesak ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; h\nraish ---&gt; a\naisha ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; l\nsohal ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; s\n.fors ---&gt; h\nforsh ---&gt; a\norsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; j\n.mahj ---&gt; a\nmahja ---&gt; b\nahjab ---&gt; i\nhjabi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; a\n.baga ---&gt; r\nbagar ---&gt; a\nagara ---&gt; m\ngaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; k\nshaik ---&gt; h\nhaikh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; y\n..pay ---&gt; a\n.paya ---&gt; n\npayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; u\n.lalu ---&gt; t\nlalut ---&gt; h\naluth ---&gt; a\nlutha ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; k\n.deek ---&gt; s\ndeeks ---&gt; h\neeksh ---&gt; a\neksha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; j\nshanj ---&gt; h\nhanjh ---&gt; a\nanjha ---&gt; n\nnjhan ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; a\n.sora ---&gt; v\nsorav ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; a\ndhana ---&gt; n\nhanan ---&gt; j\nananj ---&gt; a\nnanja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; y\n.nary ---&gt; a\nnarya ---&gt; n\naryan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; a\ndhura ---&gt; v\nhurav ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; i\n.nami ---&gt; t\nnamit ---&gt; a\namita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; a\n.sira ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; o\n.amro ---&gt; o\namroo ---&gt; t\nmroot ---&gt; i\nrooti ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; b\nchhab ---&gt; e\nhhabe ---&gt; e\nhabee ---&gt; l\nabeel ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; a\nmjana ---&gt; m\njanam ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; i\n.nadi ---&gt; m\nnadim ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; t\n.neet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; t\nsunet ---&gt; a\nuneta ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; j\n.sehj ---&gt; a\nsehja ---&gt; l\nehjal ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; i\n.dili ---&gt; p\ndilip ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; k\n.dilk ---&gt; u\ndilku ---&gt; s\nilkus ---&gt; h\nlkush ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; h\n..boh ---&gt; a\n.boha ---&gt; t\nbohat ---&gt; i\nohati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; a\n.lala ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; u\n.salu ---&gt; p\nsalup ---&gt; a\nalupa ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; a\n.laxa ---&gt; m\nlaxam ---&gt; i\naxami ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; r\n.aafr ---&gt; i\naafri ---&gt; n\nafrin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; p\nranip ---&gt; a\nanipa ---&gt; l\nnipal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; n\n.maan ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; a\ngaura ---&gt; v\naurav ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; e\n..ume ---&gt; s\n.umes ---&gt; h\numesh ---&gt; a\nmesha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; r\nshayr ---&gt; a\nhayra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; i\nshusi ---&gt; l\nhusil ---&gt; a\nusila ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; d\narshd ---&gt; e\nrshde ---&gt; e\nshdee ---&gt; p\nhdeep ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; p\n..dep ---&gt; a\n.depa ---&gt; l\ndepal ---&gt; i\nepali ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; s\n..sas ---&gt; h\n.sash ---&gt; i\nsashi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; i\naushi ---&gt; k\nushik ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; a\n.dula ---&gt; r\ndular ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; m\n..umm ---&gt; a\n.umma ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; l\n.rajl ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; .\n..... ---&gt; e\n....e ---&gt; n\n...en ---&gt; g\n..eng ---&gt; l\n.engl ---&gt; a\nengla ---&gt; s\nnglas ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; n\nudhan ---&gt; s\ndhans ---&gt; u\nhansu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; v\nkalav ---&gt; a\nalava ---&gt; t\nlavat ---&gt; i\navati ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; i\n.supi ---&gt; r\nsupir ---&gt; i\nupiri ---&gt; y\npiriy ---&gt; a\niriya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; v\n.visv ---&gt; a\nvisva ---&gt; j\nisvaj ---&gt; e\nsvaje ---&gt; e\nvajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; j\n.sukj ---&gt; v\nsukjv ---&gt; e\nukjve ---&gt; e\nkjvee ---&gt; r\njveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; t\nsavit ---&gt; r\navitr ---&gt; i\nvitri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; n\nrachn ---&gt; u\nachnu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; b\n..mub ---&gt; a\n.muba ---&gt; r\nmubar ---&gt; i\nubari ---&gt; k\nbarik ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; m\n.manm ---&gt; a\nmanma ---&gt; t\nanmat ---&gt; h\nnmath ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; p\npushp ---&gt; e\nushpe ---&gt; n\nshpen ---&gt; d\nhpend ---&gt; r\npendr ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; w\nmanow ---&gt; a\nanowa ---&gt; r\nnowar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; a\navina ---&gt; t\nvinat ---&gt; h\ninath ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; u\n.shku ---&gt; n\nshkun ---&gt; t\nhkunt ---&gt; a\nkunta ---&gt; l\nuntal ---&gt; a\nntala ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; a\nnatha ---&gt; n\nathan ---&gt; i\nthani ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; n\n..dun ---&gt; i\n.duni ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; y\n..fiy ---&gt; a\n.fiya ---&gt; z\nfiyaz ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; y\n..pay ---&gt; a\n.paya ---&gt; l\npayal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; j\n.birj ---&gt; e\nbirje ---&gt; s\nirjes ---&gt; h\nrjesh ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; b\nmohab ---&gt; b\nohabb ---&gt; a\nhabba ---&gt; t\nabbat ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; j\ntanuj ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; a\n.amra ---&gt; t\namrat ---&gt; a\nmrata ---&gt; l\nratal ---&gt; a\natala ---&gt; l\ntalal ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; i\n.khai ---&gt; r\nkhair ---&gt; u\nhairu ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; l\n.tril ---&gt; o\ntrilo ---&gt; k\nrilok ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; s\n.miss ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; p\n.silp ---&gt; a\nsilpa ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; k\n.kank ---&gt; u\nkanku ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; t\n.surt ---&gt; i\nsurti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; i\n.momi ---&gt; n\nmomin ---&gt; a\nomina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; p\n.sanp ---&gt; a\nsanpa ---&gt; t\nanpat ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; t\n.gant ---&gt; a\nganta ---&gt; n\nantan ---&gt; t\nntant ---&gt; r\ntantr ---&gt; a\nantra ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; e\n.site ---&gt; n\nsiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; b\n.balb ---&gt; i\nbalbi ---&gt; r\nalbir ---&gt; i\nlbiri ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; p\nhramp ---&gt; l\nrampl ---&gt; a\nampla ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; k\n.istk ---&gt; h\nistkh ---&gt; a\nstkha ---&gt; r\ntkhar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; u\nmansu ---&gt; r\nansur ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; i\n.radi ---&gt; k\nradik ---&gt; h\nadikh ---&gt; a\ndikha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; a\nsarva ---&gt; s\narvas ---&gt; h\nrvash ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; j\nulnaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; v\n..nov ---&gt; i\n.novi ---&gt; s\nnovis ---&gt; h\novish ---&gt; a\nvisha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; b\naghub ---&gt; i\nghubi ---&gt; r\nhubir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; d\nsajid ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; n\n.been ---&gt; u\nbeenu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; a\nmadha ---&gt; r\nadhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; e\n.sane ---&gt; h\nsaneh ---&gt; a\naneha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; t\nranit ---&gt; a\nanita ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; s\n.sris ---&gt; h\nsrish ---&gt; t\nrisht ---&gt; y\nishty ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; i\nsubhi ---&gt; y\nubhiy ---&gt; a\nbhiya ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; b\n..rob ---&gt; i\n.robi ---&gt; n\nrobin ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; m\n.resm ---&gt; i\nresmi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; a\nshela ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; a\nreeta ---&gt; m\neetam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; r\n.balr ---&gt; a\nbalra ---&gt; m\nalram ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; d\nsukhd ---&gt; e\nukhde ---&gt; v\nkhdev ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; a\nranja ---&gt; n\nanjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; t\n..ekt ---&gt; a\n.ekta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; a\nchata ---&gt; n\nhatan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; n\nfaran ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; t\n.jitt ---&gt; e\njitte ---&gt; n\nitten ---&gt; d\nttend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; y\n.kaly ---&gt; a\nkalya ---&gt; n\nalyan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; r\nsayar ---&gt; a\nayara ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; b\n.seeb ---&gt; a\nseeba ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; s\n.gars ---&gt; h\ngarsh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; u\nbadru ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; h\n.mish ---&gt; b\nmishb ---&gt; h\nishbh ---&gt; a\nshbha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; o\nrakho ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; i\nsabri ---&gt; n\nabrin ---&gt; a\nbrina ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; b\n.parb ---&gt; h\nparbh ---&gt; u\narbhu ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; o\n.yaso ---&gt; d\nyasod ---&gt; h\nasodh ---&gt; a\nsodha ---&gt; .\n..... ---&gt; f\n....f ---&gt; e\n...fe ---&gt; l\n..fel ---&gt; i\n.feli ---&gt; c\nfelic ---&gt; i\nelici ---&gt; a\nlicia ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; e\nparde ---&gt; s\nardes ---&gt; h\nrdesh ---&gt; i\ndeshi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; o\n..alo ---&gt; k\n.alok ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; w\nahnaw ---&gt; a\nhnawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; t\n.shat ---&gt; h\nshath ---&gt; i\nhathi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; l\n.seel ---&gt; a\nseela ---&gt; m\neelam ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; i\nroshi ---&gt; n\noshin ---&gt; i\nshini ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; n\ndipan ---&gt; s\nipans ---&gt; h\npansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; h\nmansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; p\n.simp ---&gt; i\nsimpi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; r\n.saur ---&gt; a\nsaura ---&gt; v\naurav ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; l\n.leel ---&gt; a\nleela ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; h\nramph ---&gt; a\nampha ---&gt; l\nmphal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; k\n..nek ---&gt; k\n.nekk ---&gt; i\nnekki ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; e\n.hite ---&gt; n\nhiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; a\nhusha ---&gt; l\nushal ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; w\n..omw ---&gt; a\n.omwa ---&gt; t\nomwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; d\n.mund ---&gt; e\nmunde ---&gt; r\nunder ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; s\n.bans ---&gt; h\nbansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; o\npinko ---&gt; o\ninkoo ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; h\n.afsh ---&gt; a\nafsha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; a\nshara ---&gt; d\nharad ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; y\nrinky ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; r\nnajir ---&gt; a\najira ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; p\nshamp ---&gt; a\nhampa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; e\n..aae ---&gt; n\n.aaen ---&gt; a\naaena ---&gt; b\naenab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; y\n.sany ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; s\n..lus ---&gt; h\n.lush ---&gt; i\nlushi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; a\nmunna ---&gt; w\nunnaw ---&gt; a\nnnawa ---&gt; r\nnawar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; n\n.saan ---&gt; i\nsaani ---&gt; y\naaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; f\n.kaif ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; k\ndevik ---&gt; a\nevika ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; l\n..aml ---&gt; e\n.amle ---&gt; s\namles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; k\nashak ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; w\n..paw ---&gt; a\n.pawa ---&gt; n\npawan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; i\n.jali ---&gt; l\njalil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; i\nnandi ---&gt; t\nandit ---&gt; a\nndita ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; i\n..aji ---&gt; m\n.ajim ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; e\n.ambe ---&gt; r\namber ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; a\n..kaa ---&gt; r\n.kaar ---&gt; t\nkaart ---&gt; i\naarti ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; a\n.puna ---&gt; m\npunam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; s\n..sas ---&gt; h\n.sash ---&gt; i\nsashi ---&gt; k\nashik ---&gt; a\nshika ---&gt; l\nhikal ---&gt; a\nikala ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; u\n.minu ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; j\n..nej ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; o\n..jho ---&gt; r\n.jhor ---&gt; a\njhora ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; e\nshele ---&gt; n\nhelen ---&gt; d\nelend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; t\n.seet ---&gt; a\nseeta ---&gt; l\neetal ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; s\n..irs ---&gt; h\n.irsh ---&gt; a\nirsha ---&gt; d\nrshad ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; k\n.mukk ---&gt; e\nmukke ---&gt; s\nukkes ---&gt; h\nkkesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; w\n..paw ---&gt; n\n.pawn ---&gt; a\npawna ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; h\njeeth ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; a\n.rita ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; h\n.mukh ---&gt; t\nmukht ---&gt; a\nukhta ---&gt; r\nkhtar ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; m\n.bhim ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; g\n..mog ---&gt; a\n.moga ---&gt; n\nmogan ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; a\n.reka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; w\n.rajw ---&gt; a\nrajwa ---&gt; t\najwat ---&gt; i\njwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; a\n.aata ---&gt; m\naatam ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; l\n.khal ---&gt; i\nkhali ---&gt; l\nhalil ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; w\n.kisw ---&gt; a\nkiswa ---&gt; r\niswar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; y\n..lay ---&gt; b\n.layb ---&gt; a\nlayba ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; r\n..smr ---&gt; i\n.smri ---&gt; t\nsmrit ---&gt; i\nmriti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; m\n.manm ---&gt; o\nmanmo ---&gt; h\nanmoh ---&gt; a\nnmoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; i\n.kavi ---&gt; t\nkavit ---&gt; a\navita ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; b\n..vib ---&gt; h\n.vibh ---&gt; u\nvibhu ---&gt; t\nibhut ---&gt; i\nbhuti ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; m\n.naim ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; p\n.swap ---&gt; a\nswapa ---&gt; n\nwapan ---&gt; e\napane ---&gt; s\npanes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; i\ntulsi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; i\n.moti ---&gt; l\nmotil ---&gt; a\notila ---&gt; a\ntilaa ---&gt; l\nilaal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; w\n.satw ---&gt; a\nsatwa ---&gt; n\natwan ---&gt; t\ntwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; a\nparva ---&gt; s\narvas ---&gt; h\nrvash ---&gt; i\nvashi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; r\nnazir ---&gt; u\naziru ---&gt; l\nzirul ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; u\n.monu ---&gt; p\nmonup ---&gt; a\nonupa ---&gt; l\nnupal ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; u\n..asu ---&gt; t\n.asut ---&gt; o\nasuto ---&gt; s\nsutos ---&gt; h\nutosh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; t\n.nait ---&gt; i\nnaiti ---&gt; k\naitik ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; w\n..isw ---&gt; e\n.iswe ---&gt; r\niswer ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; n\n..kon ---&gt; t\n.kont ---&gt; i\nkonti ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; i\nnathi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; u\n.hazu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; i\naruni ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; i\nramni ---&gt; h\namnih ---&gt; o\nmniho ---&gt; r\nnihor ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; h\n.sanh ---&gt; a\nsanha ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; e\nnoore ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; e\n.bake ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; e\n.sube ---&gt; s\nsubes ---&gt; h\nubesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; f\n.jaff ---&gt; a\njaffa ---&gt; r\naffar ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; i\ntulsi ---&gt; r\nulsir ---&gt; a\nlsira ---&gt; m\nsiram ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; r\nkeshr ---&gt; a\neshra ---&gt; m\nshram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; r\nparer ---&gt; n\narern ---&gt; a\nrerna ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; i\nmansi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; a\nchuna ---&gt; r\nhunar ---&gt; a\nunara ---&gt; m\nnaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; m\nsalim ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; a\nbhara ---&gt; t\nharat ---&gt; i\narati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; b\nlakhb ---&gt; i\nakhbi ---&gt; r\nkhbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; i\n.rupi ---&gt; n\nrupin ---&gt; d\nupind ---&gt; e\npinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; k\n..puk ---&gt; h\n.pukh ---&gt; a\npukha ---&gt; r\nukhar ---&gt; a\nkhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; m\n.khum ---&gt; a\nkhuma ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; d\n..umd ---&gt; a\n.umda ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; m\n..mum ---&gt; t\n.mumt ---&gt; a\nmumta ---&gt; j\numtaj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; i\nharbi ---&gt; n\narbin ---&gt; d\nrbind ---&gt; e\nbinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; d\n..bid ---&gt; h\n.bidh ---&gt; y\nbidhy ---&gt; a\nidhya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; a\nshaba ---&gt; n\nhaban ---&gt; a\nabana ---&gt; m\nbanam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; y\n.baby ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; m\nsharm ---&gt; i\nharmi ---&gt; l\narmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; d\n.budd ---&gt; h\nbuddh ---&gt; a\nuddha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; i\nbhavi ---&gt; s\nhavis ---&gt; h\navish ---&gt; y\nvishy ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; p\n..utp ---&gt; a\n.utpa ---&gt; n\nutpan ---&gt; a\ntpana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; t\nsagit ---&gt; a\nagita ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; v\n.garv ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; d\n.fird ---&gt; o\nfirdo ---&gt; s\nirdos ---&gt; h\nrdosh ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; r\n..war ---&gt; i\n.wari ---&gt; s\nwaris ---&gt; h\narish ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; e\n.gane ---&gt; s\nganes ---&gt; i\nanesi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; j\n.gurj ---&gt; e\ngurje ---&gt; e\nurjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; a\n.saba ---&gt; n\nsaban ---&gt; a\nabana ---&gt; m\nbanam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; r\nsahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; f\nashrf ---&gt; i\nshrfi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; e\n.name ---&gt; n\nnamen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; w\n.sarw ---&gt; a\nsarwa ---&gt; n\narwan ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; b\n..akb ---&gt; a\n.akba ---&gt; r\nakbar ---&gt; i\nkbari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; l\nhival ---&gt; i\nivali ---&gt; k\nvalik ---&gt; a\nalika ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; i\n.khai ---&gt; r\nkhair ---&gt; t\nhairt ---&gt; i\nairti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; r\nmohar ---&gt; a\nohara ---&gt; n\nharan ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; s\nyoges ---&gt; h\nogesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; u\n..atu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; e\n.hase ---&gt; e\nhasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; d\n..red ---&gt; h\n.redh ---&gt; e\nredhe ---&gt; m\nedhem ---&gt; a\ndhema ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; a\n.dila ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; w\n.jalw ---&gt; a\njalwa ---&gt; d\nalwad ---&gt; .\n..... ---&gt; v\n....v ---&gt; u\n...vu ---&gt; d\n..vud ---&gt; e\n.vude ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; e\njagde ---&gt; s\nagdes ---&gt; h\ngdesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; a\nahaja ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; b\n.gurb ---&gt; a\ngurba ---&gt; x\nurbax ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; n\n..tun ---&gt; d\n.tund ---&gt; i\ntundi ---&gt; r\nundir ---&gt; a\nndira ---&gt; m\ndiram ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; i\nruksi ---&gt; n\nuksin ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; n\nraven ---&gt; d\navend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; t\n.bhat ---&gt; a\nbhata ---&gt; r\nhatar ---&gt; i\natari ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; t\n..met ---&gt; h\n.meth ---&gt; u\nmethu ---&gt; n\nethun ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; n\nparin ---&gt; k\narink ---&gt; a\nrinka ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; l\nanjal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; t\n.lavt ---&gt; a\nlavta ---&gt; r\navtar ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; d\n.ajad ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; u\n.lilu ---&gt; .\n..... ---&gt; i\n....i ---&gt; f\n...if ---&gt; t\n..ift ---&gt; e\n.ifte ---&gt; s\niftes ---&gt; h\nftesh ---&gt; y\nteshy ---&gt; a\neshya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; b\nahnab ---&gt; a\nhnaba ---&gt; z\nnabaz ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; l\n.gajl ---&gt; a\ngajla ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; t\n..jot ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; b\n..alb ---&gt; i\n.albi ---&gt; n\nalbin ---&gt; a\nlbina ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; z\nfiroz ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; i\n.tali ---&gt; b\ntalib ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; i\nkanhi ---&gt; y\nanhiy ---&gt; a\nnhiya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; i\nanchi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; u\n.manu ---&gt; j\nmanuj ---&gt; a\nanuja ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; p\n..bap ---&gt; u\n.bapu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; m\n.sudm ---&gt; a\nsudma ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; s\n.hans ---&gt; h\nhansh ---&gt; r\nanshr ---&gt; a\nnshra ---&gt; j\nshraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; m\nrazim ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; w\natyaw ---&gt; a\ntyawa ---&gt; n\nyawan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; k\nmjank ---&gt; i\njanki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; k\n.jank ---&gt; r\njankr ---&gt; a\nankra ---&gt; j\nnkraj ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; p\nchanp ---&gt; a\nhanpa ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; m\n..gam ---&gt; e\n.game ---&gt; r\ngamer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; m\n.jagm ---&gt; a\njagma ---&gt; l\nagmal ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; u\n.yamu ---&gt; n\nyamun ---&gt; a\namuna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; n\n.asan ---&gt; a\nasana ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; a\n.sika ---&gt; n\nsikan ---&gt; d\nikand ---&gt; e\nkande ---&gt; r\nander ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; a\n.lava ---&gt; l\nlaval ---&gt; i\navali ---&gt; .\n..... ---&gt; h\n....h ---&gt; r\n...hr ---&gt; i\n..hri ---&gt; t\n.hrit ---&gt; h\nhrith ---&gt; i\nrithi ---&gt; k\nithik ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; e\n.muke ---&gt; s\nmukes ---&gt; h\nukesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; n\n.bhun ---&gt; d\nbhund ---&gt; k\nhundk ---&gt; i\nundki ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; n\narojn ---&gt; i\nrojni ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; o\n.rubo ---&gt; .\n..... ---&gt; o\n....o ---&gt; v\n...ov ---&gt; a\n..ova ---&gt; i\n.ovai ---&gt; s\novais ---&gt; h\nvaish ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; e\nchote ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; r\n.vikr ---&gt; a\nvikra ---&gt; m\nikram ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; i\nbudhi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; n\n.arin ---&gt; d\narind ---&gt; r\nrindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; m\n.nagm ---&gt; a\nnagma ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; d\n..hud ---&gt; i\n.hudi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; l\n.sujl ---&gt; i\nsujli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; m\n.ratm ---&gt; o\nratmo ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; j\n.bhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; a\nkamra ---&gt; a\namraa ---&gt; n\nmraan ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; l\n.doll ---&gt; e\ndolle ---&gt; y\nolley ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; l\nbabul ---&gt; a\nabula ---&gt; l\nbulal ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; m\n.yasm ---&gt; e\nyasme ---&gt; e\nasmee ---&gt; n\nsmeen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; u\n.ramu ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; i\ndivai ---&gt; e\nivaie ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; n\nandan ---&gt; i\nndani ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; f\n..suf ---&gt; i\n.sufi ---&gt; a\nsufia ---&gt; n\nufian ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; t\nlalit ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; b\ngulab ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; n\nasmin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; v\n.jagv ---&gt; i\njagvi ---&gt; r\nagvir ---&gt; i\ngviri ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; w\nshahw ---&gt; a\nhahwa ---&gt; j\nahwaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; n\n..jon ---&gt; i\n.joni ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; i\n.sohi ---&gt; l\nsohil ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; i\n.jari ---&gt; n\njarin ---&gt; a\narina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; e\nsamee ---&gt; m\nameem ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; a\n.laka ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; g\nchhag ---&gt; a\nhhaga ---&gt; n\nhagan ---&gt; a\nagana ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; p\npushp ---&gt; a\nushpa ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; a\n..tra ---&gt; n\n.tran ---&gt; n\ntrann ---&gt; u\nrannu ---&gt; m\nannum ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; q\n..faq ---&gt; i\n.faqi ---&gt; r\nfaqir ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; t\n..kit ---&gt; t\n.kitt ---&gt; i\nkitti ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; j\n..ujj ---&gt; a\n.ujja ---&gt; w\nujjaw ---&gt; a\njjawa ---&gt; l\njawal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; u\nlakhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; d\nugand ---&gt; h\ngandh ---&gt; a\nandha ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; u\n.begu ---&gt; m\nbegum ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; y\n.vidy ---&gt; a\nvidya ---&gt; w\nidyaw ---&gt; a\ndyawa ---&gt; t\nyawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; t\n.shit ---&gt; e\nshite ---&gt; e\nhitee ---&gt; l\niteel ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; e\nsushe ---&gt; l\nushel ---&gt; a\nshela ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; k\n..lek ---&gt; o\n.leko ---&gt; s\nlekos ---&gt; e\nekose ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; a\nkalpa ---&gt; d\nalpad ---&gt; m\nlpadm ---&gt; a\npadma ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; i\n..hai ---&gt; d\n.haid ---&gt; e\nhaide ---&gt; r\naider ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; r\n.meer ---&gt; a\nmeera ---&gt; m\neeram ---&gt; a\nerama ---&gt; i\nramai ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; n\nalman ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; a\ngolda ---&gt; n\noldan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; u\nramku ---&gt; m\namkum ---&gt; a\nmkuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; i\narshi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; w\ndhanw ---&gt; a\nhanwa ---&gt; n\nanwan ---&gt; t\nnwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; e\nmanje ---&gt; s\nanjes ---&gt; h\nnjesh ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; j\n.tanj ---&gt; i\ntanji ---&gt; m\nanjim ---&gt; a\nnjima ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; n\nmahin ---&gt; u\nahinu ---&gt; d\nhinud ---&gt; i\ninudi ---&gt; n\nnudin ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; n\n.badn ---&gt; a\nbadna ---&gt; t\nadnat ---&gt; h\ndnath ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; t\n.aast ---&gt; h\naasth ---&gt; a\nastha ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; b\n..omb ---&gt; i\n.ombi ---&gt; r\nombir ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; m\neelam ---&gt; .\n..... ---&gt; u\n....u ---&gt; n\n...un ---&gt; k\n..unk ---&gt; n\n.unkn ---&gt; o\nunkno ---&gt; w\nnknow ---&gt; n\nknown ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; a\namira ---&gt; k\nmirak ---&gt; a\niraka ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; n\nviren ---&gt; d\nirend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; i\n.rati ---&gt; k\nratik ---&gt; a\natika ---&gt; n\ntikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; a\n.sava ---&gt; n\nsavan ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; u\n.kumu ---&gt; d\nkumud ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; c\n.kanc ---&gt; h\nkanch ---&gt; i\nanchi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; b\nandab ---&gt; a\nndaba ---&gt; i\ndabai ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; s\n..ahs ---&gt; a\n.ahsa ---&gt; n\nahsan ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; v\n.palv ---&gt; i\npalvi ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; e\ngolde ---&gt; n\nolden ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; y\nsaziy ---&gt; a\naziya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; d\nugand ---&gt; b\ngandb ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; s\nprins ---&gt; h\nrinsh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; b\nshamb ---&gt; h\nhambh ---&gt; u\nambhu ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; s\n..nus ---&gt; r\n.nusr ---&gt; a\nnusra ---&gt; n\nusran ---&gt; t\nsrant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; r\nnazir ---&gt; a\nazira ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; r\n..nor ---&gt; t\n.nort ---&gt; i\nnorti ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; m\nkaram ---&gt; j\naramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; w\n.gajw ---&gt; a\ngajwa ---&gt; n\najwan ---&gt; t\njwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; s\nghans ---&gt; y\nhansy ---&gt; a\nansya ---&gt; m\nnsyam ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; e\n.anje ---&gt; e\nanjee ---&gt; v\nnjeev ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; m\n..hum ---&gt; e\n.hume ---&gt; r\nhumer ---&gt; a\numera ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; a\n..kaa ---&gt; r\n.kaar ---&gt; i\nkaari ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; o\n.damo ---&gt; d\ndamod ---&gt; a\namoda ---&gt; r\nmodar ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; b\n..beb ---&gt; y\n.beby ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; l\n.maal ---&gt; t\nmaalt ---&gt; i\naalti ---&gt; .\n..... ---&gt; g\n....g ---&gt; l\n...gl ---&gt; a\n..gla ---&gt; d\n.glad ---&gt; w\ngladw ---&gt; i\nladwi ---&gt; n\nadwin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; d\nsukhd ---&gt; e\nukhde ---&gt; v\nkhdev ---&gt; i\nhdevi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; i\n.hani ---&gt; s\nhanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; h\nmrith ---&gt; a\nritha ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; i\n.indi ---&gt; r\nindir ---&gt; a\nndira ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; d\nsahad ---&gt; e\nahade ---&gt; b\nhadeb ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; s\nkiras ---&gt; h\nirash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; m\n.mamm ---&gt; t\nmammt ---&gt; a\nammta ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; m\ndeepm ---&gt; a\neepma ---&gt; l\nepmal ---&gt; a\npmala ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; n\n.saan ---&gt; u\nsaanu ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; o\n..goo ---&gt; p\n.goop ---&gt; i\ngoopi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; n\nharen ---&gt; d\narend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; u\npappu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; a\nkalpa ---&gt; n\nalpan ---&gt; a\nlpana ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; i\n.juli ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; w\n.somw ---&gt; a\nsomwa ---&gt; t\nomwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; a\n.tila ---&gt; k\ntilak ---&gt; r\nilakr ---&gt; a\nlakra ---&gt; j\nakraj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; i\nharbi ---&gt; r\narbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; a\nreeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; h\nruksh ---&gt; a\nuksha ---&gt; r\nkshar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; r\nsamir ---&gt; o\namiro ---&gt; n\nmiron ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; m\n.jalm ---&gt; i\njalmi ---&gt; .\n..... ---&gt; n\n....n ---&gt; s\n...ns ---&gt; h\n..nsh ---&gt; i\n.nshi ---&gt; m\nnshim ---&gt; a\nshima ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; n\n.heen ---&gt; a\nheena ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; s\ninaks ---&gt; i\nnaksi ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; h\n.brah ---&gt; a\nbraha ---&gt; m\nraham ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; i\n.rini ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; r\n.savr ---&gt; e\nsavre ---&gt; e\navree ---&gt; n\nvreen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; r\n.ramr ---&gt; a\nramra ---&gt; t\namrat ---&gt; i\nmrati ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; a\n.java ---&gt; l\njaval ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; e\n..aje ---&gt; e\n.ajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; m\n..jim ---&gt; i\n.jimi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; b\n.satb ---&gt; i\nsatbi ---&gt; r\natbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; n\nmanen ---&gt; d\nanend ---&gt; e\nnende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; r\n.shir ---&gt; a\nshira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; a\n.sora ---&gt; b\nsorab ---&gt; h\norabh ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; r\nchetr ---&gt; a\nhetra ---&gt; m\netram ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; u\nchinu ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; t\nulsht ---&gt; a\nlshta ---&gt; b\nshtab ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; w\ndayaw ---&gt; a\nayawa ---&gt; t\nyawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; i\n..bui ---&gt; t\n.buit ---&gt; y\nbuity ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; i\nimani ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; y\n.sazy ---&gt; a\nsazya ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; a\nusuma ---&gt; k\nsumak ---&gt; a\numaka ---&gt; r\nmakar ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; s\n.alis ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; u\nnooru ---&gt; d\noorud ---&gt; d\norudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; i\n.gari ---&gt; b\ngarib ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; m\nmanim ---&gt; e\nanime ---&gt; g\nnimeg ---&gt; l\nimegl ---&gt; a\nmegla ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; s\n.nass ---&gt; i\nnassi ---&gt; n\nassin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; w\nnandw ---&gt; a\nandwa ---&gt; t\nndwat ---&gt; i\ndwati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; j\n..laj ---&gt; a\n.laja ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; h\n..mih ---&gt; i\n.mihi ---&gt; r\nmihir ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; v\nchhav ---&gt; i\nhhavi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; a\nlaxma ---&gt; n\naxman ---&gt; r\nxmanr ---&gt; a\nmanra ---&gt; m\nanram ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; a\n.rasa ---&gt; b\nrasab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; r\n.sajr ---&gt; u\nsajru ---&gt; d\najrud ---&gt; d\njrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; u\nlakhu ---&gt; a\nakhua ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; l\nshall ---&gt; u\nhallu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; e\n.sate ---&gt; e\nsatee ---&gt; s\natees ---&gt; h\nteesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; a\n.anga ---&gt; d\nangad ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; g\n.narg ---&gt; i\nnargi ---&gt; s\nargis ---&gt; h\nrgish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; e\nsunde ---&gt; e\nundee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; b\n.bobb ---&gt; i\nbobbi ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; f\n..zaf ---&gt; a\n.zafa ---&gt; r\nzafar ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; n\nrupan ---&gt; j\nupanj ---&gt; a\npanja ---&gt; l\nanjal ---&gt; i\nnjali ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; t\nampat ---&gt; i\nmpati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; h\n.nash ---&gt; i\nnashi ---&gt; m\nashim ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; m\n..bim ---&gt; a\n.bima ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; d\n..yad ---&gt; v\n.yadv ---&gt; i\nyadvi ---&gt; r\nadvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; t\nshant ---&gt; i\nhanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; a\nanura ---&gt; d\nnurad ---&gt; h\nuradh ---&gt; a\nradha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; w\nramsw ---&gt; r\namswr ---&gt; u\nmswru ---&gt; p\nswrup ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; z\n.sehz ---&gt; a\nsehza ---&gt; d\nehzad ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; a\nbhava ---&gt; n\nhavan ---&gt; a\navana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; p\nkalap ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; o\n.bato ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; r\n.supr ---&gt; i\nsupri ---&gt; y\nupriy ---&gt; a\npriya ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; t\n.indt ---&gt; a\nindta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; g\n.sahg ---&gt; u\nsahgu ---&gt; f\nahguf ---&gt; t\nhguft ---&gt; a\ngufta ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; o\n.viro ---&gt; n\nviron ---&gt; i\nironi ---&gt; k\nronik ---&gt; a\nonika ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; l\n..jil ---&gt; e\n.jile ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; a\n.gopa ---&gt; l\ngopal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; i\n.kali ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; k\n..duk ---&gt; h\n.dukh ---&gt; i\ndukhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; r\nidhar ---&gt; a\ndhara ---&gt; t\nharat ---&gt; h\narath ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; r\nshaur ---&gt; a\nhaura ---&gt; b\naurab ---&gt; h\nurabh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; b\n.shib ---&gt; u\nshibu ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; i\n.fari ---&gt; n\nfarin ---&gt; a\narina ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; w\n..law ---&gt; l\n.lawl ---&gt; i\nlawli ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; m\n.naim ---&gt; u\nnaimu ---&gt; d\naimud ---&gt; d\nimudd ---&gt; i\nmuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; p\n.jaip ---&gt; r\njaipr ---&gt; k\naiprk ---&gt; e\niprke ---&gt; s\nprkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; u\nramdu ---&gt; l\namdul ---&gt; a\nmdula ---&gt; r\ndular ---&gt; i\nulari ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; i\nparvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; l\n.kall ---&gt; a\nkalla ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; f\n.atif ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; n\n.jamn ---&gt; a\njamna ---&gt; a\namnaa ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; r\n..pir ---&gt; y\n.piry ---&gt; a\npirya ---&gt; n\niryan ---&gt; k\nryank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; r\nambar ---&gt; o\nmbaro ---&gt; s\nbaros ---&gt; e\narose ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; d\n.swad ---&gt; h\nswadh ---&gt; i\nwadhi ---&gt; n\nadhin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; n\nshadn ---&gt; a\nhadna ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; t\n..fit ---&gt; r\n.fitr ---&gt; a\nfitra ---&gt; t\nitrat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; i\nmanvi ---&gt; r\nanvir ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; a\n..jaa ---&gt; n\n.jaan ---&gt; k\njaank ---&gt; i\naanki ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; l\n.musl ---&gt; i\nmusli ---&gt; m\nuslim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; e\njagde ---&gt; e\nagdee ---&gt; s\ngdees ---&gt; h\ndeesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; i\n.baji ---&gt; n\nbajin ---&gt; d\najind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; w\n.ramw ---&gt; a\nramwa ---&gt; t\namwat ---&gt; a\nmwata ---&gt; r\nwatar ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; d\n..avd ---&gt; h\n.avdh ---&gt; e\navdhe ---&gt; s\nvdhes ---&gt; h\ndhesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; s\n.bhus ---&gt; a\nbhusa ---&gt; n\nhusan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; d\n.band ---&gt; a\nbanda ---&gt; n\nandan ---&gt; i\nndani ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; p\nshisp ---&gt; a\nhispa ---&gt; l\nispal ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; n\n.veen ---&gt; a\nveena ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; a\n.taba ---&gt; s\ntabas ---&gt; s\nabass ---&gt; u\nbassu ---&gt; m\nassum ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; z\n..vaz ---&gt; i\n.vazi ---&gt; d\nvazid ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; e\n.dhre ---&gt; e\ndhree ---&gt; j\nhreej ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; c\n..buc ---&gt; h\n.buch ---&gt; h\nbuchh ---&gt; i\nuchhi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; i\nguddi ---&gt; b\nuddib ---&gt; a\nddiba ---&gt; i\ndibai ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; k\ndevik ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; n\n..pon ---&gt; e\n.pone ---&gt; e\nponee ---&gt; m\noneem ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; k\n..pak ---&gt; a\n.paka ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; n\nruben ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; u\nshanu ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; e\n.jube ---&gt; d\njubed ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; m\n.narm ---&gt; d\nnarmd ---&gt; a\narmda ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; q\n..maq ---&gt; s\n.maqs ---&gt; o\nmaqso ---&gt; o\naqsoo ---&gt; d\nqsood ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; e\n.gane ---&gt; s\nganes ---&gt; h\nanesh ---&gt; i\nneshi ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; u\nbudhu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; i\nmanji ---&gt; t\nanjit ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; i\n..jhi ---&gt; n\n.jhin ---&gt; g\njhing ---&gt; i\nhingi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; o\n..soo ---&gt; r\n.soor ---&gt; a\nsoora ---&gt; j\nooraj ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; m\ntaram ---&gt; a\narama ---&gt; t\nramat ---&gt; i\namati ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; r\n.hajr ---&gt; a\nhajra ---&gt; t\najrat ---&gt; .\n..... ---&gt; d\n....d ---&gt; r\n...dr ---&gt; o\n..dro ---&gt; p\n.drop ---&gt; t\ndropt ---&gt; i\nropti ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; l\n..vel ---&gt; a\n.vela ---&gt; r\nvelar ---&gt; a\nelara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; s\nsukhs ---&gt; o\nukhso ---&gt; h\nkhsoh ---&gt; a\nhsoha ---&gt; n\nsohan ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; e\nmanee ---&gt; s\nanees ---&gt; h\nneesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; i\n.jami ---&gt; l\njamil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; n\nnavin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; a\nmanda ---&gt; n\nandan ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; u\n.nitu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; s\n.tass ---&gt; a\ntassa ---&gt; d\nassad ---&gt; u\nssadu ---&gt; l\nsadul ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; k\n.devk ---&gt; a\ndevka ---&gt; l\nevkal ---&gt; i\nvkali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; n\nsumen ---&gt; t\nument ---&gt; r\nmentr ---&gt; a\nentra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; y\n.shiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; s\n.jais ---&gt; m\njaism ---&gt; e\naisme ---&gt; e\nismee ---&gt; n\nsmeen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; r\n.satr ---&gt; u\nsatru ---&gt; g\natrug ---&gt; h\ntrugh ---&gt; a\nrugha ---&gt; n\nughan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; n\n.jarn ---&gt; a\njarna ---&gt; i\narnai ---&gt; l\nrnail ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; m\nrajam ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; t\n..jot ---&gt; y\n.joty ---&gt; i\njotyi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; r\nmehar ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; a\n..pea ---&gt; r\n.pear ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; a\nshana ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; e\nbrije ---&gt; s\nrijes ---&gt; h\nijesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; g\n...ag ---&gt; y\n..agy ---&gt; a\n.agya ---&gt; p\nagyap ---&gt; a\ngyapa ---&gt; d\nyapad ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; t\n..smt ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; n\naakan ---&gt; k\nakank ---&gt; s\nkanks ---&gt; h\nanksh ---&gt; a\nnksha ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; y\nadity ---&gt; a\nditya ---&gt; l\nityal ---&gt; o\ntyalo ---&gt; k\nyalok ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; n\n..kin ---&gt; y\n.kiny ---&gt; a\nkinya ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; p\nijayp ---&gt; a\njaypa ---&gt; l\naypal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; e\nnanke ---&gt; s\nankes ---&gt; h\nnkesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; p\npushp ---&gt; a\nushpa ---&gt; n\nshpan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; w\n.ashw ---&gt; a\nashwa ---&gt; n\nshwan ---&gt; i\nhwani ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; p\nveerp ---&gt; a\neerpa ---&gt; l\nerpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; h\n.nenh ---&gt; e\nnenhe ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; m\nakshm ---&gt; a\nkshma ---&gt; n\nshman ---&gt; .\n..... ---&gt; s\n....s ---&gt; p\n...sp ---&gt; a\n..spa ---&gt; n\n.span ---&gt; a\nspana ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; y\n..hay ---&gt; a\n.haya ---&gt; t\nhayat ---&gt; u\nayatu ---&gt; l\nyatul ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; v\n..aav ---&gt; e\n.aave ---&gt; s\naaves ---&gt; h\navesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; m\nshaym ---&gt; u\nhaymu ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; l\n..isl ---&gt; a\n.isla ---&gt; m\nislam ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; s\nyashs ---&gt; v\nashsv ---&gt; i\nshsvi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; y\n.samy ---&gt; a\nsamya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; f\n.shif ---&gt; a\nshifa ---&gt; l\nhifal ---&gt; i\nifali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; i\nsunai ---&gt; n\nunain ---&gt; a\nnaina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; a\n.saka ---&gt; l\nsakal ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; i\nnishi ---&gt; t\nishit ---&gt; h\nshith ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; t\nuchit ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; y\n.ruky ---&gt; a\nrukya ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; n\nbiran ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; s\n.vers ---&gt; a\nversa ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; k\n.dhuk ---&gt; h\ndhukh ---&gt; u\nhukhu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; y\n..niy ---&gt; a\n.niya ---&gt; a\nniyaa ---&gt; z\niyaaz ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; i\n.faki ---&gt; h\nfakih ---&gt; a\nakiha ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; u\n..atu ---&gt; l\n.atul ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; k\n..sek ---&gt; h\n.sekh ---&gt; a\nsekha ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; j\n..ujj ---&gt; w\n.ujjw ---&gt; a\nujjwa ---&gt; l\njjwal ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; u\n.vipu ---&gt; l\nvipul ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; j\n..puj ---&gt; a\n.puja ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; t\nmanot ---&gt; i\nanoti ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; i\nbindi ---&gt; y\nindiy ---&gt; a\nndiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; o\n.sado ---&gt; d\nsadod ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; e\n.heee ---&gt; r\nheeer ---&gt; a\neeera ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; b\nsahib ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; e\n.amre ---&gt; e\namree ---&gt; n\nmreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; i\n.sohi ---&gt; b\nsohib ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; s\n.jees ---&gt; h\njeesh ---&gt; a\neesha ---&gt; n\neshan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; e\nnasre ---&gt; e\nasree ---&gt; n\nsreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; m\nhamim ---&gt; a\namima ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; i\n.rati ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; r\n..bhr ---&gt; k\n.bhrk ---&gt; a\nbhrka ---&gt; t\nhrkat ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; j\n.farj ---&gt; a\nfarja ---&gt; n\narjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; j\n.vaij ---&gt; a\nvaija ---&gt; n\naijan ---&gt; t\nijant ---&gt; i\njanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; m\nsamim ---&gt; .\n..... ---&gt; d\n....d ---&gt; w\n...dw ---&gt; a\n..dwa ---&gt; r\n.dwar ---&gt; k\ndwark ---&gt; a\nwarka ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; a\nbhaga ---&gt; t\nhagat ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; o\nbitto ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; i\n.bali ---&gt; r\nbalir ---&gt; a\nalira ---&gt; m\nliram ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; i\nyashi ---&gt; k\nashik ---&gt; a\nshika ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; a\n.hasa ---&gt; n\nhasan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; h\n.fath ---&gt; e\nfathe ---&gt; r\nather ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; a\nharba ---&gt; n\narban ---&gt; s\nrbans ---&gt; h\nbansh ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; a\n..ina ---&gt; y\n.inay ---&gt; a\ninaya ---&gt; t\nnayat ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; b\nharmb ---&gt; i\narmbi ---&gt; r\nrmbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; h\najesh ---&gt; v\njeshv ---&gt; r\neshvr ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; h\n.isth ---&gt; a\nistha ---&gt; r\nsthar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; l\n..bul ---&gt; b\n.bulb ---&gt; u\nbulbu ---&gt; l\nulbul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; r\nshokr ---&gt; a\nhokra ---&gt; j\nokraj ---&gt; i\nkraji ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; j\n.baij ---&gt; n\nbaijn ---&gt; a\naijna ---&gt; t\nijnat ---&gt; h\njnath ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; a\nijaya ---&gt; n\njayan ---&gt; t\nayant ---&gt; a\nyanta ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; a\navina ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; i\nrajbi ---&gt; r\najbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; h\nmansh ---&gt; a\nansha ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; n\n..tun ---&gt; d\n.tund ---&gt; a\ntunda ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; e\n.gule ---&gt; s\ngules ---&gt; t\nulest ---&gt; a\nlesta ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; r\n..war ---&gt; m\n.warm ---&gt; a\nwarma ---&gt; t\narmat ---&gt; i\nrmati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; n\n.nain ---&gt; a\nnaina ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; w\nshanw ---&gt; a\nhanwa ---&gt; j\nanwaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; n\nkaran ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; u\n.anku ---&gt; s\nankus ---&gt; h\nnkush ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; i\n.aasi ---&gt; m\naasim ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; w\n..kuw ---&gt; a\n.kuwa ---&gt; r\nkuwar ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; n\n.busn ---&gt; e\nbusne ---&gt; s\nusnes ---&gt; s\nsness ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; v\n..shv ---&gt; e\n.shve ---&gt; t\nshvet ---&gt; a\nhveta ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; m\n..bim ---&gt; l\n.biml ---&gt; a\nbimla ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; l\n..pul ---&gt; i\n.puli ---&gt; t\npulit ---&gt; a\nulita ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; n\nhawan ---&gt; i\nawani ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; m\n..pem ---&gt; a\n.pema ---&gt; r\npemar ---&gt; a\nemara ---&gt; m\nmaram ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; r\n.azar ---&gt; u\nazaru ---&gt; d\nzarud ---&gt; d\narudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; t\nrajat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; o\n.rajo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; i\n.hazi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; u\n.shku ---&gt; r\nshkur ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; e\n.vine ---&gt; e\nvinee ---&gt; t\nineet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; f\n..guf ---&gt; r\n.gufr ---&gt; a\ngufra ---&gt; a\nufraa ---&gt; n\nfraan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; e\nnasre ---&gt; e\nasree ---&gt; m\nsreem ---&gt; a\nreema ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; b\n.babb ---&gt; u\nbabbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; a\nradha ---&gt; b\nadhab ---&gt; a\ndhaba ---&gt; i\nhabai ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; t\n.birt ---&gt; h\nbirth ---&gt; a\nirtha ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; n\n.virn ---&gt; i\nvirni ---&gt; t\nirnit ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; e\n.jaye ---&gt; s\njayes ---&gt; h\nayesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; r\nsudar ---&gt; s\nudars ---&gt; h\ndarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; n\n.jhun ---&gt; n\njhunn ---&gt; a\nhunna ---&gt; .\n..... ---&gt; d\n....d ---&gt; r\n...dr ---&gt; o\n..dro ---&gt; p\n.drop ---&gt; a\ndropa ---&gt; t\nropat ---&gt; i\nopati ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; a\nrajna ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; n\naamin ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; a\n.gaja ---&gt; l\ngajal ---&gt; a\najala ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; h\nbachh ---&gt; u\nachhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; n\nsalin ---&gt; d\nalind ---&gt; e\nlinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; i\n..azi ---&gt; m\n.azim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; a\nramna ---&gt; t\namnat ---&gt; h\nmnath ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; j\nnderj ---&gt; e\nderje ---&gt; e\nerjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; k\ndeepk ---&gt; a\neepka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; p\nrachp ---&gt; r\nachpr ---&gt; e\nchpre ---&gt; e\nhpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; i\nramvi ---&gt; l\namvil ---&gt; a\nmvila ---&gt; s\nvilas ---&gt; h\nilash ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; e\n.fake ---&gt; r\nfaker ---&gt; e\nakere ---&gt; y\nkerey ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; s\n..pos ---&gt; h\n.posh ---&gt; e\nposhe ---&gt; t\noshet ---&gt; t\nshett ---&gt; y\nhetty ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; l\n.khal ---&gt; i\nkhali ---&gt; d\nhalid ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; s\n.jams ---&gt; h\njamsh ---&gt; e\namshe ---&gt; d\nmshed ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; s\n.kars ---&gt; h\nkarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; k\n.bark ---&gt; a\nbarka ---&gt; t\narkat ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; m\n.ishm ---&gt; i\nishmi ---&gt; t\nshmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; b\n.jasb ---&gt; i\njasbi ---&gt; r\nasbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; t\n.mamt ---&gt; a\nmamta ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; k\n..tak ---&gt; i\n.taki ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; l\nhyaml ---&gt; a\nyamla ---&gt; l\namlal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; t\n.mast ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; i\nshari ---&gt; b\nharib ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; i\n.hami ---&gt; d\nhamid ---&gt; a\namida ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; e\npinke ---&gt; y\ninkey ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; t\n.amit ---&gt; a\namita ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; a\ngaura ---&gt; v\naurav ---&gt; g\nuravg ---&gt; i\nravgi ---&gt; l\navgil ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; s\niyans ---&gt; u\nyansu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; e\nsatye ---&gt; n\natyen ---&gt; d\ntyend ---&gt; e\nyende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; t\n..set ---&gt; h\n.seth ---&gt; i\nsethi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; a\nlaxma ---&gt; n\naxman ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; m\n..amm ---&gt; a\n.amma ---&gt; r\nammar ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; a\narcha ---&gt; n\nrchan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; s\nhahis ---&gt; t\nahist ---&gt; a\nhista ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; n\n.khan ---&gt; g\nkhang ---&gt; a\nhanga ---&gt; r\nangar ---&gt; a\nngara ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; b\n.vaib ---&gt; a\nvaiba ---&gt; v\naibav ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; s\nsheis ---&gt; h\nheish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; a\n.sapa ---&gt; n\nsapan ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; a\ndimpa ---&gt; l\nimpal ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; c\nchanc ---&gt; h\nhanch ---&gt; l\nanchl ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; a\n.basa ---&gt; n\nbasan ---&gt; t\nasant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; h\n.nanh ---&gt; u\nnanhu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; e\n.gaje ---&gt; n\ngajen ---&gt; d\najend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; m\n..ahm ---&gt; a\n.ahma ---&gt; d\nahmad ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; u\n.mayu ---&gt; r\nmayur ---&gt; i\nayuri ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; h\n.mosh ---&gt; r\nmoshr ---&gt; i\noshri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; d\n.azad ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; p\nyashp ---&gt; a\nashpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; s\n.naus ---&gt; a\nnausa ---&gt; d\nausad ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; n\n.shin ---&gt; e\nshine ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; n\n..run ---&gt; i\n.runi ---&gt; y\nruniy ---&gt; a\nuniya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; n\nsakin ---&gt; a\nakina ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; a\nsonia ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; a\n.akaa ---&gt; s\nakaas ---&gt; h\nkaash ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; t\n.bant ---&gt; i\nbanti ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; m\n.jhum ---&gt; a\njhuma ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; i\n.chai ---&gt; n\nchain ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; m\nudham ---&gt; a\ndhama ---&gt; .\n..... ---&gt; u\n....u ---&gt; g\n...ug ---&gt; m\n..ugm ---&gt; a\n.ugma ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; u\nbadru ---&gt; l\nadrul ---&gt; l\ndrull ---&gt; a\nrulla ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; k\n.nikk ---&gt; i\nnikki ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; l\n.gurl ---&gt; a\ngurla ---&gt; l\nurlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; u\nshilu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; a\nshima ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; j\n.durj ---&gt; a\ndurja ---&gt; n\nurjan ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; p\n.kirp ---&gt; a\nkirpa ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; f\n.praf ---&gt; u\nprafu ---&gt; l\nraful ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; a\n.dama ---&gt; n\ndaman ---&gt; j\namanj ---&gt; e\nmanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; p\n...sp ---&gt; n\n..spn ---&gt; a\n.spna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; n\n.jain ---&gt; a\njaina ---&gt; b\nainab ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; l\nsheil ---&gt; e\nheile ---&gt; s\neiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; a\nbhina ---&gt; y\nhinay ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; r\n.ishr ---&gt; a\nishra ---&gt; n\nshran ---&gt; a\nhrana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; o\n.sano ---&gt; j\nsanoj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; t\n.maat ---&gt; w\nmaatw ---&gt; a\naatwa ---&gt; r\natwar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; l\nmangl ---&gt; a\nangla ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; i\n.muli ---&gt; n\nmulin ---&gt; a\nulina ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; e\n.name ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; m\n..anm ---&gt; o\n.anmo ---&gt; l\nanmol ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; r\n.umar ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; e\n.anke ---&gt; e\nankee ---&gt; t\nnkeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; n\n.avin ---&gt; s\navins ---&gt; h\nvinsh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; t\numant ---&gt; h\nmanth ---&gt; r\nanthr ---&gt; a\nnthra ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; g\n.garg ---&gt; i\ngargi ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; l\nhooll ---&gt; a\noolla ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; k\nriyak ---&gt; i\niyaki ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; n\n..den ---&gt; i\n.deni ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; m\n.rehm ---&gt; a\nrehma ---&gt; t\nehmat ---&gt; i\nhmati ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; v\nhyamv ---&gt; a\nyamva ---&gt; t\namvat ---&gt; i\nmvati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; a\nsafia ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; e\n.rave ---&gt; n\nraven ---&gt; a\navena ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; o\n.paro ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; h\nruksh ---&gt; a\nuksha ---&gt; n\nkshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; m\n.aasm ---&gt; a\naasma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; y\nnajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; i\nsanji ---&gt; d\nanjid ---&gt; a\nnjida ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; r\n..ajr ---&gt; a\n.ajra ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; l\n..bil ---&gt; k\n.bilk ---&gt; e\nbilke ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; g\n..gag ---&gt; a\n.gaga ---&gt; n\ngagan ---&gt; d\nagand ---&gt; e\ngande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; n\n.pran ---&gt; n\nprann ---&gt; a\nranna ---&gt; b\nannab ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; b\n.makb ---&gt; u\nmakbu ---&gt; l\nakbul ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; j\n.nirj ---&gt; a\nnirja ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; e\n.chhe ---&gt; d\nchhed ---&gt; i\nhhedi ---&gt; l\nhedil ---&gt; a\nedila ---&gt; l\ndilal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; z\n.hamz ---&gt; a\nhamza ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; i\n.tapi ---&gt; s\ntapis ---&gt; h\napish ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; j\nfiroj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; m\n.kalm ---&gt; a\nkalma ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; n\n..non ---&gt; i\n.noni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; u\n.saku ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; m\nkishm ---&gt; a\nishma ---&gt; t\nshmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; r\nahnar ---&gt; a\nhnara ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; m\n..gum ---&gt; a\n.guma ---&gt; n\nguman ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; u\nguddu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; i\n.sapi ---&gt; t\nsapit ---&gt; a\napita ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; h\n.kosh ---&gt; l\nkoshl ---&gt; y\noshly ---&gt; a\nshlya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; t\n.vipt ---&gt; a\nvipta ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; s\n..tis ---&gt; a\n.tisa ---&gt; r\ntisar ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; w\n..kew ---&gt; a\n.kewa ---&gt; l\nkewal ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; m\n..urm ---&gt; i\n.urmi ---&gt; l\nurmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; a\n.teja ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; d\n..god ---&gt; a\n.goda ---&gt; m\ngodam ---&gt; b\nodamb ---&gt; a\ndamba ---&gt; r\nambar ---&gt; i\nmbari ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; k\n.prak ---&gt; e\nprake ---&gt; s\nrakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; d\n.sidd ---&gt; h\nsiddh ---&gt; a\niddha ---&gt; r\nddhar ---&gt; t\ndhart ---&gt; h\nharth ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; i\nsamai ---&gt; a\namaia ---&gt; l\nmaial ---&gt; i\naiali ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; a\n.naza ---&gt; h\nnazah ---&gt; a\nazaha ---&gt; t\nzahat ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; a\n.vina ---&gt; y\nvinay ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; b\n.sahb ---&gt; a\nsahba ---&gt; j\nahbaj ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; h\n..muh ---&gt; m\n.muhm ---&gt; a\nmuhma ---&gt; d\nuhmad ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; s\nsahis ---&gt; h\nahish ---&gt; t\nhisht ---&gt; a\nishta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; s\nrahis ---&gt; h\nahish ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; l\n.dhal ---&gt; c\ndhalc ---&gt; h\nhalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; a\njasva ---&gt; n\nasvan ---&gt; t\nsvant ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; l\n..all ---&gt; a\n.alla ---&gt; u\nallau ---&gt; d\nllaud ---&gt; d\nlaudd ---&gt; i\nauddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; a\n.bena ---&gt; y\nbenay ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; a\nrisha ---&gt; b\nishab ---&gt; h\nshabh ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; k\n.umak ---&gt; a\numaka ---&gt; n\nmakan ---&gt; t\nakant ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; n\n.moon ---&gt; a\nmoona ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; c\n..pac ---&gt; h\n.pach ---&gt; u\npachu ---&gt; r\nachur ---&gt; a\nchura ---&gt; m\nhuram ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; l\n.vipl ---&gt; o\nviplo ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; h\nushbh ---&gt; u\nshbhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; u\nshisu ---&gt; p\nhisup ---&gt; a\nisupa ---&gt; l\nsupal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; m\nrahim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; p\n.rabp ---&gt; r\nrabpr ---&gt; e\nabpre ---&gt; e\nbpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; v\nhramv ---&gt; e\nramve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; v\n.rizv ---&gt; a\nrizva ---&gt; n\nizvan ---&gt; a\nzvana ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; j\nparmj ---&gt; e\narmje ---&gt; e\nrmjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; t\n..sut ---&gt; i\n.suti ---&gt; y\nsutiy ---&gt; a\nutiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; b\n.samb ---&gt; h\nsambh ---&gt; u\nambhu ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; a\nbhina ---&gt; s\nhinas ---&gt; h\ninash ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; h\nfarah ---&gt; e\narahe ---&gt; e\nrahee ---&gt; n\naheen ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; a\n.roha ---&gt; n\nrohan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; w\n.rijw ---&gt; a\nrijwa ---&gt; n\nijwan ---&gt; a\njwana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; j\n.banj ---&gt; u\nbanju ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; e\nshake ---&gt; e\nhakee ---&gt; n\nakeen ---&gt; a\nkeena ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; v\n..yuv ---&gt; e\n.yuve ---&gt; r\nyuver ---&gt; a\nuvera ---&gt; j\nveraj ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; s\n.nens ---&gt; i\nnensi ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; f\n..asf ---&gt; a\n.asfa ---&gt; q\nasfaq ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; v\nbhagv ---&gt; a\nhagva ---&gt; t\nagvat ---&gt; i\ngvati ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; n\nrihan ---&gt; a\nihana ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; s\n.dars ---&gt; h\ndarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; f\nashif ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; a\nsavia ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; z\n..roz ---&gt; i\n.rozi ---&gt; n\nrozin ---&gt; a\nozina ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; e\n.muje ---&gt; f\nmujef ---&gt; i\nujefi ---&gt; r\njefir ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; k\nishak ---&gt; h\nshakh ---&gt; a\nhakha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; l\n.rajl ---&gt; a\nrajla ---&gt; x\najlax ---&gt; m\njlaxm ---&gt; i\nlaxmi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; p\n.binp ---&gt; a\nbinpa ---&gt; l\ninpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; e\n.sahe ---&gt; b\nsaheb ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; r\n..dor ---&gt; d\n.dord ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; e\n.hite ---&gt; n\nhiten ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; t\n.murt ---&gt; i\nmurti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; b\nshaub ---&gt; h\nhaubh ---&gt; a\naubha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; s\n.bads ---&gt; h\nbadsh ---&gt; y\nadshy ---&gt; a\ndshya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; l\nsakil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; e\n.naye ---&gt; e\nnayee ---&gt; m\nayeem ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; s\n.mohs ---&gt; i\nmohsi ---&gt; n\nohsin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; i\n.suni ---&gt; t\nsunit ---&gt; a\nunita ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; g\nsundg ---&gt; a\nundga ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; i\n.mini ---&gt; t\nminit ---&gt; w\ninitw ---&gt; a\nnitwa ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; e\ndhure ---&gt; n\nhuren ---&gt; d\nurend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; r\n..kur ---&gt; b\n.kurb ---&gt; a\nkurba ---&gt; n\nurban ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; b\n.shub ---&gt; h\nshubh ---&gt; a\nhubha ---&gt; m\nubham ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; b\n..tub ---&gt; a\n.tuba ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; b\n.sarb ---&gt; a\nsarba ---&gt; s\narbas ---&gt; h\nrbash ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; i\n..phi ---&gt; r\n.phir ---&gt; d\nphird ---&gt; o\nhirdo ---&gt; s\nirdos ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; r\n..nor ---&gt; e\n.nore ---&gt; e\nnoree ---&gt; n\noreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; m\n.shym ---&gt; o\nshymo ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; d\n.joyd ---&gt; e\njoyde ---&gt; e\noydee ---&gt; p\nydeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; l\nshell ---&gt; y\nhelly ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; a\n.pura ---&gt; n\npuran ---&gt; m\nuranm ---&gt; a\nranma ---&gt; l\nanmal ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; k\nratik ---&gt; s\natiks ---&gt; h\ntiksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; s\n.maks ---&gt; u\nmaksu ---&gt; d\naksud ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; c\n.kanc ---&gt; h\nkanch ---&gt; n\nanchn ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; d\nangad ---&gt; h\nngadh ---&gt; a\ngadha ---&gt; r\nadhar ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; l\n.babl ---&gt; u\nbablu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; b\n.jaib ---&gt; i\njaibi ---&gt; r\naibir ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; r\nrendr ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; g\n.parg ---&gt; a\nparga ---&gt; t\nargat ---&gt; i\nrgati ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; a\namara ---&gt; p\nmarap ---&gt; a\narapa ---&gt; l\nrapal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; w\n.kalw ---&gt; a\nkalwa ---&gt; n\nalwan ---&gt; t\nlwant ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; m\n..jum ---&gt; r\n.jumr ---&gt; a\njumra ---&gt; t\numrat ---&gt; i\nmrati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; w\n..sow ---&gt; a\n.sowa ---&gt; r\nsowar ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; l\neepal ---&gt; i\nepali ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; k\n.rakk ---&gt; i\nrakki ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; a\n.jisa ---&gt; n\njisan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; r\nshahr ---&gt; u\nhahru ---&gt; f\nahruf ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; w\n..raw ---&gt; a\n.rawa ---&gt; n\nrawan ---&gt; a\nawana ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; s\nghans ---&gt; h\nhansh ---&gt; y\nanshy ---&gt; a\nnshya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; u\ngeetu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; u\nkashu ---&gt; m\nashum ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; i\nshali ---&gt; n\nhalin ---&gt; i\nalini ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; u\n..fau ---&gt; i\n.faui ---&gt; n\nfauin ---&gt; a\nauina ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; l\nabhil ---&gt; a\nbhila ---&gt; s\nhilas ---&gt; h\nilash ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; i\n.vari ---&gt; s\nvaris ---&gt; h\narish ---&gt; a\nrisha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; h\nprash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; a\nshana ---&gt; t\nhanat ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; a\n..ava ---&gt; n\n.avan ---&gt; t\navant ---&gt; i\nvanti ---&gt; k\nantik ---&gt; a\nntika ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; m\n.dham ---&gt; i\ndhami ---&gt; n\nhamin ---&gt; i\namini ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; r\n..ibr ---&gt; a\n.ibra ---&gt; h\nibrah ---&gt; i\nbrahi ---&gt; m\nrahim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; s\n.jals ---&gt; i\njalsi ---&gt; n\nalsin ---&gt; g\nlsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; n\nhandn ---&gt; i\nandni ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; e\n.jule ---&gt; k\njulek ---&gt; h\nulekh ---&gt; a\nlekha ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; i\n.gari ---&gt; m\ngarim ---&gt; a\narima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; l\n.sall ---&gt; y\nsally ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; s\n..tus ---&gt; h\n.tush ---&gt; a\ntusha ---&gt; l\nushal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; a\n.pata ---&gt; n\npatan ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; j\n.gunj ---&gt; a\ngunja ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; e\n.dave ---&gt; n\ndaven ---&gt; d\navend ---&gt; e\nvende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; h\n..prh ---&gt; a\n.prha ---&gt; l\nprhal ---&gt; a\nrhala ---&gt; d\nhalad ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; i\nneeli ---&gt; m\neelim ---&gt; a\nelima ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; b\n.bhab ---&gt; h\nbhabh ---&gt; a\nhabha ---&gt; v\nabhav ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; n\nfaran ---&gt; a\narana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; e\n.rake ---&gt; e\nrakee ---&gt; b\nakeeb ---&gt; a\nkeeba ---&gt; .\n..... ---&gt; y\n....y ---&gt; e\n...ye ---&gt; s\n..yes ---&gt; p\n.yesp ---&gt; a\nyespa ---&gt; l\nespal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; m\nnshum ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; a\n.inda ---&gt; r\nindar ---&gt; a\nndara ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; a\nrhana ---&gt; a\nhanaa ---&gt; z\nanaaz ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; d\nnaved ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; y\n..any ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; r\n.sitr ---&gt; a\nsitra ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; a\n.baga ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; m\nrishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; w\n..baw ---&gt; a\n.bawa ---&gt; n\nbawan ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; m\n..zam ---&gt; i\n.zami ---&gt; l\nzamil ---&gt; e\namile ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; n\n..yun ---&gt; u\n.yunu ---&gt; s\nyunus ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; b\nhasib ---&gt; u\nasibu ---&gt; l\nsibul ---&gt; .\n..... ---&gt; u\n....u ---&gt; s\n...us ---&gt; m\n..usm ---&gt; a\n.usma ---&gt; a\nusmaa ---&gt; n\nsmaan ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; e\nharie ---&gt; s\naries ---&gt; h\nriesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; a\namala ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; i\nruchi ---&gt; k\nuchik ---&gt; a\nchika ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; m\n.sarm ---&gt; i\nsarmi ---&gt; l\narmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; l\n.kull ---&gt; i\nkulli ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; j\n.kunj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; b\nharib ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; j\namarj ---&gt; e\nmarje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; r\n.umar ---&gt; d\numard ---&gt; a\nmarda ---&gt; r\nardar ---&gt; a\nrdara ---&gt; z\ndaraz ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; t\n.sunt ---&gt; i\nsunti ---&gt; a\nuntia ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; h\nhursh ---&gt; i\nurshi ---&gt; d\nrshid ---&gt; a\nshida ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; y\n.faiy ---&gt; a\nfaiya ---&gt; a\naiyaa ---&gt; z\niyaaz ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; t\n..cht ---&gt; a\n.chta ---&gt; r\nchtar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; b\nraghb ---&gt; i\naghbi ---&gt; r\nghbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; d\n.rand ---&gt; h\nrandh ---&gt; e\nandhe ---&gt; e\nndhee ---&gt; r\ndheer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; j\n.gulj ---&gt; a\ngulja ---&gt; r\nuljar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; i\nmjani ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; a\n.nana ---&gt; k\nnanak ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; j\n.harj ---&gt; e\nharje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; q\n..toq ---&gt; u\n.toqu ---&gt; i\ntoqui ---&gt; r\noquir ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; a\n..ata ---&gt; n\n.atan ---&gt; u\natanu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; n\nsumen ---&gt; d\numend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; e\n.suje ---&gt; e\nsujee ---&gt; t\nujeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; m\nrupam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; e\n.sake ---&gt; t\nsaket ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; k\nshrik ---&gt; a\nhrika ---&gt; n\nrikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; y\n.divy ---&gt; a\ndivya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; m\nnazim ---&gt; a\nazima ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; u\nnathu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; a\nharva ---&gt; n\narvan ---&gt; s\nrvans ---&gt; h\nvansh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; j\nhivaj ---&gt; i\nivaji ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; v\n..juv ---&gt; e\n.juve ---&gt; l\njuvel ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; h\n..adh ---&gt; i\n.adhi ---&gt; s\nadhis ---&gt; h\ndhish ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; f\n.akif ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; s\n.kirs ---&gt; h\nkirsh ---&gt; n\nirshn ---&gt; a\nrshna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; g\n.asag ---&gt; a\nasaga ---&gt; r\nsagar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; g\n.jahg ---&gt; e\njahge ---&gt; e\nahgee ---&gt; r\nhgeer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; a\n.jaya ---&gt; d\njayad ---&gt; a\nayada ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; r\nsushr ---&gt; e\nushre ---&gt; e\nshree ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; y\n..ray ---&gt; i\n.rayi ---&gt; s\nrayis ---&gt; h\nayish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; r\nsumir ---&gt; t\numirt ---&gt; a\nmirta ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; n\n.monn ---&gt; y\nmonny ---&gt; .\n..... ---&gt; s\n....s ---&gt; p\n...sp ---&gt; a\n..spa ---&gt; r\n.spar ---&gt; s\nspars ---&gt; h\nparsh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; n\n..fan ---&gt; n\n.fann ---&gt; i\nfanni ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; n\nbaban ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; d\nmohad ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; y\n..siy ---&gt; a\n.siya ---&gt; .\n..... ---&gt; y\n....y ---&gt; e\n...ye ---&gt; s\n..yes ---&gt; h\n.yesh ---&gt; p\nyeshp ---&gt; a\neshpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; n\n.binn ---&gt; u\nbinnu ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; r\neshar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; u\nmangu ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; i\n.gudi ---&gt; a\ngudia ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; e\n.bire ---&gt; n\nbiren ---&gt; d\nirend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; w\n..jaw ---&gt; h\n.jawh ---&gt; a\njawha ---&gt; r\nawhar ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; a\n.sora ---&gt; b\nsorab ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; b\n.babb ---&gt; y\nbabby ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; u\n..pau ---&gt; l\n.paul ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; i\nguddi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; n\n.pavn ---&gt; e\npavne ---&gt; s\navnes ---&gt; h\nvnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; l\n.sabl ---&gt; u\nsablu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; f\n.shef ---&gt; a\nshefa ---&gt; l\nhefal ---&gt; i\nefali ---&gt; .\n..... ---&gt; c\n....c ---&gt; o\n...co ---&gt; s\n..cos ---&gt; m\n.cosm ---&gt; i\ncosmi ---&gt; c\nosmic ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; s\nshams ---&gt; h\nhamsh ---&gt; e\namshe ---&gt; r\nmsher ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; u\n.monu ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; t\n..set ---&gt; i\n.seti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; h\nshobh ---&gt; i\nhobhi ---&gt; t\nobhit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; i\nsalmi ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; l\n..rol ---&gt; l\n.roll ---&gt; y\nrolly ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; i\n.udai ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; l\nsakil ---&gt; a\nakila ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; t\n.arat ---&gt; i\narati ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; u\n.karu ---&gt; n\nkarun ---&gt; a\naruna ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; a\n.kesa ---&gt; r\nkesar ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; g\n.jhag ---&gt; d\njhagd ---&gt; o\nhagdo ---&gt; o\nagdoo ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; e\nmanee ---&gt; t\naneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; u\nrishu ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; n\nulnan ---&gt; j\nlnanj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; a\nramsa ---&gt; t\namsat ---&gt; i\nmsati ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; r\n..tur ---&gt; v\n.turv ---&gt; e\nturve ---&gt; n\nurven ---&gt; y\nrveny ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; w\n.jasw ---&gt; a\njaswa ---&gt; n\naswan ---&gt; t\nswant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; e\n.nade ---&gt; e\nnadee ---&gt; m\nadeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; d\nhahid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; d\n..mod ---&gt; h\n.modh ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; d\n..fid ---&gt; a\n.fida ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; n\nkiran ---&gt; t\nirant ---&gt; i\nranti ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; x\nminax ---&gt; n\ninaxn ---&gt; i\nnaxni ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; n\n..arn ---&gt; a\n.arna ---&gt; b\narnab ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; o\n..doo ---&gt; d\n.dood ---&gt; h\ndoodh ---&gt; n\noodhn ---&gt; a\nodhna ---&gt; t\ndhnat ---&gt; h\nhnath ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; l\n.adil ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; u\nveeru ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; o\n..leo ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; e\nrajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; s\n..shs ---&gt; i\n.shsi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; d\n.ched ---&gt; d\nchedd ---&gt; i\nheddi ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; b\n.khub ---&gt; h\nkhubh ---&gt; u\nhubhu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; n\n.vinn ---&gt; a\nvinna ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; s\n..khs ---&gt; h\n.khsh ---&gt; b\nkhshb ---&gt; u\nhshbu ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; a\n.juba ---&gt; i\njubai ---&gt; r\nubair ---&gt; a\nbaira ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; s\n.nurs ---&gt; a\nnursa ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; i\n.dali ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; e\ndurge ---&gt; s\nurges ---&gt; h\nrgesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; k\n.shuk ---&gt; v\nshukv ---&gt; e\nhukve ---&gt; e\nukvee ---&gt; r\nkveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; n\n.somn ---&gt; a\nsomna ---&gt; t\nomnat ---&gt; h\nmnath ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; d\n.rand ---&gt; e\nrande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; k\n.teek ---&gt; a\nteeka ---&gt; m\neekam ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; m\n.naim ---&gt; a\nnaima ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; i\n.faki ---&gt; r\nfakir ---&gt; i\nakiri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; a\n.saja ---&gt; n\nsajan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; u\n.aasu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; m\naashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; p\nmritp ---&gt; a\nritpa ---&gt; l\nitpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; s\n.bhis ---&gt; a\nbhisa ---&gt; n\nhisan ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; r\nishar ---&gt; a\nshara ---&gt; r\nharar ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; q\n.atiq ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; s\nurgas ---&gt; h\nrgash ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; i\n.zahi ---&gt; r\nzahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; t\n.shet ---&gt; a\nsheta ---&gt; l\nhetal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; e\n.sale ---&gt; s\nsales ---&gt; h\nalesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; n\n.leen ---&gt; a\nleena ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; e\n..sne ---&gt; h\n.sneh ---&gt; l\nsnehl ---&gt; a\nnehla ---&gt; t\nehlat ---&gt; a\nhlata ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; y\n.prey ---&gt; a\npreya ---&gt; n\nreyan ---&gt; k\neyank ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; i\n.loki ---&gt; n\nlokin ---&gt; d\nokind ---&gt; r\nkindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; r\njeetr ---&gt; a\neetra ---&gt; m\netram ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; n\nharen ---&gt; d\narend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; e\n.sude ---&gt; r\nsuder ---&gt; s\nuders ---&gt; h\ndersh ---&gt; a\nersha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; k\n..alk ---&gt; a\n.alka ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; t\n.mamt ---&gt; h\nmamth ---&gt; a\namtha ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; m\n..rom ---&gt; a\n.roma ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; n\ndevan ---&gt; a\nevana ---&gt; n\nvanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; v\n..lov ---&gt; l\n.lovl ---&gt; y\nlovly ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; u\nsantu ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; m\n.nimm ---&gt; y\nnimmy ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; z\nehnaz ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; e\n.vase ---&gt; e\nvasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; n\ndevin ---&gt; d\nevind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; h\n.narh ---&gt; a\nnarha ---&gt; s\narhas ---&gt; i\nrhasi ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; i\n.firi ---&gt; d\nfirid ---&gt; a\nirida ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; u\n..mau ---&gt; j\n.mauj ---&gt; i\nmauji ---&gt; m\naujim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; v\nmandv ---&gt; i\nandvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; s\n.khas ---&gt; t\nkhast ---&gt; i\nhasti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; r\nmohar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; i\n.hani ---&gt; f\nhanif ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; a\n.mena ---&gt; k\nmenak ---&gt; s\nenaks ---&gt; h\nnaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; v\n.dilv ---&gt; a\ndilva ---&gt; r\nilvar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; i\n.musi ---&gt; b\nmusib ---&gt; a\nusiba ---&gt; t\nsibat ---&gt; .\n..... ---&gt; s\n....s ---&gt; k\n...sk ---&gt; u\n..sku ---&gt; n\n.skun ---&gt; a\nskuna ---&gt; t\nkunat ---&gt; l\nunatl ---&gt; a\nnatla ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; u\n.batu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; t\nmangt ---&gt; a\nangta ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; k\n..lek ---&gt; h\n.lekh ---&gt; r\nlekhr ---&gt; a\nekhra ---&gt; j\nkhraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; r\n.samr ---&gt; a\nsamra ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; a\n.dara ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; e\nharme ---&gt; e\narmee ---&gt; t\nrmeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; v\nnjeev ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; t\nsumit ---&gt; a\numita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; a\n.sina ---&gt; a\nsinaa ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; i\ngeeti ---&gt; k\neetik ---&gt; a\netika ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; a\nandra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; j\nmanoj ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; h\n..fah ---&gt; i\n.fahi ---&gt; r\nfahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; a\n.soba ---&gt; n\nsoban ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; i\n.kabi ---&gt; d\nkabid ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; a\n.saba ---&gt; n\nsaban ---&gt; a\nabana ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; s\n.aras ---&gt; l\narasl ---&gt; a\nrasla ---&gt; n\naslan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; n\n.nain ---&gt; s\nnains ---&gt; h\nainsh ---&gt; i\ninshi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; b\n.arab ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; m\n.anam ---&gt; o\nanamo ---&gt; l\nnamol ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; o\nbudho ---&gt; .\n..... ---&gt; e\n....e ---&gt; l\n...el ---&gt; w\n..elw ---&gt; i\n.elwi ---&gt; n\nelwin ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; h\nhursh ---&gt; i\nurshi ---&gt; d\nrshid ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; f\n.julf ---&gt; i\njulfi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; s\ntabbs ---&gt; u\nabbsu ---&gt; m\nbbsum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; s\n.nais ---&gt; i\nnaisi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; m\nsahim ---&gt; a\nahima ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; i\narshi ---&gt; t\nrshit ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; z\n.marz ---&gt; i\nmarzi ---&gt; n\narzin ---&gt; a\nrzina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; r\nsahar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; s\n..ahs ---&gt; a\n.ahsa ---&gt; m\nahsam ---&gt; i\nhsami ---&gt; n\nsamin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; a\nambha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; j\n.sajj ---&gt; u\nsajju ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; h\n.mush ---&gt; r\nmushr ---&gt; r\nushrr ---&gt; a\nshrra ---&gt; f\nhrraf ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; j\nabhij ---&gt; i\nbhiji ---&gt; t\nhijit ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; l\n.jull ---&gt; y\njully ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; u\n..kou ---&gt; c\n.kouc ---&gt; h\nkouch ---&gt; u\nouchu ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; r\n..hur ---&gt; j\n.hurj ---&gt; i\nhurji ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; t\n..dat ---&gt; a\n.data ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; m\nnajim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; s\nmahes ---&gt; h\nahesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; o\n..jyo ---&gt; t\n.jyot ---&gt; y\njyoty ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; o\nmanjo ---&gt; o\nanjoo ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; i\nsanji ---&gt; t\nanjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; u\n.shru ---&gt; t\nshrut ---&gt; i\nhruti ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; a\n..kra ---&gt; s\n.kras ---&gt; h\nkrash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; a\nshana ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; s\n.kirs ---&gt; h\nkirsh ---&gt; a\nirsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; m\n.rajm ---&gt; a\nrajma ---&gt; n\najman ---&gt; i\njmani ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; n\n.peen ---&gt; u\npeenu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; a\nmuska ---&gt; a\nuskaa ---&gt; n\nskaan ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; a\n..tra ---&gt; u\n.trau ---&gt; n\ntraun ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; l\n..gil ---&gt; a\n.gila ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; m\nparem ---&gt; .\n..... ---&gt; u\n....u ---&gt; n\n...un ---&gt; n\n..unn ---&gt; a\n.unna ---&gt; t\nunnat ---&gt; i\nnnati ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; a\n.anna ---&gt; t\nannat ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; e\nmunne ---&gt; m\nunnem ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; a\nharma ---&gt; n\narman ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; a\netana ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; i\narshi ---&gt; t\nrshit ---&gt; a\nshita ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; y\nparty ---&gt; u\nartyu ---&gt; s\nrtyus ---&gt; h\ntyush ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; t\n..utt ---&gt; a\n.utta ---&gt; m\nuttam ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; i\nvashi ---&gt; s\nashis ---&gt; a\nshisa ---&gt; s\nhisas ---&gt; t\nisast ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; y\n.saiy ---&gt; a\nsaiya ---&gt; d\naiyad ---&gt; a\niyada ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; g\n.bang ---&gt; a\nbanga ---&gt; l\nangal ---&gt; i\nngali ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; a\n.shoa ---&gt; n\nshoan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; r\njanar ---&gt; d\nanard ---&gt; a\nnarda ---&gt; m\nardam ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; j\n..laj ---&gt; j\n.lajj ---&gt; a\nlajja ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; u\nshalu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; u\n.saru ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; a\nashia ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; a\n.bina ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; h\najesh ---&gt; w\njeshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; e\nlakhe ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; a\n.juna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; u\nfaizu ---&gt; d\naizud ---&gt; e\nizude ---&gt; e\nzudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; u\n.ramu ---&gt; l\nramul ---&gt; u\namulu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; j\n..arj ---&gt; i\n.arji ---&gt; n\narjin ---&gt; a\nrjina ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; a\nshaba ---&gt; n\nhaban ---&gt; a\nabana ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; i\n.dipi ---&gt; k\ndipik ---&gt; a\nipika ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; n\n.saan ---&gt; a\nsaana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; a\nradha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; i\nbhuri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; u\npratu ---&gt; l\nratul ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; t\n.kirt ---&gt; i\nkirti ---&gt; m\nirtim ---&gt; a\nrtima ---&gt; n\ntiman ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; a\n.mita ---&gt; l\nmital ---&gt; i\nitali ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; n\naamin ---&gt; a\namina ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; s\n.taus ---&gt; e\ntause ---&gt; e\nausee ---&gt; k\nuseek ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; r\nsundr ---&gt; i\nundri ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; u\n.taru ---&gt; n\ntarun ---&gt; a\naruna ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; h\nramdh ---&gt; a\namdha ---&gt; n\nmdhan ---&gt; i\ndhani ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; i\n.isti ---&gt; k\nistik ---&gt; a\nstika ---&gt; r\ntikar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; t\nrohit ---&gt; a\nohita ---&gt; s\nhitas ---&gt; h\nitash ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; u\nvansu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; s\nmanas ---&gt; h\nanash ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; i\n..omi ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; w\nurgaw ---&gt; a\nrgawa ---&gt; t\ngawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; i\n.lavi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; e\ndeepe ---&gt; n\neepen ---&gt; d\nepend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; f\nshaif ---&gt; a\nhaifa ---&gt; l\naifal ---&gt; i\nifali ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; a\nitara ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; w\n..tiw ---&gt; n\n.tiwn ---&gt; k\ntiwnk ---&gt; a\niwnka ---&gt; l\nwnkal ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; h\n.tash ---&gt; n\ntashn ---&gt; i\nashni ---&gt; m\nshnim ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; m\n.simm ---&gt; i\nsimmi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; j\n.harj ---&gt; i\nharji ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; y\nrubey ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; d\n.bhad ---&gt; u\nbhadu ---&gt; r\nhadur ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; j\n.faij ---&gt; a\nfaija ---&gt; l\naijal ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; a\ntabba ---&gt; s\nabbas ---&gt; u\nbbasu ---&gt; m\nbasum ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; m\n..jim ---&gt; m\n.jimm ---&gt; i\njimmi ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; o\n..jyo ---&gt; t\n.jyot ---&gt; i\njyoti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; y\nrajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; a\nantra ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; s\n.tohs ---&gt; e\ntohse ---&gt; e\nohsee ---&gt; e\nhseee ---&gt; m\nseeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; r\nidhar ---&gt; t\ndhart ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; y\n..gay ---&gt; t\n.gayt ---&gt; r\ngaytr ---&gt; e\naytre ---&gt; e\nytree ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; t\n.bart ---&gt; i\nbarti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; d\n.rajd ---&gt; u\nrajdu ---&gt; t\najdut ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; k\n..alk ---&gt; e\n.alke ---&gt; s\nalkes ---&gt; h\nlkesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; j\n.ajaj ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; j\n..inj ---&gt; m\n.injm ---&gt; a\ninjma ---&gt; m\nnjmam ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; u\nnathu ---&gt; l\nathul ---&gt; a\nthula ---&gt; l\nhulal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; b\n.gurb ---&gt; a\ngurba ---&gt; c\nurbac ---&gt; h\nrbach ---&gt; a\nbacha ---&gt; n\nachan ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; a\n.prya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; m\nsharm ---&gt; i\nharmi ---&gt; l\narmil ---&gt; i\nrmili ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; v\n.samv ---&gt; e\nsamve ---&gt; d\namved ---&gt; n\nmvedn ---&gt; a\nvedna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; p\nsukhp ---&gt; a\nukhpa ---&gt; l\nkhpal ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; n\n..jin ---&gt; c\n.jinc ---&gt; y\njincy ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; t\nparit ---&gt; i\nariti ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; h\n..fah ---&gt; i\n.fahi ---&gt; m\nfahim ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; o\nsubho ---&gt; d\nubhod ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; d\n.prad ---&gt; e\nprade ---&gt; p\nradep ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; n\nsumin ---&gt; d\numind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; o\nkisho ---&gt; r\nishor ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; a\nprita ---&gt; m\nritam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; v\n.satv ---&gt; i\nsatvi ---&gt; r\natvir ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; v\n.jaiv ---&gt; e\njaive ---&gt; e\naivee ---&gt; r\niveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; a\n.saka ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; a\nshaya ---&gt; n\nhayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; u\nkamru ---&gt; d\namrud ---&gt; d\nmrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; a\n.muja ---&gt; f\nmujaf ---&gt; f\nujaff ---&gt; a\njaffa ---&gt; r\naffar ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; o\n..dho ---&gt; d\n.dhod ---&gt; i\ndhodi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; w\n..haw ---&gt; a\n.hawa ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; t\nompat ---&gt; i\nmpati ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; a\noorja ---&gt; h\norjah ---&gt; a\nrjaha ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; m\n.hasm ---&gt; i\nhasmi ---&gt; t\nasmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; w\nramsw ---&gt; r\namswr ---&gt; o\nmswro ---&gt; o\nswroo ---&gt; p\nwroop ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; i\n.maji ---&gt; d\nmajid ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; u\nraisu ---&gt; d\naisud ---&gt; d\nisudd ---&gt; i\nsuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; d\nsudhd ---&gt; e\nudhde ---&gt; v\ndhdev ---&gt; .\n..... ---&gt; s\n....s ---&gt; y\n...sy ---&gt; h\n..syh ---&gt; a\n.syha ---&gt; m\nsyham ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; e\n..ane ---&gt; k\n.anek ---&gt; h\nanekh ---&gt; a\nnekha ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; r\n.swar ---&gt; n\nswarn ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; k\n.bhik ---&gt; h\nbhikh ---&gt; a\nhikha ---&gt; r\nikhar ---&gt; i\nkhari ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; o\nyasho ---&gt; d\nashod ---&gt; a\nshoda ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; a\n.rima ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; y\nashiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; d\nubhad ---&gt; r\nbhadr ---&gt; a\nhadra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; h\n.sadh ---&gt; u\nsadhu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; a\nnazia ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; o\n.jayo ---&gt; t\njayot ---&gt; i\nayoti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; r\n.anar ---&gt; o\nanaro ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; m\narshm ---&gt; e\nrshme ---&gt; e\nshmee ---&gt; t\nhmeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; b\n.satb ---&gt; a\nsatba ---&gt; r\natbar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; a\nrabha ---&gt; t\nabhat ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; s\nvires ---&gt; h\niresh ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; i\n.dani ---&gt; e\ndanie ---&gt; l\naniel ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; m\n.faim ---&gt; u\nfaimu ---&gt; d\naimud ---&gt; d\nimudd ---&gt; i\nmuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; e\nramse ---&gt; w\namsew ---&gt; a\nmsewa ---&gt; k\nsewak ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; a\n..faa ---&gt; i\n.faai ---&gt; j\nfaaij ---&gt; a\naaija ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; t\nsumit ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; b\n.narb ---&gt; d\nnarbd ---&gt; a\narbda ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; l\n.lavl ---&gt; i\nlavli ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; u\namiru ---&gt; l\nmirul ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; t\n.meht ---&gt; a\nmehta ---&gt; b\nehtab ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; n\n.amin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; n\nshain ---&gt; a\nhaina ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; i\n.vini ---&gt; t\nvinit ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; s\n..sis ---&gt; t\n.sist ---&gt; y\nsisty ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; a\nanupa ---&gt; m\nnupam ---&gt; a\nupama ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; a\n.kaja ---&gt; r\nkajar ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; n\n..sen ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; t\nparvt ---&gt; i\narvti ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; i\nashmi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; j\nahnaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; c\n.kanc ---&gt; h\nkanch ---&gt; a\nancha ---&gt; n\nnchan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; h\n.rath ---&gt; a\nratha ---&gt; n\nathan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; s\n..rus ---&gt; h\n.rush ---&gt; a\nrusha ---&gt; l\nushal ---&gt; i\nshali ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; n\nnoorn ---&gt; a\noorna ---&gt; b\nornab ---&gt; i\nrnabi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; h\n.chah ---&gt; a\nchaha ---&gt; t\nhahat ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; b\nhahib ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; a\narsha ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; l\n..kil ---&gt; l\n.kill ---&gt; o\nkillo ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; w\n..diw ---&gt; a\n.diwa ---&gt; k\ndiwak ---&gt; a\niwaka ---&gt; r\nwakar ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; m\n..kim ---&gt; m\n.kimm ---&gt; i\nkimmi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; h\n..nih ---&gt; a\n.niha ---&gt; l\nnihal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; n\nanjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; t\namart ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; i\n.afri ---&gt; n\nafrin ---&gt; a\nfrina ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; a\n..uta ---&gt; m\n.utam ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; e\n.lave ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; u\nparsu ---&gt; r\narsur ---&gt; a\nrsura ---&gt; m\nsuram ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; y\n.isty ---&gt; a\nistya ---&gt; r\nstyar ---&gt; a\ntyara ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; e\n.sude ---&gt; s\nsudes ---&gt; h\nudesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; g\n.prag ---&gt; a\npraga ---&gt; t\nragat ---&gt; i\nagati ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; i\n.nosi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; g\n..hag ---&gt; a\n.haga ---&gt; m\nhagam ---&gt; i\nagami ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; u\n..rau ---&gt; n\n.raun ---&gt; a\nrauna ---&gt; f\naunaf ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; d\nharid ---&gt; u\naridu ---&gt; t\nridut ---&gt; t\nidutt ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; t\n..gyt ---&gt; r\n.gytr ---&gt; i\ngytri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; e\nmange ---&gt; l\nangel ---&gt; a\nngela ---&gt; l\ngelal ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; w\nbudhw ---&gt; a\nudhwa ---&gt; n\ndhwan ---&gt; t\nhwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; a\n.java ---&gt; d\njavad ---&gt; e\navade ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; a\n..daa ---&gt; u\n.daau ---&gt; d\ndaaud ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; k\n..nek ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; j\npremj ---&gt; i\nremji ---&gt; t\nemjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; i\nsumai ---&gt; l\numail ---&gt; a\nmaila ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; k\n..akk ---&gt; n\n.akkn ---&gt; i\nakkni ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; a\n.juga ---&gt; n\njugan ---&gt; i\nugani ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; r\ndhanr ---&gt; a\nhanra ---&gt; j\nanraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; i\nmusti ---&gt; k\nustik ---&gt; e\nstike ---&gt; e\ntikee ---&gt; m\nikeem ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; a\nkamla ---&gt; j\namlaj ---&gt; e\nmlaje ---&gt; e\nlajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; i\n.afri ---&gt; d\nafrid ---&gt; i\nfridi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; s\ninaks ---&gt; h\nnaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; t\nsavit ---&gt; i\naviti ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; t\n.jant ---&gt; a\njanta ---&gt; r\nantar ---&gt; p\nntarp ---&gt; a\ntarpa ---&gt; l\narpal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; i\numari ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; y\nramdy ---&gt; a\namdya ---&gt; l\nmdyal ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; n\nchetn ---&gt; a\nhetna ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; i\n.nabi ---&gt; l\nnabil ---&gt; a\nabila ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; n\nsheen ---&gt; s\nheens ---&gt; a\neensa ---&gt; r\nensar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; t\n.swat ---&gt; i\nswati ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; t\ndeept ---&gt; i\neepti ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; n\n..hon ---&gt; g\n.hong ---&gt; s\nhongs ---&gt; i\nongsi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; t\nagwat ---&gt; i\ngwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; b\n..aab ---&gt; i\n.aabi ---&gt; d\naabid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; i\n.sani ---&gt; a\nsania ---&gt; .\n..... ---&gt; v\n....v ---&gt; r\n...vr ---&gt; e\n..vre ---&gt; n\n.vren ---&gt; d\nvrend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; e\nrishe ---&gt; n\nishen ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; h\nvandh ---&gt; n\nandhn ---&gt; a\nndhna ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; n\nramen ---&gt; d\namend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; h\nsandh ---&gt; a\nandha ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; s\nhares ---&gt; h\naresh ---&gt; w\nreshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; e\n.amee ---&gt; r\nameer ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; r\nbalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; b\nahnab ---&gt; a\nhnaba ---&gt; j\nnabaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; a\n.amba ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; k\n.shik ---&gt; h\nshikh ---&gt; a\nhikha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; m\n.bhem ---&gt; j\nbhemj ---&gt; i\nhemji ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; y\n.mary ---&gt; a\nmarya ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; r\nashar ---&gt; a\nshara ---&gt; m\nharam ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; u\nranju ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; s\n.nars ---&gt; i\nnarsi ---&gt; n\narsin ---&gt; g\nrsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; e\n.amee ---&gt; n\nameen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; n\nsunen ---&gt; a\nunena ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; m\n..nem ---&gt; s\n.nems ---&gt; i\nnemsi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; v\n..arv ---&gt; i\n.arvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; s\nriyas ---&gt; a\niyasa ---&gt; t\nyasat ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; r\n..iqr ---&gt; a\n.iqra ---&gt; r\niqrar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; i\n.aasi ---&gt; f\naasif ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; s\n.gens ---&gt; i\ngensi ---&gt; n\nensin ---&gt; g\nnsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; n\n..zen ---&gt; a\n.zena ---&gt; b\nzenab ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; e\narvee ---&gt; n\nrveen ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; k\n.nikk ---&gt; h\nnikkh ---&gt; a\nikkha ---&gt; r\nkkhar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; r\nnasir ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; e\n.kale ---&gt; e\nkalee ---&gt; m\naleem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; n\nhivan ---&gt; i\nivani ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; a\nmanga ---&gt; t\nangat ---&gt; .\n..... ---&gt; f\n....f ---&gt; r\n...fr ---&gt; j\n..frj ---&gt; a\n.frja ---&gt; n\nfrjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; n\neghan ---&gt; a\nghana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; i\n.rami ---&gt; n\nramin ---&gt; d\namind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; i\n.vipi ---&gt; n\nvipin ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; r\n.aamr ---&gt; i\naamri ---&gt; n\namrin ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; l\n..bul ---&gt; e\n.bule ---&gt; t\nbulet ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; u\n.ishu ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; i\n.hiri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; f\n.mahf ---&gt; o\nmahfo ---&gt; o\nahfoo ---&gt; j\nhfooj ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; c\n.renc ---&gt; h\nrench ---&gt; o\nencho ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; u\npinku ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; t\n..hot ---&gt; i\n.hoti ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; p\ndhanp ---&gt; a\nhanpa ---&gt; t\nanpat ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; i\n.jasi ---&gt; r\njasir ---&gt; a\nasira ---&gt; m\nsiram ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; j\n..afj ---&gt; a\n.afja ---&gt; l\nafjal ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; d\n..abd ---&gt; u\n.abdu ---&gt; l\nabdul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; e\nradhe ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; a\nrenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; r\n..bhr ---&gt; a\n.bhra ---&gt; m\nbhram ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; e\n.nawe ---&gt; d\nnawed ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; y\npinky ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; n\nrishn ---&gt; a\nishna ---&gt; l\nshnal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; k\nshank ---&gt; r\nhankr ---&gt; i\nankri ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; w\nbhinw ---&gt; a\nhinwa ---&gt; v\ninwav ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; s\n.naus ---&gt; h\nnaush ---&gt; a\nausha ---&gt; d\nushad ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; n\nrajan ---&gt; i\najani ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; r\n.neer ---&gt; a\nneera ---&gt; j\neeraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; a\n.maya ---&gt; k\nmayak ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; m\nusham ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; u\n.vasu ---&gt; k\nvasuk ---&gt; i\nasuki ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; h\n.tash ---&gt; k\ntashk ---&gt; k\nashkk ---&gt; u\nshkku ---&gt; r\nhkkur ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; n\nrupen ---&gt; d\nupend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; k\niyank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; s\n..rus ---&gt; a\n.rusa ---&gt; n\nrusan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; t\n.mart ---&gt; i\nmarti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; t\n..met ---&gt; i\n.meti ---&gt; l\nmetil ---&gt; y\netily ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; s\n..ais ---&gt; h\n.aish ---&gt; e\naishe ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; i\n.dani ---&gt; s\ndanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; n\nkaran ---&gt; d\narand ---&gt; e\nrande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; u\n..rau ---&gt; d\n.raud ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; i\n.ishi ---&gt; k\nishik ---&gt; a\nshika ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; a\nruksa ---&gt; n\nuksan ---&gt; a\nksana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; a\nkanha ---&gt; i\nanhai ---&gt; y\nnhaiy ---&gt; a\nhaiya ---&gt; .\n..... ---&gt; i\n....i ---&gt; e\n...ie ---&gt; m\n..iem ---&gt; a\n.iema ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; i\n.hani ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; t\nchott ---&gt; u\nhottu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; y\n.mony ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; a\n.shea ---&gt; k\nsheak ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; i\nnandi ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; d\namard ---&gt; e\nmarde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; k\n..nek ---&gt; i\n.neki ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; n\nurgan ---&gt; a\nrgana ---&gt; n\nganan ---&gt; d\nanand ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; u\ndimpu ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; u\n.ansu ---&gt; l\nansul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; s\nshaks ---&gt; h\nhaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; m\nmaham ---&gt; m\nahamm ---&gt; d\nhammd ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; b\n.khub ---&gt; i\nkhubi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; t\n.bhat ---&gt; r\nbhatr ---&gt; i\nhatri ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; n\n.pann ---&gt; u\npannu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; h\n.nanh ---&gt; e\nnanhe ---&gt; y\nanhey ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; t\n..git ---&gt; i\n.giti ---&gt; k\ngitik ---&gt; a\nitika ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; i\njagdi ---&gt; e\nagdie ---&gt; s\ngdies ---&gt; h\ndiesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; i\npriti ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; m\nveerm ---&gt; a\neerma ---&gt; t\nermat ---&gt; i\nrmati ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; y\n.kaly ---&gt; a\nkalya ---&gt; n\nalyan ---&gt; i\nlyani ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; i\n.bali ---&gt; y\nbaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; i\n.tari ---&gt; f\ntarif ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; v\n..tav ---&gt; i\n.tavi ---&gt; n\ntavin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; k\n..luk ---&gt; m\n.lukm ---&gt; a\nlukma ---&gt; n\nukman ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; b\n..sib ---&gt; a\n.siba ---&gt; n\nsiban ---&gt; a\nibana ---&gt; z\nbanaz ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; b\n..omb ---&gt; e\n.ombe ---&gt; e\nombee ---&gt; r\nmbeer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; d\nsarad ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; e\n..ome ---&gt; n\n.omen ---&gt; d\nomend ---&gt; r\nmendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; e\n.save ---&gt; e\nsavee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; u\nmunnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; l\nsabil ---&gt; a\nabila ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; r\n.umar ---&gt; d\numard ---&gt; e\nmarde ---&gt; e\nardee ---&gt; n\nrdeen ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; s\n.nens ---&gt; h\nnensh ---&gt; i\nenshi ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; d\n..rud ---&gt; r\n.rudr ---&gt; a\nrudra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; i\nmanji ---&gt; v\nanjiv ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; t\nbhart ---&gt; e\nharte ---&gt; n\narten ---&gt; d\nrtend ---&gt; u\ntendu ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; u\n..gou ---&gt; r\n.gour ---&gt; i\ngouri ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; l\nsonal ---&gt; i\nonali ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; o\n..soo ---&gt; n\n.soon ---&gt; a\nsoona ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; a\n.rada ---&gt; t\nradat ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; t\n.beet ---&gt; i\nbeeti ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; e\n.khee ---&gt; m\nkheem ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; y\ntaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; t\n.bait ---&gt; a\nbaita ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; w\n..urw ---&gt; a\n.urwa ---&gt; s\nurwas ---&gt; h\nrwash ---&gt; i\nwashi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; j\n.prij ---&gt; u\npriju ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; b\ngulab ---&gt; s\nulabs ---&gt; h\nlabsh ---&gt; a\nabsha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; u\n..bau ---&gt; d\n.baud ---&gt; i\nbaudi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; i\nsudhi ---&gt; r\nudhir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; n\nanjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; j\npramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; a\nkamla ---&gt; s\namlas ---&gt; h\nmlash ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; t\n..alt ---&gt; m\n.altm ---&gt; a\naltma ---&gt; s\nltmas ---&gt; h\ntmash ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; u\n.rahu ---&gt; l\nrahul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; n\n.rahn ---&gt; u\nrahnu ---&gt; m\nahnum ---&gt; a\nhnuma ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; i\nradhi ---&gt; k\nadhik ---&gt; a\ndhika ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; v\nnderv ---&gt; e\nderve ---&gt; s\nerves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; s\nmahas ---&gt; i\nahasi ---&gt; n\nhasin ---&gt; g\nasing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; y\n..tay ---&gt; a\n.taya ---&gt; b\ntayab ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; s\n.maks ---&gt; o\nmakso ---&gt; o\naksoo ---&gt; d\nksood ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; u\n.renu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; e\nkaile ---&gt; s\nailes ---&gt; h\nilesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; a\n.sida ---&gt; r\nsidar ---&gt; t\nidart ---&gt; h\ndarth ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; u\nmeghu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; i\nharvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; a\n.nisa ---&gt; t\nnisat ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; o\n..too ---&gt; l\n.tool ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; c\n..mac ---&gt; h\n.mach ---&gt; h\nmachh ---&gt; a\nachha ---&gt; l\nchhal ---&gt; a\nhhala ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; e\nshale ---&gt; s\nhales ---&gt; h\nalesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; w\n..suw ---&gt; a\n.suwa ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; h\nfarah ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; i\n.yogi ---&gt; t\nyogit ---&gt; a\nogita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; k\nchink ---&gt; i\nhinki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; u\nramku ---&gt; m\namkum ---&gt; a\nmkuma ---&gt; r\nkumar ---&gt; i\numari ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; j\n..nij ---&gt; a\n.nija ---&gt; m\nnijam ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; h\n.sith ---&gt; a\nsitha ---&gt; l\nithal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; c\nbhagc ---&gt; h\nhagch ---&gt; a\nagcha ---&gt; n\ngchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; h\n.vedh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; r\nrajar ---&gt; a\najara ---&gt; m\njaram ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; v\n.jeev ---&gt; a\njeeva ---&gt; n\neevan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; n\n..shn ---&gt; e\n.shne ---&gt; h\nshneh ---&gt; a\nhneha ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; a\n.dama ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; t\n..imt ---&gt; i\n.imti ---&gt; y\nimtiy ---&gt; a\nmtiya ---&gt; z\ntiyaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; i\n.sohi ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; d\nyashd ---&gt; h\nashdh ---&gt; r\nshdhr ---&gt; a\nhdhra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; s\nantos ---&gt; h\nntosh ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; j\n.braj ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; w\n.rizw ---&gt; a\nrizwa ---&gt; n\nizwan ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; a\n.fora ---&gt; n\nforan ---&gt; t\norant ---&gt; i\nranti ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; n\nchann ---&gt; u\nhannu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; v\n.palv ---&gt; i\npalvi ---&gt; n\nalvin ---&gt; d\nlvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; e\n.dipe ---&gt; e\ndipee ---&gt; k\nipeek ---&gt; a\npeeka ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; e\nchame ---&gt; l\nhamel ---&gt; i\nameli ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; y\n.sony ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; t\n.bint ---&gt; u\nbintu ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; e\ndhane ---&gt; s\nhanes ---&gt; w\nanesw ---&gt; a\nneswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; n\nrohin ---&gt; i\nohini ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; u\nandru ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; a\ngulsa ---&gt; n\nulsan ---&gt; a\nlsana ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; e\npuspe ---&gt; n\nuspen ---&gt; d\nspend ---&gt; a\npenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; o\nsanjo ---&gt; h\nanjoh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; o\n..dho ---&gt; l\n.dhol ---&gt; i\ndholi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; h\n.nikh ---&gt; a\nnikha ---&gt; d\nikhad ---&gt; .\n..... ---&gt; a\n....a ---&gt; g\n...ag ---&gt; r\n..agr ---&gt; e\n.agre ---&gt; j\nagrej ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; i\nmanji ---&gt; n\nanjin ---&gt; d\nnjind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; m\n..jom ---&gt; i\n.jomi ---&gt; y\njomiy ---&gt; a\nomiya ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; h\n..keh ---&gt; k\n.kehk ---&gt; a\nkehka ---&gt; s\nehkas ---&gt; h\nhkash ---&gt; a\nkasha ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; t\n..got ---&gt; i\n.goti ---&gt; y\ngotiy ---&gt; a\notiya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; a\nnazma ---&gt; a\nazmaa ---&gt; .\n..... ---&gt; e\n....e ---&gt; t\n...et ---&gt; a\n..eta ---&gt; h\n.etah ---&gt; s\netahs ---&gt; a\ntahsa ---&gt; a\nahsaa ---&gt; m\nhsaam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; i\naroji ---&gt; n\nrojin ---&gt; i\nojini ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; u\n..mou ---&gt; s\n.mous ---&gt; a\nmousa ---&gt; m\nousam ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; u\n..dou ---&gt; l\n.doul ---&gt; a\ndoula ---&gt; t\noulat ---&gt; .\n..... ---&gt; e\n....e ---&gt; m\n...em ---&gt; r\n..emr ---&gt; a\n.emra ---&gt; n\nemran ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; m\n.ramm ---&gt; u\nrammu ---&gt; r\nammur ---&gt; t\nmmurt ---&gt; i\nmurti ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; m\n..rem ---&gt; a\n.rema ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; r\n.sour ---&gt; a\nsoura ---&gt; b\nourab ---&gt; h\nurabh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; m\n.farm ---&gt; e\nfarme ---&gt; e\narmee ---&gt; n\nrmeen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; a\nhaila ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; p\n.trip ---&gt; u\ntripu ---&gt; r\nripur ---&gt; a\nipura ---&gt; r\npurar ---&gt; i\nurari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; n\nhakun ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; h\npusph ---&gt; a\nuspha ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; l\n.mall ---&gt; o\nmallo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; i\n.hami ---&gt; m\nhamim ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; y\nashiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; a\n..jaa ---&gt; n\n.jaan ---&gt; u\njaanu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; h\n..aah ---&gt; i\n.aahi ---&gt; m\naahim ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; f\ngulaf ---&gt; s\nulafs ---&gt; a\nlafsa ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; b\nyashb ---&gt; i\nashbi ---&gt; r\nshbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; y\nmariy ---&gt; a\nariya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; s\n.taus ---&gt; h\ntaush ---&gt; i\naushi ---&gt; f\nushif ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; h\najesh ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; t\n.bunt ---&gt; i\nbunti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; p\n.menp ---&gt; a\nmenpa ---&gt; l\nenpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; t\n..net ---&gt; r\n.netr ---&gt; a\nnetra ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; u\n.varu ---&gt; n\nvarun ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; a\n.nita ---&gt; s\nnitas ---&gt; h\nitash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; e\n.mune ---&gt; s\nmunes ---&gt; h\nunesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; r\n..mer ---&gt; o\n.mero ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; s\n..jus ---&gt; t\n.just ---&gt; i\njusti ---&gt; n\nustin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; i\nramvi ---&gt; h\namvih ---&gt; a\nmviha ---&gt; r\nvihar ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; r\n.sour ---&gt; a\nsoura ---&gt; v\nourav ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; h\n.rekh ---&gt; a\nrekha ---&gt; i\nekhai ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; d\n.gurd ---&gt; e\ngurde ---&gt; e\nurdee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; i\nbhagi ---&gt; r\nhagir ---&gt; a\nagira ---&gt; t\ngirat ---&gt; h\nirath ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; n\n..gon ---&gt; s\n.gons ---&gt; a\ngonsa ---&gt; n\nonsan ---&gt; .\n..... ---&gt; n\n....n ---&gt; j\n...nj ---&gt; a\n..nja ---&gt; r\n.njar ---&gt; e\nnjare ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; n\nhushn ---&gt; u\nushnu ---&gt; d\nshnud ---&gt; i\nhnudi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; d\nsahid ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; i\n.jasi ---&gt; m\njasim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; t\n.natt ---&gt; h\nnatth ---&gt; u\natthu ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; j\n.alij ---&gt; a\nalija ---&gt; n\nlijan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; f\n..suf ---&gt; i\n.sufi ---&gt; a\nsufia ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; f\n.mahf ---&gt; o\nmahfo ---&gt; o\nahfoo ---&gt; z\nhfooz ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; t\n..ant ---&gt; i\n.anti ---&gt; m\nantim ---&gt; a\nntima ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; l\nsujal ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; w\n.ishw ---&gt; e\nishwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; r\nsamir ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; a\nresha ---&gt; m\nesham ---&gt; i\nshami ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; d\n.jaid ---&gt; u\njaidu ---&gt; l\naidul ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; a\nchana ---&gt; b\nhanab ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; t\nvijet ---&gt; a\nijeta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; h\n.makh ---&gt; a\nmakha ---&gt; n\nakhan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; h\n.mash ---&gt; e\nmashe ---&gt; e\nashee ---&gt; n\nsheen ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; s\nmanos ---&gt; h\nanosh ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; p\n.trip ---&gt; t\ntript ---&gt; i\nripti ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; b\nurgab ---&gt; a\nrgaba ---&gt; i\ngabai ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; i\nyashi ---&gt; n\nashin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; e\nmanve ---&gt; r\nanver ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; k\nashik ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; r\n.anar ---&gt; k\nanark ---&gt; a\nnarka ---&gt; l\narkal ---&gt; i\nrkali ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; a\n.anka ---&gt; j\nankaj ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; e\nbhave ---&gt; s\nhaves ---&gt; h\navesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; n\nshabn ---&gt; a\nhabna ---&gt; m\nabnam ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; f\nashif ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; e\n.shie ---&gt; n\nshien ---&gt; a\nhiena ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; a\n.reha ---&gt; a\nrehaa ---&gt; n\nehaan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; u\n.vipu ---&gt; n\nvipun ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; n\n.najn ---&gt; i\nnajni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; n\nsabin ---&gt; a\nabina ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; a\n.jaha ---&gt; n\njahan ---&gt; i\nahani ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; n\n..ton ---&gt; i\n.toni ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; g\nchang ---&gt; o\nhango ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; o\n.sano ---&gt; s\nsanos ---&gt; h\nanosh ---&gt; i\nnoshi ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; l\n.junl ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; s\nshams ---&gt; h\nhamsh ---&gt; i\namshi ---&gt; n\nmshin ---&gt; a\nshina ---&gt; .\n..... ---&gt; e\n....e ---&gt; l\n...el ---&gt; i\n..eli ---&gt; y\n.eliy ---&gt; a\neliya ---&gt; s\nliyas ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; i\n.suri ---&gt; n\nsurin ---&gt; d\nurind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; c\n..anc ---&gt; h\n.anch ---&gt; a\nancha ---&gt; l\nnchal ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; k\n..yuk ---&gt; i\n.yuki ---&gt; l\nyukil ---&gt; a\nukila ---&gt; l\nkilal ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; m\n..gom ---&gt; t\n.gomt ---&gt; i\ngomti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; r\n.mehr ---&gt; u\nmehru ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; t\nsanat ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; a\nshaba ---&gt; n\nhaban ---&gt; m\nabanm ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; b\n.parb ---&gt; h\nparbh ---&gt; a\narbha ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; e\n.pate ---&gt; n\npaten ---&gt; d\natend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; t\n.jait ---&gt; u\njaitu ---&gt; n\naitun ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; p\nsushp ---&gt; a\nushpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; r\n.balr ---&gt; a\nbalra ---&gt; j\nalraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; w\n..jew ---&gt; a\n.jewa ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; t\n.akht ---&gt; a\nakhta ---&gt; r\nkhtar ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; s\n.mohs ---&gt; i\nmohsi ---&gt; n\nohsin ---&gt; a\nhsina ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; c\n.malc ---&gt; h\nmalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; a\nndera ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; m\n.tamm ---&gt; a\ntamma ---&gt; n\namman ---&gt; n\nmmann ---&gt; e\nmanne ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; a\n.afsa ---&gt; n\nafsan ---&gt; a\nfsana ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; e\nbinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; s\n.atis ---&gt; h\natish ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; l\nshaul ---&gt; a\nhaula ---&gt; l\naulal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; n\nkalpn ---&gt; a\nalpna ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; j\nshahj ---&gt; a\nhahja ---&gt; d\nahjad ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; y\nakshy ---&gt; a\nkshya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; i\nreeti ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; s\n.vers ---&gt; h\nversh ---&gt; a\nersha ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; r\nangar ---&gt; a\nngara ---&gt; m\ngaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; o\n.kiro ---&gt; d\nkirod ---&gt; i\nirodi ---&gt; m\nrodim ---&gt; a\nodima ---&gt; l\ndimal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; a\naroja ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; a\nlakha ---&gt; n\nakhan ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; s\n..ays ---&gt; h\n.aysh ---&gt; a\naysha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; v\n.tasv ---&gt; e\ntasve ---&gt; e\nasvee ---&gt; r\nsveer ---&gt; a\nveera ---&gt; n\neeran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; a\nsazia ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; u\n.golu ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; s\n.anus ---&gt; o\nanuso ---&gt; y\nnusoy ---&gt; a\nusoya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; h\n.jash ---&gt; i\njashi ---&gt; n\nashin ---&gt; a\nshina ---&gt; l\nhinal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; a\nrenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; n\nidhan ---&gt; t\ndhant ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; n\nhushn ---&gt; u\nushnu ---&gt; m\nshnum ---&gt; a\nhnuma ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; a\nitara ---&gt; m\ntaram ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; t\n.murt ---&gt; i\nmurti ---&gt; b\nurtib ---&gt; a\nrtiba ---&gt; i\ntibai ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; r\n.aafr ---&gt; e\naafre ---&gt; e\nafree ---&gt; n\nfreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; w\n.tajw ---&gt; a\ntajwa ---&gt; r\najwar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; i\nnajri ---&gt; n\najrin ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; a\nbudha ---&gt; n\nudhan ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; a\nnirma ---&gt; l\nirmal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; a\nrajka ---&gt; l\najkal ---&gt; i\njkali ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; b\n..beb ---&gt; i\n.bebi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; f\n.arif ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; p\n.bhop ---&gt; a\nbhopa ---&gt; l\nhopal ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; b\n.vaib ---&gt; a\nvaiba ---&gt; h\naibah ---&gt; v\nibahv ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; u\n.dhru ---&gt; v\ndhruv ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; t\n.isht ---&gt; k\nishtk ---&gt; a\nshtka ---&gt; r\nhtkar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; u\npartu ---&gt; l\nartul ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; i\n.madi ---&gt; n\nmadin ---&gt; a\nadina ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; i\nbadri ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; v\n.talv ---&gt; e\ntalve ---&gt; n\nalven ---&gt; d\nlvend ---&gt; e\nvende ---&gt; r\nender ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; l\n..gal ---&gt; u\n.galu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; o\n..kho ---&gt; k\n.khok ---&gt; a\nkhoka ---&gt; n\nhokan ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; n\n..hen ---&gt; a\n.hena ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; r\n.anar ---&gt; a\nanara ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; a\n.vipa ---&gt; n\nvipan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; s\n.naus ---&gt; h\nnaush ---&gt; i\naushi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; l\n.lall ---&gt; u\nlallu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; c\nmoolc ---&gt; h\noolch ---&gt; u\nolchu ---&gt; l\nlchul ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; l\nshanl ---&gt; a\nhanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; t\n.kast ---&gt; u\nkastu ---&gt; r\nastur ---&gt; i\nsturi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; s\n.mohs ---&gt; e\nmohse ---&gt; e\nohsee ---&gt; n\nhseen ---&gt; a\nseena ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; a\nnoora ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; y\nadity ---&gt; a\nditya ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; w\n..biw ---&gt; a\n.biwa ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; r\nsushr ---&gt; i\nushri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; m\nrashm ---&gt; i\nashmi ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; e\n.loke ---&gt; s\nlokes ---&gt; h\nokesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; a\n.shra ---&gt; v\nshrav ---&gt; a\nhrava ---&gt; n\nravan ---&gt; i\navani ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; i\nfarhi ---&gt; n\narhin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; a\n..sma ---&gt; r\n.smar ---&gt; i\nsmari ---&gt; t\nmarit ---&gt; i\nariti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; a\nrajba ---&gt; l\najbal ---&gt; a\njbala ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; w\nyashw ---&gt; a\nashwa ---&gt; n\nshwan ---&gt; t\nhwant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; d\n.sadd ---&gt; h\nsaddh ---&gt; a\naddha ---&gt; m\nddham ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; a\n..uja ---&gt; m\n.ujam ---&gt; m\nujamm ---&gt; a\njamma ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; r\n.gyar ---&gt; s\ngyars ---&gt; i\nyarsi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; j\n.parj ---&gt; i\nparji ---&gt; n\narjin ---&gt; d\nrjind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; a\nrabha ---&gt; k\nabhak ---&gt; a\nbhaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; h\n..juh ---&gt; i\n.juhi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; u\n.kasu ---&gt; m\nkasum ---&gt; b\nasumb ---&gt; i\nsumbi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; i\nsurji ---&gt; t\nurjit ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; t\n.aaft ---&gt; a\naafta ---&gt; b\naftab ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; m\n..gam ---&gt; b\n.gamb ---&gt; h\ngambh ---&gt; i\nambhi ---&gt; r\nmbhir ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; k\nmonik ---&gt; a\nonika ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; j\n.subj ---&gt; e\nsubje ---&gt; e\nubjee ---&gt; t\nbjeet ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; e\n.yame ---&gt; e\nyamee ---&gt; n\nameen ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; u\n..abu ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; l\n..bul ---&gt; a\n.bula ---&gt; d\nbulad ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; p\n..rep ---&gt; u\n.repu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; a\npalla ---&gt; w\nallaw ---&gt; i\nllawi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; u\n.masu ---&gt; m\nmasum ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; n\n..zin ---&gt; a\n.zina ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; n\nagwan ---&gt; a\ngwana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; n\nhamin ---&gt; a\namina ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; n\nrishn ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; t\nrohit ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; e\nruche ---&gt; n\nuchen ---&gt; d\nchend ---&gt; r\nhendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; y\n..lay ---&gt; m\n.laym ---&gt; a\nlayma ---&gt; n\nayman ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; q\nashiq ---&gt; u\nshiqu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; a\nsajia ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; a\nshima ---&gt; l\nhimal ---&gt; a\nimala ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; t\nuchit ---&gt; a\nchita ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; e\n..zee ---&gt; n\n.zeen ---&gt; a\nzeena ---&gt; t\neenat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; r\n.patr ---&gt; i\npatri ---&gt; c\natric ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; r\n..akr ---&gt; a\n.akra ---&gt; m\nakram ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; m\n.reem ---&gt; a\nreema ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; r\n.seer ---&gt; i\nseeri ---&gt; y\neeriy ---&gt; a\neriya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; y\nnaziy ---&gt; a\naziya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; k\n.vick ---&gt; k\nvickk ---&gt; y\nickky ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; m\n.seem ---&gt; a\nseema ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; m\n.saym ---&gt; a\nsayma ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; o\n.jaho ---&gt; o\njahoo ---&gt; r\nahoor ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; n\n.rann ---&gt; i\nranni ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; l\naamil ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; e\nitare ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; u\nambhu ---&gt; l\nmbhul ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; b\n.sorb ---&gt; h\nsorbh ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; e\n.pune ---&gt; e\npunee ---&gt; t\nuneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; w\n.sarw ---&gt; e\nsarwe ---&gt; s\narwes ---&gt; h\nrwesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; a\n.tapa ---&gt; s\ntapas ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; r\n.chur ---&gt; a\nchura ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; d\nsubhd ---&gt; r\nubhdr ---&gt; a\nbhdra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; t\nparat ---&gt; i\narati ---&gt; b\nratib ---&gt; h\natibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; a\n.taba ---&gt; s\ntabas ---&gt; u\nabasu ---&gt; m\nbasum ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; p\nnderp ---&gt; a\nderpa ---&gt; a\nerpaa ---&gt; l\nrpaal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; n\n.bahn ---&gt; u\nbahnu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; a\nbisha ---&gt; n\nishan ---&gt; a\nshana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; d\njamad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; w\n.parw ---&gt; a\nparwa ---&gt; t\narwat ---&gt; i\nrwati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; h\n.sabh ---&gt; y\nsabhy ---&gt; a\nabhya ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; e\nfarhe ---&gt; e\narhee ---&gt; n\nrheen ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; m\n.rimm ---&gt; i\nrimmi ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; h\n.mish ---&gt; r\nmishr ---&gt; i\nishri ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; s\nabhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; z\n.shaz ---&gt; i\nshazi ---&gt; d\nhazid ---&gt; a\nazida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; q\nsadiq ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; m\n..lam ---&gt; a\n.lama ---&gt; n\nlaman ---&gt; i\namani ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; e\n..sne ---&gt; h\n.sneh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; r\nrambr ---&gt; i\nambri ---&gt; j\nmbrij ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; c\n..jac ---&gt; o\n.jaco ---&gt; b\njacob ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; p\n.rimp ---&gt; y\nrimpy ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; s\nubhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; w\n.deew ---&gt; a\ndeewa ---&gt; n\neewan ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; n\n.tarn ---&gt; n\ntarnn ---&gt; u\narnnu ---&gt; m\nrnnum ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; m\nratim ---&gt; a\natima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; i\nsangi ---&gt; t\nangit ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; t\n..kat ---&gt; w\n.katw ---&gt; a\nkatwa ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; k\n..mok ---&gt; i\n.moki ---&gt; d\nmokid ---&gt; a\nokida ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; n\n.kamn ---&gt; a\nkamna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; n\nsamin ---&gt; a\namina ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; w\nmadhw ---&gt; i\nadhwi ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; m\nabhim ---&gt; a\nbhima ---&gt; n\nhiman ---&gt; y\nimany ---&gt; u\nmanyu ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; b\naramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; d\nsabid ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; j\n.veej ---&gt; i\nveeji ---&gt; t\neejit ---&gt; a\nejita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; b\n.sahb ---&gt; u\nsahbu ---&gt; d\nahbud ---&gt; d\nhbudd ---&gt; i\nbuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; i\n.laxi ---&gt; t\nlaxit ---&gt; a\naxita ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; i\n.yogi ---&gt; n\nyogin ---&gt; d\nogind ---&gt; e\nginde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; n\n...mn ---&gt; y\n..mny ---&gt; a\n.mnya ---&gt; k\nmnyak ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; a\n.lada ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; u\nbhanu ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; w\n.kulw ---&gt; i\nkulwi ---&gt; n\nulwin ---&gt; d\nlwind ---&gt; e\nwinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; i\nranji ---&gt; t\nanjit ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; t\n.mont ---&gt; y\nmonty ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; m\nhasim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; n\nrabin ---&gt; a\nabina ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; a\nnoora ---&gt; l\nooral ---&gt; h\noralh ---&gt; a\nralha ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; q\n..jaq ---&gt; u\n.jaqu ---&gt; i\njaqui ---&gt; r\naquir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; a\nnatha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; e\n.bhee ---&gt; m\nbheem ---&gt; a\nheema ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; o\n.vino ---&gt; d\nvinod ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; b\nshanb ---&gt; h\nhanbh ---&gt; u\nanbhu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; q\n..muq ---&gt; h\n.muqh ---&gt; t\nmuqht ---&gt; a\nuqhta ---&gt; r\nqhtar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; i\n.aani ---&gt; l\naanil ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; k\n.shuk ---&gt; l\nshukl ---&gt; a\nhukla ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; m\n..hum ---&gt; a\n.huma ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; y\n..liy ---&gt; a\n.liya ---&gt; k\nliyak ---&gt; a\niyaka ---&gt; t\nyakat ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; t\n..ast ---&gt; h\n.asth ---&gt; a\nastha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; a\n.tama ---&gt; n\ntaman ---&gt; n\namann ---&gt; a\nmanna ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; f\n..tof ---&gt; i\n.tofi ---&gt; k\ntofik ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; i\n.kasi ---&gt; d\nkasid ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; a\npusha ---&gt; n\nushan ---&gt; k\nshank ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; w\nhoolw ---&gt; a\noolwa ---&gt; t\nolwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; b\n.trib ---&gt; a\ntriba ---&gt; v\nribav ---&gt; a\nibava ---&gt; n\nbavan ---&gt; .\n..... ---&gt; m\n....m ---&gt; n\n...mn ---&gt; i\n..mni ---&gt; s\n.mnis ---&gt; h\nmnish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; i\nfarhi ---&gt; b\narhib ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; k\nmanak ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; w\nehnaw ---&gt; a\nhnawa ---&gt; j\nnawaj ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; v\nvishv ---&gt; a\nishva ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; r\n.sahr ---&gt; i\nsahri ---&gt; k\nahrik ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; h\nprash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; t\nshant ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; r\nsidhr ---&gt; a\nidhra ---&gt; t\ndhrat ---&gt; h\nhrath ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; a\n.anna ---&gt; v\nannav ---&gt; i\nnnavi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; p\n..bip ---&gt; n\n.bipn ---&gt; e\nbipne ---&gt; s\nipnes ---&gt; h\npnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; a\n.saja ---&gt; n\nsajan ---&gt; a\najana ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; g\n.chog ---&gt; a\nchoga ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; i\n.papi ---&gt; y\npapiy ---&gt; a\napiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; n\n.sumn ---&gt; e\nsumne ---&gt; s\numnes ---&gt; h\nmnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; r\nshivr ---&gt; a\nhivra ---&gt; m\nivram ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; t\n.aart ---&gt; i\naarti ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; k\n..huk ---&gt; u\n.huku ---&gt; m\nhukum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; h\n.nash ---&gt; r\nnashr ---&gt; i\nashri ---&gt; n\nshrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; a\n.sala ---&gt; m\nsalam ---&gt; a\nalama ---&gt; n\nlaman ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; h\n.sadh ---&gt; a\nsadha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; k\n....k ---&gt; m\n...km ---&gt; o\n..kmo ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; p\nchamp ---&gt; a\nhampa ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; k\n..huk ---&gt; m\n.hukm ---&gt; a\nhukma ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; a\nprema ---&gt; t\nremat ---&gt; u\nematu ---&gt; r\nmatur ---&gt; e\nature ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; n\n..jin ---&gt; a\n.jina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; u\n.saru ---&gt; f\nsaruf ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; k\nbhark ---&gt; a\nharka ---&gt; h\narkah ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; h\n.mash ---&gt; l\nmashl ---&gt; i\nashli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; v\n..tav ---&gt; r\n.tavr ---&gt; a\ntavra ---&gt; j\navraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; e\nsukhe ---&gt; e\nukhee ---&gt; y\nkheey ---&gt; a\nheeya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; j\n.balj ---&gt; i\nbalji ---&gt; n\naljin ---&gt; d\nljind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; u\n.rahu ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; n\n.jhun ---&gt; n\njhunn ---&gt; u\nhunnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; h\nshobh ---&gt; a\nhobha ---&gt; r\nobhar ---&gt; a\nbhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; m\n..hom ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; e\nulshe ---&gt; r\nlsher ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; u\n.sanu ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; a\n..eka ---&gt; m\n.ekam ---&gt; j\nekamj ---&gt; e\nkamje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; v\n.mehv ---&gt; i\nmehvi ---&gt; s\nehvis ---&gt; h\nhvish ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; s\n.bhis ---&gt; m\nbhism ---&gt; p\nhismp ---&gt; a\nismpa ---&gt; l\nsmpal ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; v\n.perv ---&gt; e\nperve ---&gt; e\nervee ---&gt; n\nrveen ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; e\nveere ---&gt; n\neeren ---&gt; d\nerend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; u\n.jisu ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; w\n.ashw ---&gt; i\nashwi ---&gt; n\nshwin ---&gt; i\nhwini ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; l\nsukhl ---&gt; a\nukhla ---&gt; l\nkhlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; t\nmanit ---&gt; a\nanita ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; s\n.mars ---&gt; h\nmarsh ---&gt; i\narshi ---&gt; l\nrshil ---&gt; a\nshila ---&gt; h\nhilah ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; u\n.fazu ---&gt; l\nfazul ---&gt; l\nazull ---&gt; a\nzulla ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; a\nompra ---&gt; k\nmprak ---&gt; e\nprake ---&gt; s\nrakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; b\n...kb ---&gt; e\n..kbe ---&gt; t\n.kbet ---&gt; a\nkbeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; i\n.rasi ---&gt; d\nrasid ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; v\nmahav ---&gt; e\nahave ---&gt; e\nhavee ---&gt; r\naveer ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; i\nprami ---&gt; l\nramil ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; o\n.samo ---&gt; n\nsamon ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; r\nmanor ---&gt; m\nanorm ---&gt; a\nnorma ---&gt; .\n..... ---&gt; f\n....f ---&gt; h\n...fh ---&gt; a\n..fha ---&gt; i\n.fhai ---&gt; s\nfhais ---&gt; h\nhaish ---&gt; a\naisha ---&gt; l\nishal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; a\n.nana ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; n\nmehan ---&gt; d\nehand ---&gt; i\nhandi ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; u\n.anku ---&gt; s\nankus ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; y\nsajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; n\n.bann ---&gt; u\nbannu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; e\n.jame ---&gt; e\njamee ---&gt; l\nameel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; k\n.milk ---&gt; a\nmilka ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; t\nsamit ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; i\n.yami ---&gt; n\nyamin ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; d\n.fird ---&gt; o\nfirdo ---&gt; u\nirdou ---&gt; s\nrdous ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; j\n.rajj ---&gt; i\nrajji ---&gt; t\najjit ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; a\npusha ---&gt; p\nushap ---&gt; a\nshapa ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; o\n.aamo ---&gt; s\naamos ---&gt; h\namosh ---&gt; .\n..... ---&gt; o\n....o ---&gt; p\n...op ---&gt; e\n..ope ---&gt; n\n.open ---&gt; d\nopend ---&gt; a\npenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; i\nmanoi ---&gt; s\nanois ---&gt; h\nnoish ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; m\nmoham ---&gt; a\nohama ---&gt; d\nhamad ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; i\nmithi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; s\n.vais ---&gt; h\nvaish ---&gt; a\naisha ---&gt; l\nishal ---&gt; i\nshali ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; h\n..arh ---&gt; a\n.arha ---&gt; m\narham ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; n\n.mohn ---&gt; i\nmohni ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; m\n.janm ---&gt; e\njanme ---&gt; s\nanmes ---&gt; h\nnmesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; i\nmunti ---&gt; y\nuntiy ---&gt; a\nntiya ---&gt; j\ntiyaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; i\nhushi ---&gt; l\nushil ---&gt; a\nshila ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; r\nravir ---&gt; a\navira ---&gt; j\nviraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; n\nsohan ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; n\nbhavn ---&gt; a\nhavna ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; i\n.bali ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; f\n..sof ---&gt; a\n.sofa ---&gt; l\nsofal ---&gt; i\nofali ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; i\n.kavi ---&gt; t\nkavit ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; b\n.narb ---&gt; i\nnarbi ---&gt; r\narbir ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; a\nharva ---&gt; n\narvan ---&gt; s\nrvans ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; n\nnitin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; a\nkunda ---&gt; n\nundan ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; i\nlaxmi ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; k\n..rik ---&gt; k\n.rikk ---&gt; e\nrikke ---&gt; n\nikken ---&gt; c\nkkenc ---&gt; h\nkench ---&gt; i\nenchi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; r\nharpr ---&gt; e\narpre ---&gt; e\nrpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; l\n.heml ---&gt; a\nhemla ---&gt; t\nemlat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; n\n.main ---&gt; k\nmaink ---&gt; a\nainka ---&gt; .\n..... ---&gt; l\n....l ---&gt; t\n...lt ---&gt; a\n..lta ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; t\n.pret ---&gt; e\nprete ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; e\n.nime ---&gt; s\nnimes ---&gt; h\nimesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; w\n...nw ---&gt; e\n..nwe ---&gt; d\n.nwed ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; u\nmannu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; n\nmahin ---&gt; d\nahind ---&gt; r\nhindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; e\nrende ---&gt; r\nender ---&gt; a\nndera ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; u\n..ghu ---&gt; r\n.ghur ---&gt; u\nghuru ---&gt; l\nhurul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; h\nshadh ---&gt; a\nhadha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; w\n.bagw ---&gt; a\nbagwa ---&gt; n\nagwan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; m\n.najm ---&gt; i\nnajmi ---&gt; n\najmin ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; k\n..tik ---&gt; k\n.tikk ---&gt; u\ntikku ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; w\n.kalw ---&gt; a\nkalwa ---&gt; t\nalwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; r\nompar ---&gt; k\nmpark ---&gt; a\nparka ---&gt; s\narkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; n\nlshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; a\nrhana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; l\nhakil ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; e\nmange ---&gt; r\nanger ---&gt; a\nngera ---&gt; m\ngeram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; h\nparth ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; .\n..... ---&gt; n\n....n ---&gt; t\n...nt ---&gt; a\n..nta ---&gt; s\n.ntas ---&gt; h\nntash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; a\n.mula ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; e\n.jane ---&gt; s\njanes ---&gt; h\nanesh ---&gt; w\nneshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; i\n.vasi ---&gt; y\nvasiy ---&gt; a\nasiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; t\n.asht ---&gt; h\nashth ---&gt; a\nshtha ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; b\n.bobb ---&gt; y\nbobby ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; m\nreshm ---&gt; i\neshmi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; a\nchama ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; m\n.chom ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; a\n.reha ---&gt; n\nrehan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; e\n.sale ---&gt; e\nsalee ---&gt; m\naleem ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; t\nsagit ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; d\nrajid ---&gt; e\najide ---&gt; r\njider ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; r\nmanir ---&gt; a\nanira ---&gt; m\nniram ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; m\n.mosm ---&gt; i\nmosmi ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; o\n..jio ---&gt; t\n.jiot ---&gt; y\njioty ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; a\nharpa ---&gt; l\narpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; b\n.rubb ---&gt; i\nrubbi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; r\nmuntr ---&gt; i\nuntri ---&gt; n\nntrin ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; s\nharis ---&gt; o\nariso ---&gt; n\nrison ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; i\nhooli ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; a\nugana ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; u\n.anuu ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; s\n.jees ---&gt; a\njeesa ---&gt; n\neesan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; e\n.anke ---&gt; s\nankes ---&gt; h\nnkesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; w\nhahnw ---&gt; a\nahnwa ---&gt; j\nhnwaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; m\n.shum ---&gt; i\nshumi ---&gt; t\nhumit ---&gt; a\numita ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; a\n.nama ---&gt; r\nnamar ---&gt; t\namart ---&gt; a\nmarta ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; b\n.ayub ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; s\nvikas ---&gt; .\n..... ---&gt; n\n....n ---&gt; r\n...nr ---&gt; e\n..nre ---&gt; n\n.nren ---&gt; d\nnrend ---&gt; r\nrendr ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; c\n.sakc ---&gt; h\nsakch ---&gt; a\nakcha ---&gt; m\nkcham ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; s\n.khas ---&gt; h\nkhash ---&gt; b\nhashb ---&gt; u\nashbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; l\nampal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; j\nshivj ---&gt; i\nhivji ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; v\n..arv ---&gt; i\n.arvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; z\n..foz ---&gt; i\n.fozi ---&gt; a\nfozia ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; s\n.sars ---&gt; h\nsarsh ---&gt; w\narshw ---&gt; a\nrshwa ---&gt; t\nshwat ---&gt; i\nhwati ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; a\n..iqa ---&gt; r\n.iqar ---&gt; a\niqara ---&gt; r\nqarar ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; i\n..swi ---&gt; t\n.swit ---&gt; i\nswiti ---&gt; .\n..... ---&gt; j\n....j ---&gt; w\n...jw ---&gt; a\n..jwa ---&gt; l\n.jwal ---&gt; a\njwala ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; h\n.hash ---&gt; i\nhashi ---&gt; m\nashim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; e\nnajre ---&gt; e\najree ---&gt; n\njreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; e\n.shre ---&gt; e\nshree ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; k\nchink ---&gt; u\nhinku ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; h\nmansh ---&gt; a\nansha ---&gt; r\nnshar ---&gt; a\nshara ---&gt; m\nharam ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; n\nrupen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; i\n.kabi ---&gt; r\nkabir ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; n\n..arn ---&gt; a\n.arna ---&gt; b\narnab ---&gt; j\nrnabj ---&gt; i\nnabji ---&gt; t\nabjit ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; e\n..ume ---&gt; s\n.umes ---&gt; h\numesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; y\n.shoy ---&gt; a\nshoya ---&gt; b\nhoyab ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; l\n.mall ---&gt; i\nmalli ---&gt; k\nallik ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; p\n.manp ---&gt; h\nmanph ---&gt; o\nanpho ---&gt; o\nnphoo ---&gt; l\nphool ---&gt; .\n..... ---&gt; z\n....z ---&gt; o\n...zo ---&gt; y\n..zoy ---&gt; a\n.zoya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; r\n.sayr ---&gt; a\nsayra ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; m\nbharm ---&gt; a\nharma ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; a\nmusta ---&gt; f\nustaf ---&gt; a\nstafa ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; n\n.deen ---&gt; d\ndeend ---&gt; y\neendy ---&gt; a\nendya ---&gt; l\nndyal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; r\n.kumr ---&gt; e\nkumre ---&gt; s\numres ---&gt; h\nmresh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; e\nbhude ---&gt; v\nhudev ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; k\n..huk ---&gt; a\n.huka ---&gt; m\nhukam ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; b\n..dib ---&gt; y\n.diby ---&gt; a\ndibya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; a\n.naga ---&gt; j\nnagaj ---&gt; i\nagaji ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; i\n.tari ---&gt; q\ntariq ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; o\n.kiso ---&gt; r\nkisor ---&gt; i\nisori ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; u\nanchu ---&gt; r\nnchur ---&gt; i\nchuri ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; a\n.naja ---&gt; r\nnajar ---&gt; a\najara ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; a\n.papa ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; s\n.nees ---&gt; h\nneesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; m\n.birm ---&gt; a\nbirma ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; r\n..ner ---&gt; a\n.nera ---&gt; j\nneraj ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; y\n.chay ---&gt; a\nchaya ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; j\n.anuj ---&gt; a\nanuja ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; a\n..tra ---&gt; n\n.tran ---&gt; o\ntrano ---&gt; o\nranoo ---&gt; m\nanoom ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; t\nbhart ---&gt; i\nharti ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; s\n..tos ---&gt; h\n.tosh ---&gt; a\ntosha ---&gt; r\noshar ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; a\n..upa ---&gt; s\n.upas ---&gt; n\nupasn ---&gt; a\npasna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; i\n.susi ---&gt; l\nsusil ---&gt; a\nusila ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; k\nomprk ---&gt; e\nmprke ---&gt; s\nprkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; b\nhahib ---&gt; a\nahiba ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; v\nmadhv ---&gt; i\nadhvi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; n\n.saun ---&gt; i\nsauni ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; i\nreeti ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; h\n..tah ---&gt; s\n.tahs ---&gt; e\ntahse ---&gt; e\nahsee ---&gt; n\nhseen ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; i\nruchi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; n\n.jain ---&gt; a\njaina ---&gt; f\nainaf ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; i\n.lili ---&gt; m\nlilim ---&gt; a\nilima ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; z\n.prez ---&gt; i\nprezi ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; b\n..dab ---&gt; b\n.dabb ---&gt; u\ndabbu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; e\n.sate ---&gt; n\nsaten ---&gt; d\natend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; r\n.sher ---&gt; u\nsheru ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; m\n.jagm ---&gt; o\njagmo ---&gt; h\nagmoh ---&gt; a\ngmoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; c\ninakc ---&gt; h\nnakch ---&gt; i\nakchi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; n\nmahen ---&gt; d\nahend ---&gt; r\nhendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; e\n.gope ---&gt; s\ngopes ---&gt; h\nopesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; r\nnasir ---&gt; a\nasira ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; j\n.anaj ---&gt; i\nanaji ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; r\n.hemr ---&gt; a\nhemra ---&gt; j\nemraj ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; u\n.papu ---&gt; u\npapuu ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; a\n.mila ---&gt; n\nmilan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; r\n.shir ---&gt; i\nshiri ---&gt; s\nhiris ---&gt; h\nirish ---&gt; t\nrisht ---&gt; y\nishty ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; n\nsahan ---&gt; a\nahana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; c\n.baic ---&gt; h\nbaich ---&gt; a\naicha ---&gt; n\nichan ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; u\nbindu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; u\n..amu ---&gt; c\n.amuc ---&gt; h\namuch ---&gt; l\nmuchl ---&gt; a\nuchla ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; j\n..ujj ---&gt; a\n.ujja ---&gt; i\nujjai ---&gt; r\njjair ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; a\nnoora ---&gt; l\nooral ---&gt; a\norala ---&gt; m\nralam ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; r\n.anir ---&gt; u\naniru ---&gt; d\nnirud ---&gt; h\nirudh ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; n\n..avn ---&gt; i\n.avni ---&gt; t\navnit ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; s\n.anus ---&gt; k\nanusk ---&gt; a\nnuska ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; i\ndeepi ---&gt; k\neepik ---&gt; a\nepika ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; t\n.roht ---&gt; a\nrohta ---&gt; s\nohtas ---&gt; h\nhtash ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; u\ntabbu ---&gt; s\nabbus ---&gt; u\nbbusu ---&gt; m\nbusum ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; n\n.veen ---&gt; u\nveenu ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; t\n.neet ---&gt; i\nneeti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; t\n.mast ---&gt; a\nmasta ---&gt; n\nastan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; m\nsalim ---&gt; a\nalima ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; b\n.manb ---&gt; a\nmanba ---&gt; i\nanbai ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; u\n.naru ---&gt; r\nnarur ---&gt; a\narura ---&gt; m\nruram ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; c\n.manc ---&gt; h\nmanch ---&gt; a\nancha ---&gt; n\nnchan ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; n\n.forn ---&gt; t\nfornt ---&gt; a\nornta ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; p\n.tejp ---&gt; a\ntejpa ---&gt; l\nejpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; b\n.rubb ---&gt; y\nrubby ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; h\n.bakh ---&gt; t\nbakht ---&gt; v\nakhtv ---&gt; a\nkhtva ---&gt; a\nhtvaa ---&gt; r\ntvaar ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; r\nkamar ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; e\n.fare ---&gt; e\nfaree ---&gt; n\nareen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; b\n..shb ---&gt; a\n.shba ---&gt; n\nshban ---&gt; a\nhbana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; k\nsamik ---&gt; s\namiks ---&gt; a\nmiksa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; v\n.ranv ---&gt; i\nranvi ---&gt; r\nanvir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; e\n.sike ---&gt; n\nsiken ---&gt; d\nikend ---&gt; e\nkende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; h\npardh ---&gt; u\nardhu ---&gt; m\nrdhum ---&gt; a\ndhuma ---&gt; n\nhuman ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; r\n.deer ---&gt; a\ndeera ---&gt; j\neeraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; a\n.maka ---&gt; n\nmakan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; a\n.nata ---&gt; s\nnatas ---&gt; h\natash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; a\nroopa ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; r\n..ter ---&gt; e\n.tere ---&gt; n\nteren ---&gt; c\nerenc ---&gt; e\nrence ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; t\n.mamt ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; l\npreml ---&gt; a\nremla ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; y\n.kosy ---&gt; l\nkosyl ---&gt; a\nosyla ---&gt; y\nsylay ---&gt; a\nylaya ---&gt; .\n..... ---&gt; z\n....z ---&gt; u\n...zu ---&gt; v\n..zuv ---&gt; e\n.zuve ---&gt; b\nzuveb ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; j\n.sarj ---&gt; a\nsarja ---&gt; h\narjah ---&gt; a\nrjaha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; c\n.balc ---&gt; h\nbalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; e\njasve ---&gt; e\nasvee ---&gt; r\nsveer ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; i\n.tohi ---&gt; r\ntohir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; i\n.maji ---&gt; d\nmajid ---&gt; a\najida ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; i\n.guli ---&gt; s\ngulis ---&gt; t\nulist ---&gt; a\nlista ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; f\n..naf ---&gt; i\n.nafi ---&gt; s\nnafis ---&gt; a\nafisa ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; a\n.mada ---&gt; n\nmadan ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; r\nabhir ---&gt; a\nbhira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; r\nadhur ---&gt; i\ndhuri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; a\n.mama ---&gt; n\nmaman ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; r\n..dir ---&gt; g\n.dirg ---&gt; a\ndirga ---&gt; j\nirgaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; a\nsunda ---&gt; r\nundar ---&gt; i\nndari ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; i\n.udai ---&gt; v\nudaiv ---&gt; e\ndaive ---&gt; e\naivee ---&gt; r\niveer ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; r\n.yogr ---&gt; a\nyogra ---&gt; j\nograj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; s\nharis ---&gt; h\narish ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; o\n..soo ---&gt; n\n.soon ---&gt; a\nsoona ---&gt; m\noonam ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; s\n.muns ---&gt; h\nmunsh ---&gt; i\nunshi ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; a\n..kra ---&gt; n\n.kran ---&gt; t\nkrant ---&gt; i\nranti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; g\n.maig ---&gt; o\nmaigo ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; n\n.anjn ---&gt; a\nanjna ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; t\n.sint ---&gt; u\nsintu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; s\nmanis ---&gt; a\nanisa ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; s\n..zis ---&gt; h\n.zish ---&gt; a\nzisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; j\namanj ---&gt; e\nmanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; u\nmeenu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; a\naasha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; s\nnares ---&gt; h\naresh ---&gt; p\nreshp ---&gt; a\neshpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; b\n.sehb ---&gt; a\nsehba ---&gt; j\nehbaj ---&gt; .\n..... ---&gt; h\n....h ---&gt; r\n...hr ---&gt; i\n..hri ---&gt; t\n.hrit ---&gt; i\nhriti ---&gt; k\nritik ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; r\n.heer ---&gt; a\nheera ---&gt; l\neeral ---&gt; a\nerala ---&gt; l\nralal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; y\n..soy ---&gt; a\n.soya ---&gt; m\nsoyam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; r\n.sagr ---&gt; i\nsagri ---&gt; k\nagrik ---&gt; a\ngrika ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; i\n.riti ---&gt; k\nritik ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; c\n..nic ---&gt; h\n.nich ---&gt; a\nnicha ---&gt; r\nichar ---&gt; o\ncharo ---&gt; n\nharon ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; f\n..irf ---&gt; a\n.irfa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; a\n.nawa ---&gt; l\nnawal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; i\nbhani ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; j\n..omj ---&gt; o\n.omjo ---&gt; n\nomjon ---&gt; e\nmjone ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; r\n..jor ---&gt; i\n.jori ---&gt; a\njoria ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; u\nmannu ---&gt; l\nannul ---&gt; a\nnnula ---&gt; l\nnulal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; a\nsarva ---&gt; n\narvan ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; h\n.hash ---&gt; r\nhashr ---&gt; a\nashra ---&gt; t\nshrat ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; r\n.kesr ---&gt; i\nkesri ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; z\nulnaz ---&gt; a\nlnaza ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; a\n.rana ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; t\nchhat ---&gt; a\nhhata ---&gt; r\nhatar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; u\nnandu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; o\n.biro ---&gt; n\nbiron ---&gt; i\nironi ---&gt; c\nronic ---&gt; a\nonica ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; w\n..gaw ---&gt; a\n.gawa ---&gt; l\ngawal ---&gt; i\nawali ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; e\n.bale ---&gt; s\nbales ---&gt; h\nalesh ---&gt; w\nleshw ---&gt; e\neshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; r\n.mahr ---&gt; a\nmahra ---&gt; j\nahraj ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; w\n..dow ---&gt; l\n.dowl ---&gt; a\ndowla ---&gt; t\nowlat ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; k\n..omk ---&gt; a\n.omka ---&gt; r\nomkar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; p\n..kup ---&gt; a\n.kupa ---&gt; r\nkupar ---&gt; t\nupart ---&gt; h\nparth ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; r\n..tir ---&gt; l\n.tirl ---&gt; o\ntirlo ---&gt; k\nirlok ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; b\nsahab ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; b\n.somb ---&gt; i\nsombi ---&gt; r\nombir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; i\nshadi ---&gt; y\nhadiy ---&gt; a\nadiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; k\n.jank ---&gt; i\njanki ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; f\n.ashf ---&gt; a\nashfa ---&gt; q\nshfaq ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; e\nangee ---&gt; t\nngeet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; i\n.aasi ---&gt; s\naasis ---&gt; h\nasish ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; a\n.mena ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; l\n.murl ---&gt; i\nmurli ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; l\ndayal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; k\nsheik ---&gt; h\nheikh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; w\n.satw ---&gt; a\nsatwa ---&gt; n\natwan ---&gt; d\ntwand ---&gt; e\nwande ---&gt; r\nander ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; n\nsheen ---&gt; u\nheenu ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; r\n..pir ---&gt; o\n.piro ---&gt; j\npiroj ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; s\n.vais ---&gt; h\nvaish ---&gt; a\naisha ---&gt; n\nishan ---&gt; v\nshanv ---&gt; i\nhanvi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; w\n.manw ---&gt; i\nmanwi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; n\nbishn ---&gt; u\nishnu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; i\n.kawi ---&gt; t\nkawit ---&gt; a\nawita ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; d\n.dild ---&gt; a\ndilda ---&gt; r\nildar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; i\nkumai ---&gt; l\numail ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; a\n.bija ---&gt; l\nbijal ---&gt; i\nijali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; j\n.dilj ---&gt; a\ndilja ---&gt; n\niljan ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; l\n..uml ---&gt; e\n.umle ---&gt; s\numles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; b\n.matb ---&gt; a\nmatba ---&gt; r\natbar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; l\n..shl ---&gt; e\n.shle ---&gt; n\nshlen ---&gt; d\nhlend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; i\nkushi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; i\n..gai ---&gt; t\n.gait ---&gt; r\ngaitr ---&gt; i\naitri ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; u\nimamu ---&gt; d\nmamud ---&gt; d\namudd ---&gt; i\nmuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; m\n.tasm ---&gt; i\ntasmi ---&gt; n\nasmin ---&gt; a\nsmina ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; e\nbishe ---&gt; s\nishes ---&gt; h\nshesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; v\nsukhv ---&gt; e\nukhve ---&gt; e\nkhvee ---&gt; r\nhveer ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; g\n..nig ---&gt; a\n.niga ---&gt; r\nnigar ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; u\n.anku ---&gt; r\nankur ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; a\nsaida ---&gt; s\naidas ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; t\n.pint ---&gt; u\npintu ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; a\n.mila ---&gt; p\nmilap ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; w\n..anw ---&gt; a\n.anwa ---&gt; r\nanwar ---&gt; i\nnwari ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; h\ndaksh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; k\n..dik ---&gt; s\n.diks ---&gt; h\ndiksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; i\n.suri ---&gt; t\nsurit ---&gt; i\nuriti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; j\nshahj ---&gt; a\nhahja ---&gt; d\nahjad ---&gt; i\nhjadi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; w\n..siw ---&gt; a\n.siwa ---&gt; n\nsiwan ---&gt; i\niwani ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; c\n.tulc ---&gt; h\ntulch ---&gt; a\nulcha ---&gt; r\nlchar ---&gt; a\nchara ---&gt; m\nharam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; r\n.saur ---&gt; a\nsaura ---&gt; b\naurab ---&gt; h\nurabh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; a\nmansa ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; j\nehnaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; d\n.sidd ---&gt; a\nsidda ---&gt; r\niddar ---&gt; t\nddart ---&gt; h\ndarth ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; l\njamal ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; e\nharme ---&gt; n\narmen ---&gt; d\nrmend ---&gt; r\nmendr ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; k\n.mank ---&gt; u\nmanku ---&gt; r\nankur ---&gt; i\nnkuri ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; d\n..ked ---&gt; a\n.keda ---&gt; r\nkedar ---&gt; n\nedarn ---&gt; a\ndarna ---&gt; t\narnat ---&gt; h\nrnath ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; e\n..tre ---&gt; p\n.trep ---&gt; t\ntrept ---&gt; i\nrepti ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; s\n.vars ---&gt; h\nvarsh ---&gt; a\narsha ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; i\n.aati ---&gt; r\naatir ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; h\n.mukh ---&gt; t\nmukht ---&gt; y\nukhty ---&gt; a\nkhtya ---&gt; r\nhtyar ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; i\nohani ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; y\nakshy ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; l\n.naul ---&gt; e\nnaule ---&gt; s\naules ---&gt; h\nulesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; g\nbhagg ---&gt; o\nhaggo ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; l\n..pul ---&gt; k\n.pulk ---&gt; i\npulki ---&gt; t\nulkit ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; n\n.srin ---&gt; i\nsrini ---&gt; w\nriniw ---&gt; a\niniwa ---&gt; s\nniwas ---&gt; h\niwash ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; p\n..kap ---&gt; i\n.kapi ---&gt; l\nkapil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; i\n.nabi ---&gt; j\nnabij ---&gt; a\nabija ---&gt; n\nbijan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; b\nmahab ---&gt; i\nahabi ---&gt; r\nhabir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; d\n.sahd ---&gt; e\nsahde ---&gt; v\nahdev ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; p\nhishp ---&gt; a\nishpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; a\n.vira ---&gt; m\nviram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; a\nkanha ---&gt; y\nanhay ---&gt; a\nnhaya ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; p\n.krip ---&gt; a\nkripa ---&gt; l\nripal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; d\n.pand ---&gt; i\npandi ---&gt; t\nandit ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; k\n..lek ---&gt; h\n.lekh ---&gt; r\nlekhr ---&gt; a\nekhra ---&gt; m\nkhram ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; a\n.faza ---&gt; z\nfazaz ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; k\n.lakk ---&gt; i\nlakki ---&gt; .\n..... ---&gt; u\n....u ---&gt; z\n...uz ---&gt; m\n..uzm ---&gt; a\n.uzma ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; r\nsakir ---&gt; a\nakira ---&gt; n\nkiran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; t\nsawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; t\n.seet ---&gt; a\nseeta ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; a\nrisha ---&gt; m\nisham ---&gt; a\nshama ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; n\n.hann ---&gt; i\nhanni ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; v\nramav ---&gt; t\namavt ---&gt; a\nmavta ---&gt; r\navtar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; n\nsalin ---&gt; .\n..... ---&gt; r\n....r ---&gt; v\n...rv ---&gt; i\n..rvi ---&gt; n\n.rvin ---&gt; d\nrvind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; a\nindra ---&gt; j\nndraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; r\n..mer ---&gt; r\n.merr ---&gt; y\nmerry ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; b\n.balb ---&gt; i\nbalbi ---&gt; r\nalbir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; s\n.jays ---&gt; h\njaysh ---&gt; i\nayshi ---&gt; n\nyshin ---&gt; g\nshing ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; r\n.misr ---&gt; i\nmisri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; a\nprema ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; r\nsarar ---&gt; w\nararw ---&gt; a\nrarwa ---&gt; t\narwat ---&gt; i\nrwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; i\n.ambi ---&gt; y\nambiy ---&gt; a\nmbiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; k\n.jayk ---&gt; u\njayku ---&gt; m\naykum ---&gt; a\nykuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; o\nancho ---&gt; o\nnchoo ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; i\n.rubi ---&gt; n\nrubin ---&gt; a\nubina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; a\nsanta ---&gt; r\nantar ---&gt; a\nntara ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; m\n.srim ---&gt; a\nsrima ---&gt; t\nrimat ---&gt; i\nimati ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; h\n.jish ---&gt; a\njisha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; d\n.band ---&gt; a\nbanda ---&gt; n\nandan ---&gt; a\nndana ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; i\n.akhi ---&gt; l\nakhil ---&gt; e\nkhile ---&gt; s\nhiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; v\n..lov ---&gt; e\n.love ---&gt; l\nlovel ---&gt; y\novely ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; n\n.maan ---&gt; s\nmaans ---&gt; i\naansi ---&gt; n\nansin ---&gt; g\nnsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; n\nshabn ---&gt; u\nhabnu ---&gt; r\nabnur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; m\natyam ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; b\n.khub ---&gt; c\nkhubc ---&gt; h\nhubch ---&gt; a\nubcha ---&gt; n\nbchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; k\n.jank ---&gt; i\njanki ---&gt; d\nankid ---&gt; a\nnkida ---&gt; s\nkidas ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; u\n.taru ---&gt; n\ntarun ---&gt; n\narunn ---&gt; a\nrunna ---&gt; m\nunnam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; h\nsardh ---&gt; u\nardhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; t\nuchit ---&gt; r\nchitr ---&gt; a\nhitra ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; i\nveeri ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; e\nrubee ---&gt; n\nubeen ---&gt; a\nbeena ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; i\n.pati ---&gt; k\npatik ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; e\ndhire ---&gt; n\nhiren ---&gt; d\nirend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; r\n.asar ---&gt; f\nasarf ---&gt; i\nsarfi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; y\nreety ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; o\nshabo ---&gt; o\nhaboo ---&gt; b\naboob ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; l\n.jasl ---&gt; i\njasli ---&gt; n\naslin ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; j\n.murj ---&gt; i\nmurji ---&gt; n\nurjin ---&gt; a\nrjina ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; h\narish ---&gt; a\nrisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; o\n..dho ---&gt; l\n.dhol ---&gt; a\ndhola ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; k\nramak ---&gt; a\namaka ---&gt; n\nmakan ---&gt; t\nakant ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; i\n.hali ---&gt; y\nhaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; b\n.bhab ---&gt; h\nbhabh ---&gt; i\nhabhi ---&gt; y\nabhiy ---&gt; a\nbhiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; b\n.arib ---&gt; a\nariba ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; e\nhishe ---&gt; k\nishek ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; i\nmangi ---&gt; l\nangil ---&gt; a\nngila ---&gt; l\ngilal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; d\n.guld ---&gt; a\ngulda ---&gt; s\nuldas ---&gt; t\nldast ---&gt; a\ndasta ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; b\n..mob ---&gt; s\n.mobs ---&gt; h\nmobsh ---&gt; e\nobshe ---&gt; e\nbshee ---&gt; r\nsheer ---&gt; a\nheera ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; d\n..sod ---&gt; a\n.soda ---&gt; n\nsodan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; y\n.supy ---&gt; a\nsupya ---&gt; r\nupyar ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; s\n..was ---&gt; i\n.wasi ---&gt; m\nwasim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; s\nnares ---&gt; h\naresh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; e\nparee ---&gt; n\nareen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; m\nshaym ---&gt; a\nhayma ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; n\nviren ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; z\n..waz ---&gt; i\n.wazi ---&gt; d\nwazid ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; i\n.suji ---&gt; t\nsujit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; r\n.samr ---&gt; e\nsamre ---&gt; e\namree ---&gt; n\nmreen ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; s\n..das ---&gt; h\n.dash ---&gt; a\ndasha ---&gt; r\nashar ---&gt; a\nshara ---&gt; t\nharat ---&gt; h\narath ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; x\ninakx ---&gt; i\nnakxi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; o\n.salo ---&gt; m\nsalom ---&gt; i\nalomi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; a\nhaila ---&gt; s\nailas ---&gt; h\nilash ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; d\n..kad ---&gt; u\n.kadu ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; e\nindre ---&gt; s\nndres ---&gt; h\ndresh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; n\n.bhon ---&gt; u\nbhonu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; e\nshele ---&gt; n\nhelen ---&gt; d\nelend ---&gt; r\nlendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; k\nsafik ---&gt; a\nafika ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; e\n..ure ---&gt; n\n.uren ---&gt; d\nurend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; r\n.hajr ---&gt; a\nhajra ---&gt; t\najrat ---&gt; i\njrati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; i\nnanki ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; y\n.aliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; o\n.moto ---&gt; l\nmotol ---&gt; a\notola ---&gt; l\ntolal ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; b\n.tasb ---&gt; i\ntasbi ---&gt; h\nasbih ---&gt; a\nsbiha ---&gt; .\n..... ---&gt; f\n....f ---&gt; r\n...fr ---&gt; a\n..fra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; i\nparmi ---&gt; l\narmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; i\n....i ---&gt; d\n...id ---&gt; r\n..idr ---&gt; i\n.idri ---&gt; s\nidris ---&gt; h\ndrish ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; d\n.budd ---&gt; h\nbuddh ---&gt; i\nuddhi ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; n\ndivan ---&gt; s\nivans ---&gt; h\nvansh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; l\npreml ---&gt; a\nremla ---&gt; t\nemlat ---&gt; a\nmlata ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; k\nshank ---&gt; a\nhanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; d\n.khad ---&gt; a\nkhada ---&gt; k\nhadak ---&gt; .\n..... ---&gt; d\n....d ---&gt; w\n...dw ---&gt; a\n..dwa ---&gt; r\n.dwar ---&gt; i\ndwari ---&gt; k\nwarik ---&gt; a\narika ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; b\n..mob ---&gt; i\n.mobi ---&gt; n\nmobin ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; a\ngulsa ---&gt; n\nulsan ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; b\n..iqb ---&gt; a\n.iqba ---&gt; l\niqbal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; l\n..bil ---&gt; a\n.bila ---&gt; l\nbilal ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; t\n..smt ---&gt; s\n.smts ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; k\n..pok ---&gt; h\n.pokh ---&gt; a\npokha ---&gt; r\nokhar ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; w\n.virw ---&gt; a\nvirwa ---&gt; n\nirwan ---&gt; t\nrwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; i\nsanti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; h\n.mohh ---&gt; m\nmohhm ---&gt; a\nohhma ---&gt; d\nhhmad ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; e\n.vake ---&gt; e\nvakee ---&gt; l\nakeel ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; i\nsugai ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; l\nsital ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; n\n.husn ---&gt; a\nhusna ---&gt; r\nusnar ---&gt; a\nsnara ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; t\n..net ---&gt; r\n.netr ---&gt; a\nnetra ---&gt; m\netram ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; k\n..luk ---&gt; k\n.lukk ---&gt; a\nlukka ---&gt; d\nukkad ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; i\n.ragi ---&gt; n\nragin ---&gt; a\nagina ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; r\n.nazr ---&gt; e\nnazre ---&gt; e\nazree ---&gt; n\nzreen ---&gt; .\n..... ---&gt; h\n....h ---&gt; y\n...hy ---&gt; a\n..hya ---&gt; t\n.hyat ---&gt; u\nhyatu ---&gt; n\nyatun ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; i\nmooli ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; h\n..ruh ---&gt; i\n.ruhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; k\n.vick ---&gt; y\nvicky ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; j\n.tarj ---&gt; a\ntarja ---&gt; n\narjan ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; e\nhishe ---&gt; k\nishek ---&gt; h\nshekh ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; h\n..koh ---&gt; i\n.kohi ---&gt; n\nkohin ---&gt; o\nohino ---&gt; o\nhinoo ---&gt; r\ninoor ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; t\n.jagt ---&gt; a\njagta ---&gt; r\nagtar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; h\nshekh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; r\n.bher ---&gt; a\nbhera ---&gt; v\nherav ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; j\n.bhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; l\najanl ---&gt; a\njanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; v\n.balv ---&gt; e\nbalve ---&gt; e\nalvee ---&gt; r\nlveer ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; h\nparsh ---&gt; u\narshu ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; e\n..upe ---&gt; n\n.upen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; n\n.rain ---&gt; u\nrainu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; c\n..aac ---&gt; h\n.aach ---&gt; a\naacha ---&gt; l\nachal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; e\n.jave ---&gt; d\njaved ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; n\n.tasn ---&gt; i\ntasni ---&gt; m\nasnim ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; k\nsarik ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; k\nmustk ---&gt; e\nustke ---&gt; e\nstkee ---&gt; m\ntkeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; u\n.suku ---&gt; l\nsukul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; a\nramka ---&gt; n\namkan ---&gt; y\nmkany ---&gt; a\nkanya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; j\nrabhj ---&gt; e\nabhje ---&gt; e\nbhjee ---&gt; t\nhjeet ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; s\nepans ---&gt; u\npansu ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; a\n..yaa ---&gt; d\n.yaad ---&gt; r\nyaadr ---&gt; a\naadra ---&gt; m\nadram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; a\nkalua ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; e\n.chee ---&gt; t\ncheet ---&gt; e\nheete ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; a\n.faza ---&gt; l\nfazal ---&gt; e\nazale ---&gt; y\nzaley ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; j\n..laj ---&gt; j\n.lajj ---&gt; a\nlajja ---&gt; v\najjav ---&gt; a\njjava ---&gt; t\njavat ---&gt; i\navati ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; m\n.birm ---&gt; a\nbirma ---&gt; t\nirmat ---&gt; i\nrmati ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; i\nsushi ---&gt; l\nushil ---&gt; a\nshila ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; s\n..tos ---&gt; h\n.tosh ---&gt; e\ntoshe ---&gt; e\noshee ---&gt; b\nsheeb ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; r\nshair ---&gt; a\nhaira ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; m\n.alam ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; w\n..jaw ---&gt; e\n.jawe ---&gt; d\njawed ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; s\n..jos ---&gt; i\n.josi ---&gt; n\njosin ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; m\nreshm ---&gt; a\neshma ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; y\netany ---&gt; a\ntanya ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; m\n.rehm ---&gt; a\nrehma ---&gt; n\nehman ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; v\nvishv ---&gt; a\nishva ---&gt; s\nshvas ---&gt; h\nhvash ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; r\n.mahr ---&gt; o\nmahro ---&gt; j\nahroj ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; n\n.peen ---&gt; a\npeena ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; w\n..sew ---&gt; e\n.sewe ---&gt; t\nsewet ---&gt; a\neweta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; n\nmanin ---&gt; d\nanind ---&gt; e\nninde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; a\n.aana ---&gt; m\naanam ---&gt; i\nanami ---&gt; k\nnamik ---&gt; a\namika ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; a\nrajka ---&gt; r\najkar ---&gt; n\njkarn ---&gt; t\nkarnt ---&gt; a\narnta ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; n\n.parn ---&gt; a\nparna ---&gt; v\narnav ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; p\npremp ---&gt; a\nrempa ---&gt; l\nempal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; a\n.anga ---&gt; n\nangan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; o\n.nano ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; m\nnayam ---&gt; a\nayama ---&gt; t\nyamat ---&gt; i\namati ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; i\n.gopi ---&gt; r\ngopir ---&gt; a\nopira ---&gt; m\npiram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; h\narish ---&gt; a\nrisha ---&gt; m\nisham ---&gt; a\nshama ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; h\n.saah ---&gt; i\nsaahi ---&gt; n\naahin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; r\n.jagr ---&gt; t\njagrt ---&gt; i\nagrti ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; s\n.muns ---&gt; i\nmunsi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; a\n.naba ---&gt; l\nnabal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; n\nhawan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; a\nragha ---&gt; v\naghav ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; n\nhasin ---&gt; a\nasina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; j\n.sarj ---&gt; e\nsarje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; k\n.shik ---&gt; a\nshika ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; b\n..gob ---&gt; i\n.gobi ---&gt; n\ngobin ---&gt; d\nobind ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; y\nnaviy ---&gt; a\naviya ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; i\nemani ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; k\n..utk ---&gt; a\n.utka ---&gt; r\nutkar ---&gt; s\ntkars ---&gt; h\nkarsh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; j\nangaj ---&gt; a\nngaja ---&gt; l\ngajal ---&gt; i\najali ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; l\nsawal ---&gt; i\nawali ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; a\n.kosa ---&gt; l\nkosal ---&gt; y\nosaly ---&gt; a\nsalya ---&gt; .\n..... ---&gt; m\n....m ---&gt; r\n...mr ---&gt; s\n..mrs ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; s\n.fais ---&gt; a\nfaisa ---&gt; l\naisal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; k\n.pank ---&gt; u\npanku ---&gt; j\nankuj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; e\n.kale ---&gt; r\nkaler ---&gt; a\nalera ---&gt; m\nleram ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; e\n.rake ---&gt; s\nrakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; d\n.najd ---&gt; a\nnajda ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; k\n.mank ---&gt; a\nmanka ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; s\n.dhis ---&gt; a\ndhisa ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; o\nhando ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; r\n.geer ---&gt; n\ngeern ---&gt; i\neerni ---&gt; s\nernis ---&gt; h\nrnish ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; a\n..aba ---&gt; s\n.abas ---&gt; h\nabash ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; v\n.purv ---&gt; a\npurva ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; a\n.nena ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; u\n..dau ---&gt; j\n.dauj ---&gt; i\ndauji ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; u\n.ashu ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; s\n.ashs ---&gt; h\nashsh ---&gt; w\nshshw ---&gt; e\nhshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; t\n..bet ---&gt; i\n.beti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; e\n.rade ---&gt; s\nrades ---&gt; y\nadesy ---&gt; a\ndesya ---&gt; m\nesyam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; r\n.bahr ---&gt; a\nbahra ---&gt; t\nahrat ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; o\n.aamo ---&gt; d\naamod ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; a\nharda ---&gt; s\nardas ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; n\n..jon ---&gt; y\n.jony ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; b\n..jib ---&gt; a\n.jiba ---&gt; n\njiban ---&gt; t\nibant ---&gt; i\nbanti ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; k\n..tik ---&gt; a\n.tika ---&gt; r\ntikar ---&gt; a\nikara ---&gt; m\nkaram ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; p\ngurup ---&gt; r\nurupr ---&gt; e\nrupre ---&gt; e\nupree ---&gt; t\npreet ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; k\n..mik ---&gt; e\n.mike ---&gt; l\nmikel ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; r\n.musr ---&gt; a\nmusra ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; u\n.amru ---&gt; d\namrud ---&gt; i\nmrudi ---&gt; n\nrudin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; a\nsabba ---&gt; b\nabbab ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; a\nbhaga ---&gt; y\nhagay ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; m\n.kham ---&gt; c\nkhamc ---&gt; h\nhamch ---&gt; a\namcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; m\n....m ---&gt; j\n...mj ---&gt; u\n..mju ---&gt; d\n.mjud ---&gt; i\nmjudi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; t\n.vict ---&gt; o\nvicto ---&gt; r\nictor ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; z\nriyaz ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; e\njagde ---&gt; e\nagdee ---&gt; p\ngdeep ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; v\nashiv ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; e\n.rite ---&gt; s\nrites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; l\nsonal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; e\nsabee ---&gt; h\nabeeh ---&gt; a\nbeeha ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; i\n.shki ---&gt; l\nshkil ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; w\n..asw ---&gt; i\n.aswi ---&gt; n\naswin ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; o\n.chho ---&gt; t\nchhot ---&gt; u\nhhotu ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; l\nurgal ---&gt; a\nrgala ---&gt; l\ngalal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; h\n.sabh ---&gt; a\nsabha ---&gt; n\nabhan ---&gt; a\nbhana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; m\n..shm ---&gt; a\n.shma ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; m\nbadam ---&gt; i\nadami ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; g\n..mag ---&gt; a\n.maga ---&gt; t\nmagat ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; i\njasvi ---&gt; r\nasvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; e\nsabee ---&gt; r\nabeer ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; r\n.begr ---&gt; a\nbegra ---&gt; j\negraj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; n\n.harn ---&gt; a\nharna ---&gt; m\narnam ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; l\n.heml ---&gt; e\nhemle ---&gt; t\nemlet ---&gt; a\nmleta ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; i\nprami ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; u\nsabbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; e\nrajee ---&gt; v\najeev ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; a\n..baa ---&gt; m\n.baam ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; g\n..sng ---&gt; e\n.snge ---&gt; e\nsngee ---&gt; t\nngeet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; y\nmunny ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; i\nbindi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; k\n.menk ---&gt; a\nmenka ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; v\n.balv ---&gt; i\nbalvi ---&gt; n\nalvin ---&gt; d\nlvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; s\nemans ---&gt; h\nmansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; d\n.sidd ---&gt; h\nsiddh ---&gt; i\niddhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; n\nhabin ---&gt; a\nabina ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; i\nshani ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; a\n..ava ---&gt; s\n.avas ---&gt; t\navast ---&gt; h\nvasth ---&gt; a\nastha ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; h\n.alah ---&gt; b\nalahb ---&gt; a\nlahba ---&gt; s\nahbas ---&gt; r\nhbasr ---&gt; i\nbasri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; u\n.rasu ---&gt; l\nrasul ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; a\nancha ---&gt; m\nncham ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; k\n.vick ---&gt; e\nvicke ---&gt; y\nickey ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; d\n.amid ---&gt; a\namida ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; l\nshiml ---&gt; a\nhimla ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; n\n..din ---&gt; k\n.dink ---&gt; a\ndinka ---&gt; r\ninkar ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; n\nvandn ---&gt; a\nandna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; n\nudhan ---&gt; s\ndhans ---&gt; h\nhansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; u\n.paru ---&gt; .\n..... ---&gt; s\n....s ---&gt; c\n...sc ---&gt; h\n..sch ---&gt; i\n.schi ---&gt; n\nschin ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; p\n.gulp ---&gt; a\ngulpa ---&gt; c\nulpac ---&gt; h\nlpach ---&gt; a\npacha ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; i\n.mehi ---&gt; n\nmehin ---&gt; d\nehind ---&gt; e\nhinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; i\n.arsi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; m\n..alm ---&gt; a\n.alma ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; a\n..jya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; y\nvidhy ---&gt; a\nidhya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; g\n.kang ---&gt; a\nkanga ---&gt; n\nangan ---&gt; a\nngana ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; r\n..hor ---&gt; a\n.hora ---&gt; m\nhoram ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; n\nyogen ---&gt; d\nogend ---&gt; r\ngendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; f\n..naf ---&gt; e\n.nafe ---&gt; e\nnafee ---&gt; s\nafees ---&gt; a\nfeesa ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; a\nkrisa ---&gt; n\nrisan ---&gt; a\nisana ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; o\n.jaso ---&gt; d\njasod ---&gt; a\nasoda ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; l\nshivl ---&gt; a\nhivla ---&gt; l\nivlal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; n\nmahin ---&gt; d\nahind ---&gt; e\nhinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; y\n..rey ---&gt; a\n.reya ---&gt; z\nreyaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; n\nndern ---&gt; a\nderna ---&gt; t\nernat ---&gt; h\nrnath ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; l\nsantl ---&gt; a\nantla ---&gt; l\nntlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; i\n.suni ---&gt; t\nsunit ---&gt; h\nunith ---&gt; a\nnitha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; r\n.char ---&gt; u\ncharu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; .\n..... ---&gt; r\n....r ---&gt; m\n...rm ---&gt; a\n..rma ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; w\n..law ---&gt; r\n.lawr ---&gt; e\nlawre ---&gt; n\nawren ---&gt; c\nwrenc ---&gt; e\nrence ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; a\naksha ---&gt; t\nkshat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; g\n.surg ---&gt; a\nsurga ---&gt; y\nurgay ---&gt; a\nrgaya ---&gt; n\ngayan ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; v\n..kev ---&gt; a\n.keva ---&gt; l\nkeval ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; a\nsabra ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; y\n.isty ---&gt; a\nistya ---&gt; k\nstyak ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; k\nprink ---&gt; a\nrinka ---&gt; y\ninkay ---&gt; a\nnkaya ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; k\n.perk ---&gt; a\nperka ---&gt; s\nerkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; i\nsabri ---&gt; n\nabrin ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; i\n.muki ---&gt; m\nmukim ---&gt; a\nukima ---&gt; n\nkiman ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; o\n.jayo ---&gt; t\njayot ---&gt; l\nayotl ---&gt; i\nyotli ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; i\nparmi ---&gt; t\narmit ---&gt; a\nrmita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; o\n.chho ---&gt; t\nchhot ---&gt; i\nhhoti ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; e\n.vipe ---&gt; n\nvipen ---&gt; d\nipend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; a\n.nosa ---&gt; d\nnosad ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; u\n.nanu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; s\nsures ---&gt; h\nuresh ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; a\nanura ---&gt; g\nnurag ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; a\n.nila ---&gt; m\nnilam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; m\nantrm ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; n\nsawan ---&gt; a\nawana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; r\nohanr ---&gt; a\nhanra ---&gt; m\nanram ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; d\n.vaid ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; v\n.shav ---&gt; i\nshavi ---&gt; t\nhavit ---&gt; r\navitr ---&gt; i\nvitri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; s\n.sars ---&gt; w\nsarsw ---&gt; a\narswa ---&gt; t\nrswat ---&gt; i\nswati ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; m\n..arm ---&gt; i\n.armi ---&gt; t\narmit ---&gt; a\nrmita ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; i\ndhani ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; u\n.nilu ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; n\n..nin ---&gt; i\n.nini ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; l\n.tril ---&gt; o\ntrilo ---&gt; c\nriloc ---&gt; k\nilock ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; m\nsudam ---&gt; a\nudama ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; e\nshahe ---&gt; e\nhahee ---&gt; n\naheen ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; b\n.ajab ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; h\n.nosh ---&gt; a\nnosha ---&gt; d\noshad ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; t\n.sult ---&gt; a\nsulta ---&gt; n\nultan ---&gt; a\nltana ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; u\nbhanu ---&gt; m\nhanum ---&gt; a\nanuma ---&gt; t\nnumat ---&gt; i\numati ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; m\n.simm ---&gt; y\nsimmy ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; n\n.mehn ---&gt; a\nmehna ---&gt; z\nehnaz ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; t\n.lalt ---&gt; h\nlalth ---&gt; a\naltha ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; y\n.uday ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; u\n.riju ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; b\nlilab ---&gt; a\nilaba ---&gt; i\nlabai ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; i\n..moi ---&gt; n\n.moin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; l\nusuml ---&gt; a\nsumla ---&gt; t\numlat ---&gt; a\nmlata ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; i\n.nagi ---&gt; n\nnagin ---&gt; a\nagina ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; i\nnarai ---&gt; n\narain ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; j\n.anoj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; e\nrajee ---&gt; n\najeen ---&gt; a\njeena ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; p\ndhirp ---&gt; a\nhirpa ---&gt; l\nirpal ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; r\n.fakr ---&gt; u\nfakru ---&gt; d\nakrud ---&gt; e\nkrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; t\n.mukt ---&gt; i\nmukti ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; n\n.been ---&gt; a\nbeena ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; d\n..kud ---&gt; t\n.kudt ---&gt; u\nkudtu ---&gt; s\nudtus ---&gt; u\ndtusu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; z\n..taz ---&gt; a\n.taza ---&gt; u\ntazau ---&gt; d\nazaud ---&gt; e\nzaude ---&gt; e\naudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; a\nharda ---&gt; s\nardas ---&gt; s\nrdass ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; a\nompra ---&gt; k\nmprak ---&gt; a\npraka ---&gt; s\nrakas ---&gt; h\nakash ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; s\n..tus ---&gt; h\n.tush ---&gt; a\ntusha ---&gt; r\nushar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; n\nsuhan ---&gt; a\nuhana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; a\nrinka ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; s\n.anus ---&gt; h\nanush ---&gt; k\nnushk ---&gt; a\nushka ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; i\nnandi ---&gt; n\nandin ---&gt; i\nndini ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; j\n.gulj ---&gt; a\ngulja ---&gt; n\nuljan ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; a\n.bara ---&gt; k\nbarak ---&gt; h\narakh ---&gt; a\nrakha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; l\n.raml ---&gt; a\nramla ---&gt; l\namlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; r\nharmr ---&gt; a\narmra ---&gt; j\nrmraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; l\nmanal ---&gt; i\nanali ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; r\n..pir ---&gt; m\n.pirm ---&gt; i\npirmi ---&gt; k\nirmik ---&gt; a\nrmika ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; j\nhanaj ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; w\n..diw ---&gt; a\n.diwa ---&gt; n\ndiwan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; u\n.anju ---&gt; m\nanjum ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; i\n.jari ---&gt; m\njarim ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; p\n.trip ---&gt; t\ntript ---&gt; a\nripta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; t\n.satt ---&gt; u\nsattu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; y\n.prey ---&gt; o\npreyo ---&gt; j\nreyoj ---&gt; i\neyoji ---&gt; t\nyojit ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; u\n.kaju ---&gt; l\nkajul ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; d\nbabud ---&gt; d\nabudd ---&gt; i\nbuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; s\n.gees ---&gt; a\ngeesa ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; a\n.kesa ---&gt; v\nkesav ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; e\nsunee ---&gt; t\nuneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; a\n.pura ---&gt; n\npuran ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; m\n.kism ---&gt; t\nkismt ---&gt; i\nismti ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; i\n.kali ---&gt; p\nkalip ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; u\nanchu ---&gt; r\nnchur ---&gt; a\nchura ---&gt; m\nhuram ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; a\n.jita ---&gt; n\njitan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; a\nvidha ---&gt; v\nidhav ---&gt; a\ndhava ---&gt; t\nhavat ---&gt; i\navati ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; i\nparti ---&gt; m\nartim ---&gt; a\nrtima ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; l\n..val ---&gt; e\n.vale ---&gt; n\nvalen ---&gt; t\nalent ---&gt; i\nlenti ---&gt; n\nentin ---&gt; a\nntina ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; k\n.kank ---&gt; i\nkanki ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; l\n.bhal ---&gt; a\nbhala ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; n\n.poon ---&gt; a\npoona ---&gt; m\noonam ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; v\n.mahv ---&gt; e\nmahve ---&gt; e\nahvee ---&gt; r\nhveer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; n\nranjn ---&gt; i\nanjni ---&gt; k\nnjnik ---&gt; a\njnika ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; a\n.juna ---&gt; i\njunai ---&gt; d\nunaid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; k\nsarik ---&gt; a\narika ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; a\nharda ---&gt; m\nardam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; r\nkalur ---&gt; a\nalura ---&gt; m\nluram ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; n\nishan ---&gt; u\nshanu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; i\n.baki ---&gt; l\nbakil ---&gt; a\nakila ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; t\n..pit ---&gt; a\n.pita ---&gt; r\npitar ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; t\nepant ---&gt; i\npanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; a\nshoka ---&gt; t\nhokat ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; o\n..gro ---&gt; t\n.grot ---&gt; i\ngroti ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; j\n.rubj ---&gt; a\nrubja ---&gt; a\nubjaa ---&gt; n\nbjaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; u\n.sonu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; i\nhashi ---&gt; d\nashid ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; m\n.anim ---&gt; e\nanime ---&gt; s\nnimes ---&gt; h\nimesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; h\nmansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; n\n.jamn ---&gt; a\njamna ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; a\n.baha ---&gt; r\nbahar ---&gt; t\nahart ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; s\n.taps ---&gt; y\ntapsy ---&gt; u\napsyu ---&gt; m\npsyum ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; a\n.tapa ---&gt; n\ntapan ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; a\n.sika ---&gt; n\nsikan ---&gt; d\nikand ---&gt; a\nkanda ---&gt; r\nandar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; m\nsushm ---&gt; a\nushma ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; v\n..siv ---&gt; a\n.siva ---&gt; n\nsivan ---&gt; i\nivani ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; k\nsonak ---&gt; i\nonaki ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; u\n.taju ---&gt; d\ntajud ---&gt; d\najudd ---&gt; i\njuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; y\n.doly ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; n\nubhan ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; j\n.amij ---&gt; a\namija ---&gt; n\nmijan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; b\n.manb ---&gt; h\nmanbh ---&gt; a\nanbha ---&gt; r\nnbhar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; a\nmuska ---&gt; r\nuskar ---&gt; n\nskarn ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; t\nlalit ---&gt; a\nalita ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; h\n..arh ---&gt; i\n.arhi ---&gt; y\narhiy ---&gt; a\nrhiya ---&gt; n\nhiyan ---&gt; t\niyant ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; u\n..amu ---&gt; d\n.amud ---&gt; a\namuda ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; l\n..aal ---&gt; i\n.aali ---&gt; n\naalin ---&gt; a\nalina ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; t\n.kunt ---&gt; i\nkunti ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; j\n..amj ---&gt; a\n.amja ---&gt; d\namjad ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; d\n.swed ---&gt; e\nswede ---&gt; s\nwedes ---&gt; h\nedesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; e\n.loke ---&gt; n\nloken ---&gt; d\nokend ---&gt; e\nkende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; h\nshadh ---&gt; n\nhadhn ---&gt; a\nadhna ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; a\nausha ---&gt; l\nushal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; y\n.nary ---&gt; a\nnarya ---&gt; n\naryan ---&gt; i\nryani ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; q\n..yaq ---&gt; u\n.yaqu ---&gt; b\nyaqub ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; d\n.mind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; b\n..alb ---&gt; a\n.alba ---&gt; k\nalbak ---&gt; s\nlbaks ---&gt; h\nbaksh ---&gt; a\naksha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; a\n.prya ---&gt; n\npryan ---&gt; i\nryani ---&gt; k\nyanik ---&gt; a\nanika ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; m\n..dhm ---&gt; e\n.dhme ---&gt; n\ndhmen ---&gt; d\nhmend ---&gt; a\nmenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; i\n.aari ---&gt; f\naarif ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; t\nnavit ---&gt; a\navita ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; k\n.pank ---&gt; a\npanka ---&gt; j\nankaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; a\nnirma ---&gt; l\nirmal ---&gt; a\nrmala ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; v\n..sev ---&gt; a\n.seva ---&gt; k\nsevak ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; e\nkamre ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; m\n..rum ---&gt; e\n.rume ---&gt; e\nrumee ---&gt; t\numeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; u\n.salu ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; u\n..ahu ---&gt; p\n.ahup ---&gt; e\nahupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; i\n.kami ---&gt; l\nkamil ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; h\n.nikh ---&gt; a\nnikha ---&gt; t\nikhat ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; y\n..jiy ---&gt; a\n.jiya ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; u\n.muku ---&gt; l\nmukul ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; u\nnishu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; a\nparma ---&gt; n\narman ---&gt; a\nrmana ---&gt; n\nmanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; t\n....t ---&gt; h\n...th ---&gt; a\n..tha ---&gt; k\n.thak ---&gt; u\nthaku ---&gt; r\nhakur ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; t\nchott ---&gt; i\nhotti ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; a\n.muka ---&gt; r\nmukar ---&gt; a\nukara ---&gt; m\nkaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; a\n.kosa ---&gt; l\nkosal ---&gt; e\nosale ---&gt; y\nsaley ---&gt; a\naleya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; c\nprinc ---&gt; e\nrince ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; f\n..gaf ---&gt; f\n.gaff ---&gt; u\ngaffu ---&gt; r\naffur ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; i\nrabhi ---&gt; l\nabhil ---&gt; a\nbhila ---&gt; .\n..... ---&gt; z\n....z ---&gt; h\n...zh ---&gt; i\n..zhi ---&gt; n\n.zhin ---&gt; i\nzhini ---&gt; .\n..... ---&gt; i\n....i ---&gt; e\n...ie ---&gt; s\n..ies ---&gt; h\n.iesh ---&gt; a\niesha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; c\n.bacc ---&gt; h\nbacch ---&gt; a\naccha ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; r\n.hazr ---&gt; a\nhazra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; m\n.susm ---&gt; a\nsusma ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; r\nvikar ---&gt; a\nikara ---&gt; m\nkaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; n\nhakun ---&gt; a\nakuna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; i\n.jani ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; y\n.vidy ---&gt; a\nvidya ---&gt; d\nidyad ---&gt; a\ndyada ---&gt; t\nyadat ---&gt; t\nadatt ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; l\n.raml ---&gt; a\nramla ---&gt; k\namlak ---&gt; h\nmlakh ---&gt; a\nlakha ---&gt; n\nakhan ---&gt; .\n..... ---&gt; o\n....o ---&gt; l\n...ol ---&gt; i\n..oli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; i\n.ragi ---&gt; b\nragib ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; n\n.jhan ---&gt; s\njhans ---&gt; i\nhansi ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; r\n..zar ---&gt; e\n.zare ---&gt; e\nzaree ---&gt; n\nareen ---&gt; a\nreena ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; e\nmanee ---&gt; s\nanees ---&gt; h\nneesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; a\n.yasa ---&gt; r\nyasar ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; r\n..jor ---&gt; j\n.jorj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; t\n.rant ---&gt; e\nrante ---&gt; s\nantes ---&gt; h\nntesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; a\n.ajma ---&gt; l\najmal ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; v\namarv ---&gt; e\nmarve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; a\n.mena ---&gt; d\nmenad ---&gt; e\nenade ---&gt; v\nnadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; m\nisham ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; r\n.supr ---&gt; i\nsupri ---&gt; y\nupriy ---&gt; a\npriya ---&gt; l\nriyal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; g\n..mag ---&gt; e\n.mage ---&gt; r\nmager ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; h\n.rabh ---&gt; i\nrabhi ---&gt; d\nabhid ---&gt; a\nbhida ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; l\nprabl ---&gt; e\nrable ---&gt; e\nablee ---&gt; n\nbleen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; k\nubhak ---&gt; a\nbhaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; w\ndhanw ---&gt; a\nhanwa ---&gt; t\nanwat ---&gt; i\nnwati ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; i\n.vasi ---&gt; d\nvasid ---&gt; a\nasida ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; k\n..kok ---&gt; i\n.koki ---&gt; l\nkokil ---&gt; a\nokila ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; i\n.kami ---&gt; n\nkamin ---&gt; i\namini ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; s\ndevas ---&gt; h\nevash ---&gt; e\nvashe ---&gt; e\nashee ---&gt; h\nsheeh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; u\n.shou ---&gt; k\nshouk ---&gt; a\nhouka ---&gt; t\noukat ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; s\n..las ---&gt; a\n.lasa ---&gt; r\nlasar ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; a\n.nila ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; s\n.hans ---&gt; a\nhansa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; c\n.nanc ---&gt; y\nnancy ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; u\n.maju ---&gt; n\nmajun ---&gt; e\najune ---&gt; w\njunew ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; a\nshoka ---&gt; r\nhokar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; i\nhardi ---&gt; k\nardik ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; k\nriyak ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; i\n.fari ---&gt; d\nfarid ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; a\n.baha ---&gt; l\nbahal ---&gt; e\nahale ---&gt; n\nhalen ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; n\nharin ---&gt; d\narind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; w\ntaraw ---&gt; a\narawa ---&gt; t\nrawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; f\naseef ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; m\n.khem ---&gt; r\nkhemr ---&gt; a\nhemra ---&gt; j\nemraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; w\nhashw ---&gt; a\nashwa ---&gt; t\nshwat ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; g\n..asg ---&gt; a\n.asga ---&gt; r\nasgar ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; m\n..mem ---&gt; a\n.mema ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; a\nsheea ---&gt; k\nheeak ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; a\npenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; z\nulnaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; j\n..inj ---&gt; u\n.inju ---&gt; m\ninjum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; a\n.naja ---&gt; r\nnajar ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; e\nnoore ---&gt; e\nooree ---&gt; n\noreen ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; y\n..ily ---&gt; a\n.ilya ---&gt; s\nilyas ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; m\n.alim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; i\n.jasi ---&gt; n\njasin ---&gt; a\nasina ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; y\ngoldy ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; s\ntanus ---&gt; h\nanush ---&gt; r\nnushr ---&gt; i\nushri ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; a\n.nisa ---&gt; r\nnisar ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; n\nanjan ---&gt; i\nnjani ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; v\n.janv ---&gt; i\njanvi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; p\n..gap ---&gt; p\n.gapp ---&gt; u\ngappu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; e\nharde ---&gt; v\nardev ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; e\nsabee ---&gt; n\nabeen ---&gt; a\nbeena ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; a\naroja ---&gt; n\nrojan ---&gt; i\nojani ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; t\n..imt ---&gt; i\n.imti ---&gt; a\nimtia ---&gt; z\nmtiaz ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; p\n.ganp ---&gt; a\nganpa ---&gt; t\nanpat ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; j\nfoolj ---&gt; h\nooljh ---&gt; n\noljhn ---&gt; a\nljhna ---&gt; h\njhnah ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; t\nchatt ---&gt; e\nhatte ---&gt; r\natter ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; k\nmehak ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; k\n.jask ---&gt; i\njaski ---&gt; r\naskir ---&gt; a\nskira ---&gt; t\nkirat ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; n\n.jann ---&gt; a\njanna ---&gt; t\nannat ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; t\n.mitt ---&gt; o\nmitto ---&gt; p\nittop ---&gt; u\nttopu ---&gt; r\ntopur ---&gt; i\nopuri ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; i\n.anki ---&gt; t\nankit ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; u\nramsu ---&gt; r\namsur ---&gt; a\nmsura ---&gt; t\nsurat ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; a\n.dola ---&gt; t\ndolat ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; j\n.navj ---&gt; a\nnavja ---&gt; t\navjat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; s\n.suhs ---&gt; m\nsuhsm ---&gt; i\nuhsmi ---&gt; t\nhsmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; m\n.himm ---&gt; a\nhimma ---&gt; t\nimmat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; z\n.mahz ---&gt; b\nmahzb ---&gt; i\nahzbi ---&gt; n\nhzbin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; d\n..and ---&gt; h\n.andh ---&gt; a\nandha ---&gt; v\nndhav ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; o\n.asho ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; d\nmahad ---&gt; e\nahade ---&gt; v\nhadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; o\n..jyo ---&gt; t\n.jyot ---&gt; s\njyots ---&gt; a\nyotsa ---&gt; n\notsan ---&gt; a\ntsana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; m\n.balm ---&gt; i\nbalmi ---&gt; k\nalmik ---&gt; i\nlmiki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; n\nraman ---&gt; a\namana ---&gt; n\nmanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; e\nrinke ---&gt; .\n..... ---&gt; c\n....c ---&gt; o\n...co ---&gt; n\n..con ---&gt; s\n.cons ---&gt; t\nconst ---&gt; a\nonsta ---&gt; b\nnstab ---&gt; l\nstabl ---&gt; e\ntable ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; t\n..nut ---&gt; a\n.nuta ---&gt; n\nnutan ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; r\n..imr ---&gt; a\n.imra ---&gt; n\nimran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; b\nsarab ---&gt; j\narabj ---&gt; i\nrabji ---&gt; t\nabjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; w\nsahaw ---&gt; a\nahawa ---&gt; j\nhawaj ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; b\n..jab ---&gt; i\n.jabi ---&gt; r\njabir ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; a\n.akha ---&gt; l\nakhal ---&gt; a\nkhala ---&gt; k\nhalak ---&gt; .\n..... ---&gt; m\n....m ---&gt; n\n...mn ---&gt; o\n..mno ---&gt; j\n.mnoj ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; a\npuspa ---&gt; k\nuspak ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; l\n..zil ---&gt; e\n.zile ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; k\n.halk ---&gt; e\nhalke ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; i\n..tai ---&gt; t\n.tait ---&gt; r\ntaitr ---&gt; a\naitra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; i\nkamli ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; e\nbhude ---&gt; v\nhudev ---&gt; i\nudevi ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; z\n.azaz ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; d\n.baid ---&gt; n\nbaidn ---&gt; a\naidna ---&gt; t\nidnat ---&gt; h\ndnath ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; r\n..iqr ---&gt; a\n.iqra ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; b\n..abb ---&gt; a\n.abba ---&gt; s\nabbas ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; e\nsunee ---&gt; l\nuneel ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; a\n..apa ---&gt; r\n.apar ---&gt; n\naparn ---&gt; a\nparna ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; k\n.rank ---&gt; i\nranki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; d\nashid ---&gt; a\nshida ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; n\n.tamn ---&gt; n\ntamnn ---&gt; a\namnna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; j\narhaj ---&gt; a\nrhaja ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; w\ngyanw ---&gt; a\nyanwa ---&gt; t\nanwat ---&gt; i\nnwati ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; u\nmanju ---&gt; l\nanjul ---&gt; a\nnjula ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; v\n.tejv ---&gt; i\ntejvi ---&gt; r\nejvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; k\nsunak ---&gt; i\nunaki ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; k\nsonik ---&gt; a\nonika ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; n\n..avn ---&gt; e\n.avne ---&gt; e\navnee ---&gt; t\nvneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; a\n.sada ---&gt; b\nsadab ---&gt; r\nadabr ---&gt; i\ndabri ---&gt; j\nabrij ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; e\n..rae ---&gt; e\n.raee ---&gt; y\nraeey ---&gt; a\naeeya ---&gt; n\neeyan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; d\n..mid ---&gt; h\n.midh ---&gt; a\nmidha ---&gt; n\nidhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; j\n.marj ---&gt; a\nmarja ---&gt; n\narjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; w\n.mehw ---&gt; i\nmehwi ---&gt; s\nehwis ---&gt; h\nhwish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; i\n.kasi ---&gt; m\nkasim ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; e\nsarve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; j\n.sayj ---&gt; a\nsayja ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; i\n.dili ---&gt; s\ndilis ---&gt; h\nilish ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; a\n.mama ---&gt; t\nmamat ---&gt; a\namata ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; r\nharpr ---&gt; a\narpra ---&gt; s\nrpras ---&gt; a\nprasa ---&gt; d\nrasad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; e\nparee ---&gt; t\nareet ---&gt; i\nreeti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; o\nmanno ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; v\nsatyv ---&gt; r\natyvr ---&gt; a\ntyvra ---&gt; t\nyvrat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; o\n.samo ---&gt; t\nsamot ---&gt; i\namoti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; m\nmohmm ---&gt; d\nohmmd ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; e\nradhe ---&gt; y\nadhey ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; i\n.nahi ---&gt; d\nnahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; a\n.inda ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; n\nchunn ---&gt; u\nhunnu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; e\nbadre ---&gt; a\nadrea ---&gt; l\ndreal ---&gt; a\nreala ---&gt; m\nealam ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; v\n.durv ---&gt; e\ndurve ---&gt; s\nurves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; j\n.barj ---&gt; r\nbarjr ---&gt; a\narjra ---&gt; j\nrjraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; m\n.munm ---&gt; u\nmunmu ---&gt; n\nunmun ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; a\n.bata ---&gt; s\nbatas ---&gt; i\natasi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; a\n.suka ---&gt; r\nsukar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; k\nparik ---&gt; s\nariks ---&gt; h\nriksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; b\n.babb ---&gt; i\nbabbi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; a\n.bana ---&gt; r\nbanar ---&gt; s\nanars ---&gt; i\nnarsi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; k\n..jak ---&gt; r\n.jakr ---&gt; a\njakra ---&gt; .\n..... ---&gt; i\n....i ---&gt; a\n...ia ---&gt; r\n..iar ---&gt; f\n.iarf ---&gt; a\niarfa ---&gt; n\narfan ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; v\n..urv ---&gt; a\n.urva ---&gt; r\nurvar ---&gt; s\nrvars ---&gt; h\nvarsh ---&gt; i\narshi ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; l\n.mitl ---&gt; e\nmitle ---&gt; s\nitles ---&gt; h\ntlesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; u\nsamsu ---&gt; d\namsud ---&gt; i\nmsudi ---&gt; n\nsudin ---&gt; .\n..... ---&gt; r\n....r ---&gt; g\n...rg ---&gt; h\n..rgh ---&gt; u\n.rghu ---&gt; r\nrghur ---&gt; a\nghura ---&gt; j\nhuraj ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; t\n..yat ---&gt; i\n.yati ---&gt; n\nyatin ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; i\nrishi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; a\nvanda ---&gt; n\nandan ---&gt; a\nndana ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; d\n..bid ---&gt; a\n.bida ---&gt; m\nbidam ---&gt; i\nidami ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; p\n.ganp ---&gt; a\nganpa ---&gt; t\nanpat ---&gt; r\nnpatr ---&gt; a\npatra ---&gt; m\natram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; i\nparti ---&gt; k\nartik ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; h\n.sobh ---&gt; a\nsobha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; u\nhishu ---&gt; l\nishul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; r\nsagir ---&gt; a\nagira ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; f\n..taf ---&gt; s\n.tafs ---&gt; i\ntafsi ---&gt; r\nafsir ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; k\n.bark ---&gt; h\nbarkh ---&gt; a\narkha ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; d\nlshad ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; y\nlakhy ---&gt; a\nakhya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; a\npalla ---&gt; v\nallav ---&gt; i\nllavi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; y\nanjey ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; e\n.late ---&gt; s\nlates ---&gt; h\natesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; u\n.karu ---&gt; n\nkarun ---&gt; a\naruna ---&gt; k\nrunak ---&gt; a\nunaka ---&gt; r\nnakar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; e\nharde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; u\nmusku ---&gt; r\nuskur ---&gt; a\nskura ---&gt; n\nkuran ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; r\n..war ---&gt; e\n.ware ---&gt; e\nwaree ---&gt; s\narees ---&gt; h\nreesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; u\nbholu ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; a\n.sila ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; r\n..ekr ---&gt; a\n.ekra ---&gt; m\nekram ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; i\n.mili ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; l\ndimpl ---&gt; e\nimple ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; u\nsharu ---&gt; n\nharun ---&gt; a\naruna ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; n\nmahen ---&gt; d\nahend ---&gt; a\nhenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; l\nbabal ---&gt; i\nabali ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; j\n.gurj ---&gt; i\ngurji ---&gt; t\nurjit ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; e\n..upe ---&gt; n\n.upen ---&gt; d\nupend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; d\nshyad ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; l\nrohil ---&gt; a\nohila ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; a\n.tila ---&gt; k\ntilak ---&gt; r\nilakr ---&gt; a\nlakra ---&gt; m\nakram ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; s\nroops ---&gt; i\noopsi ---&gt; n\nopsin ---&gt; g\npsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; h\n..gah ---&gt; n\n.gahn ---&gt; s\ngahns ---&gt; h\nahnsh ---&gt; y\nhnshy ---&gt; a\nnshya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; t\nparat ---&gt; h\narath ---&gt; v\nrathv ---&gt; i\nathvi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; r\nsunar ---&gt; k\nunark ---&gt; a\nnarka ---&gt; l\narkal ---&gt; i\nrkali ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; i\nhaili ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; h\n.bash ---&gt; u\nbashu ---&gt; d\nashud ---&gt; e\nshude ---&gt; v\nhudev ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; a\n.vima ---&gt; l\nvimal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; v\n.kulv ---&gt; i\nkulvi ---&gt; n\nulvin ---&gt; d\nlvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; v\n..gov ---&gt; e\n.gove ---&gt; r\ngover ---&gt; d\noverd ---&gt; h\nverdh ---&gt; a\nerdha ---&gt; n\nrdhan ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; t\n.swet ---&gt; h\nsweth ---&gt; a\nwetha ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; n\n.jeen ---&gt; a\njeena ---&gt; t\neenat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; h\narash ---&gt; r\nrashr ---&gt; a\nashra ---&gt; m\nshram ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; t\nasmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; r\n.sehr ---&gt; a\nsehra ---&gt; n\nehran ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; r\nchitr ---&gt; o\nhitro ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; h\ntulsh ---&gt; i\nulshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; n\n.sohn ---&gt; i\nsohni ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; i\n.vipi ---&gt; v\nvipiv ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; n\n.brin ---&gt; d\nbrind ---&gt; a\nrinda ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; s\n.mais ---&gt; a\nmaisa ---&gt; n\naisan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; p\nsarip ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; b\nshahb ---&gt; o\nhahbo ---&gt; o\nahboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; u\n.vasu ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; a\nrisha ---&gt; w\nishaw ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; u\npallu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; k\n.jask ---&gt; a\njaska ---&gt; r\naskar ---&gt; a\nskara ---&gt; n\nkaran ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; t\n.nitt ---&gt; i\nnitti ---&gt; n\nittin ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; b\n.tarb ---&gt; e\ntarbe ---&gt; z\narbez ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; n\nsavin ---&gt; a\navina ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; s\nvikas ---&gt; h\nikash ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; n\n.nann ---&gt; i\nnanni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; s\n.sais ---&gt; t\nsaist ---&gt; a\naista ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; a\n.hira ---&gt; l\nhiral ---&gt; a\nirala ---&gt; l\nralal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; a\n.shoa ---&gt; b\nshoab ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; u\n..phu ---&gt; l\n.phul ---&gt; k\nphulk ---&gt; a\nhulka ---&gt; n\nulkan ---&gt; a\nlkana ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; m\n..kom ---&gt; a\n.koma ---&gt; l\nkomal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; e\n.shre ---&gt; e\nshree ---&gt; m\nhreem ---&gt; a\nreema ---&gt; t\neemat ---&gt; i\nemati ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; r\nhasir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; a\nshaya ---&gt; m\nhayam ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; u\n..jau ---&gt; l\n.jaul ---&gt; i\njauli ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; d\n..bod ---&gt; h\n.bodh ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; a\nshita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; i\n.sini ---&gt; l\nsinil ---&gt; .\n..... ---&gt; f\n....f ---&gt; e\n...fe ---&gt; f\n..fef ---&gt; l\n.fefl ---&gt; i\nfefli ---&gt; b\neflib ---&gt; a\nfliba ---&gt; i\nlibai ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; g\n.jaig ---&gt; u\njaigu ---&gt; n\naigun ---&gt; a\niguna ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; l\nhimal ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; g\n.prag ---&gt; t\npragt ---&gt; i\nragti ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; j\n.bhaj ---&gt; j\nbhajj ---&gt; u\nhajju ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; t\n.swet ---&gt; n\nswetn ---&gt; a\nwetna ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; a\nresha ---&gt; m\nesham ---&gt; a\nshama ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; e\n.bale ---&gt; s\nbales ---&gt; h\nalesh ---&gt; w\nleshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; i\n.kani ---&gt; s\nkanis ---&gt; k\nanisk ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; g\n..mog ---&gt; l\n.mogl ---&gt; i\nmogli ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; t\n.kant ---&gt; i\nkanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; o\nsanno ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; v\n.silv ---&gt; i\nsilvi ---&gt; a\nilvia ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; u\n.kabu ---&gt; l\nkabul ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; u\n.naju ---&gt; b\nnajub ---&gt; a\najuba ---&gt; i\njubai ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; i\n.sani ---&gt; y\nsaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; a\nchota ---&gt; n\nhotan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; i\n.rubi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; f\nsaraf ---&gt; a\narafa ---&gt; t\nrafat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; k\n.pank ---&gt; u\npanku ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; s\n.abis ---&gt; h\nabish ---&gt; e\nbishe ---&gt; k\nishek ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; n\nrajin ---&gt; d\najind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; o\nmanso ---&gt; o\nansoo ---&gt; r\nnsoor ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; a\n.saga ---&gt; r\nsagar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; e\n.jame ---&gt; e\njamee ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; a\nshara ---&gt; t\nharat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; u\n..rau ---&gt; f\n.rauf ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; g\n.gung ---&gt; u\ngungu ---&gt; n\nungun ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; z\n..roz ---&gt; i\n.rozi ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; y\n..koy ---&gt; a\n.koya ---&gt; l\nkoyal ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; e\ndhane ---&gt; y\nhaney ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; i\nsandi ---&gt; y\nandiy ---&gt; a\nndiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; f\n.julf ---&gt; i\njulfi ---&gt; k\nulfik ---&gt; a\nlfika ---&gt; a\nfikaa ---&gt; r\nikaar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; a\nranja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; n\nsahin ---&gt; a\nahina ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; y\n.deey ---&gt; a\ndeeya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; g\n.narg ---&gt; e\nnarge ---&gt; s\narges ---&gt; h\nrgesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; m\n..mum ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; n\n..ton ---&gt; y\n.tony ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; d\n.gand ---&gt; h\ngandh ---&gt; a\nandha ---&gt; r\nndhar ---&gt; v\ndharv ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; i\nbhavi ---&gt; s\nhavis ---&gt; h\navish ---&gt; y\nvishy ---&gt; a\nishya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; e\n.jane ---&gt; s\njanes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; e\nsunde ---&gt; r\nunder ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; d\nshard ---&gt; h\nhardh ---&gt; a\nardha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; r\n.sher ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; k\n.srik ---&gt; a\nsrika ---&gt; n\nrikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; a\n.naba ---&gt; v\nnabav ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; i\nmangi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; i\n.jami ---&gt; l\njamil ---&gt; a\namila ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; r\n.sukr ---&gt; i\nsukri ---&gt; t\nukrit ---&gt; a\nkrita ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; w\n..niw ---&gt; a\n.niwa ---&gt; s\nniwas ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; k\n.sark ---&gt; a\nsarka ---&gt; r\narkar ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; s\n.alis ---&gt; h\nalish ---&gt; a\nlisha ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; t\n..att ---&gt; a\n.atta ---&gt; r\nattar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; k\n..nak ---&gt; c\n.nakc ---&gt; h\nnakch ---&gt; e\nakche ---&gt; d\nkched ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; p\n.rasp ---&gt; a\nraspa ---&gt; l\naspal ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; i\n.ambi ---&gt; k\nambik ---&gt; a\nmbika ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; g\n..jig ---&gt; a\n.jiga ---&gt; r\njigar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; e\n.sube ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; c\n.succ ---&gt; h\nsucch ---&gt; a\nuccha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; g\n..shg ---&gt; i\n.shgi ---&gt; t\nshgit ---&gt; a\nhgita ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; n\n.pran ---&gt; s\nprans ---&gt; i\nransi ---&gt; s\nansis ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; f\n..naf ---&gt; e\n.nafe ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; b\n.birb ---&gt; a\nbirba ---&gt; l\nirbal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; m\nubham ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; n\nhakin ---&gt; a\nakina ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; u\n..you ---&gt; r\n.your ---&gt; a\nyoura ---&gt; j\nouraj ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; i\n..tai ---&gt; y\n.taiy ---&gt; a\ntaiya ---&gt; b\naiyab ---&gt; .\n..... ---&gt; u\n....u ---&gt; g\n...ug ---&gt; a\n..uga ---&gt; n\n.ugan ---&gt; t\nugant ---&gt; a\nganta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; u\n.saku ---&gt; n\nsakun ---&gt; t\nakunt ---&gt; l\nkuntl ---&gt; a\nuntla ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; m\n.noom ---&gt; i\nnoomi ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; a\n..gra ---&gt; c\n.grac ---&gt; e\ngrace ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; k\n.mahk ---&gt; a\nmahka ---&gt; r\nahkar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; e\narvee ---&gt; n\nrveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; l\nmangl ---&gt; e\nangle ---&gt; s\nngles ---&gt; h\nglesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; k\nprink ---&gt; y\nrinky ---&gt; a\ninkya ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; p\n..nep ---&gt; a\n.nepa ---&gt; l\nnepal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; v\n.gurv ---&gt; i\ngurvi ---&gt; n\nurvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; o\n.sulo ---&gt; c\nsuloc ---&gt; h\nuloch ---&gt; n\nlochn ---&gt; a\nochna ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; a\n.veda ---&gt; n\nvedan ---&gt; a\nedana ---&gt; n\ndanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; n\n.uman ---&gt; a\numana ---&gt; t\nmanat ---&gt; h\nanath ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; l\n..wal ---&gt; i\n.wali ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; s\nrahis ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; k\nepank ---&gt; e\npanke ---&gt; r\nanker ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; n\n.arin ---&gt; d\narind ---&gt; o\nrindo ---&gt; m\nindom ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; j\nmuntj ---&gt; a\nuntja ---&gt; r\nntjar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; u\nsanau ---&gt; l\nanaul ---&gt; l\nnaull ---&gt; a\naulla ---&gt; h\nullah ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; m\n.jasm ---&gt; e\njasme ---&gt; e\nasmee ---&gt; t\nsmeet ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; p\n..kap ---&gt; i\n.kapi ---&gt; i\nkapii ---&gt; l\napiil ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; k\n..ink ---&gt; o\n.inko ---&gt; o\ninkoo ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; e\n.rine ---&gt; s\nrines ---&gt; h\ninesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; e\n.loke ---&gt; n\nloken ---&gt; d\nokend ---&gt; r\nkendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; a\n.jaha ---&gt; g\njahag ---&gt; i\nahagi ---&gt; r\nhagir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; v\n.satv ---&gt; e\nsatve ---&gt; e\natvee ---&gt; r\ntveer ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; u\ndhuru ---&gt; v\nhuruv ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; e\n.some ---&gt; n\nsomen ---&gt; d\nomend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; u\nmithu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; r\n..mor ---&gt; k\n.mork ---&gt; i\nmorki ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; i\n.muni ---&gt; s\nmunis ---&gt; h\nunish ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; o\n..mho ---&gt; o\n.mhoo ---&gt; m\nmhoom ---&gt; a\nhooma ---&gt; d\noomad ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; e\n..ume ---&gt; d\n.umed ---&gt; i\numedi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; m\nakshm ---&gt; i\nkshmi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; e\n.gaje ---&gt; n\ngajen ---&gt; d\najend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; n\n.tann ---&gt; u\ntannu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; a\n.kaja ---&gt; l\nkajal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; l\n.kall ---&gt; u\nkallu ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; i\nmanni ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; e\n.nite ---&gt; s\nnites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; s\nkalas ---&gt; h\nalash ---&gt; i\nlashi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; u\nvishu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; n\nadhun ---&gt; i\ndhuni ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; i\n.chhi ---&gt; t\nchhit ---&gt; a\nhhita ---&gt; r\nhitar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; u\n.mayu ---&gt; r\nmayur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; c\n.sanc ---&gt; h\nsanch ---&gt; i\nanchi ---&gt; t\nnchit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; t\nsujat ---&gt; a\nujata ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; d\n.sehd ---&gt; e\nsehde ---&gt; v\nehdev ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; u\n..dau ---&gt; l\n.daul ---&gt; a\ndaula ---&gt; t\naulat ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; m\n..tum ---&gt; p\n.tump ---&gt; a\ntumpa ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; h\nprash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; s\nshans ---&gt; a\nhansa ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; v\ndhurv ---&gt; e\nhurve ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; l\n..gal ---&gt; i\n.gali ---&gt; y\ngaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; j\nahnaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; u\nnandu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; s\nmahes ---&gt; h\nahesh ---&gt; a\nhesha ---&gt; r\neshar ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; j\neelaj ---&gt; a\nelaja ---&gt; n\nlajan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; t\n..ant ---&gt; i\n.anti ---&gt; m\nantim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; r\n.mair ---&gt; i\nmairi ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; s\n..was ---&gt; e\n.wase ---&gt; e\nwasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; e\nharee ---&gt; s\narees ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; u\n.samu ---&gt; n\nsamun ---&gt; d\namund ---&gt; r\nmundr ---&gt; i\nundri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; f\n.manf ---&gt; o\nmanfo ---&gt; o\nanfoo ---&gt; l\nnfool ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; v\n.shav ---&gt; a\nshava ---&gt; n\nhavan ---&gt; i\navani ---&gt; .\n..... ---&gt; f\n....f ---&gt; h\n...fh ---&gt; a\n..fha ---&gt; r\n.fhar ---&gt; m\nfharm ---&gt; a\nharma ---&gt; n\narman ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; a\n.ajma ---&gt; n\najman ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; m\n..ism ---&gt; a\n.isma ---&gt; l\nismal ---&gt; i\nsmali ---&gt; y\nmaliy ---&gt; e\naliye ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; o\nsanjo ---&gt; g\nanjog ---&gt; t\nnjogt ---&gt; a\njogta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; c\nchanc ---&gt; h\nhanch ---&gt; a\nancha ---&gt; l\nnchal ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; s\n..ins ---&gt; a\n.insa ---&gt; f\ninsaf ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; o\n.anoo ---&gt; p\nanoop ---&gt; a\nnoopa ---&gt; m\noopam ---&gt; a\nopama ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; k\n.rook ---&gt; m\nrookm ---&gt; a\nookma ---&gt; n\nokman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; h\n.kosh ---&gt; a\nkosha ---&gt; l\noshal ---&gt; y\nshaly ---&gt; a\nhalya ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; a\ndilsa ---&gt; d\nilsad ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; a\n.inda ---&gt; l\nindal ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; l\nmithl ---&gt; e\nithle ---&gt; s\nthles ---&gt; h\nhlesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; i\nnazmi ---&gt; n\nazmin ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; b\n.akib ---&gt; .\n..... ---&gt; d\n....d ---&gt; v\n...dv ---&gt; n\n..dvn ---&gt; d\n.dvnd ---&gt; a\ndvnda ---&gt; v\nvndav ---&gt; v\nndavv ---&gt; a\ndavva ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; r\nsunar ---&gt; k\nunark ---&gt; i\nnarki ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; j\n.amrj ---&gt; e\namrje ---&gt; e\nmrjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; p\nshilp ---&gt; a\nhilpa ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; d\n.bund ---&gt; e\nbunde ---&gt; l\nundel ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; n\nandan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; n\nasmin ---&gt; a\nsmina ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; n\n.alin ---&gt; a\nalina ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; s\n..sos ---&gt; r\n.sosr ---&gt; i\nsosri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; a\nvidha ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; z\n..afz ---&gt; a\n.afza ---&gt; l\nafzal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; u\nkamru ---&gt; d\namrud ---&gt; e\nmrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; k\nshauk ---&gt; a\nhauka ---&gt; t\naukat ---&gt; .\n..... ---&gt; t\n....t ---&gt; h\n...th ---&gt; o\n..tho ---&gt; m\n.thom ---&gt; a\nthoma ---&gt; s\nhomas ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; k\nmustk ---&gt; i\nustki ---&gt; m\nstkim ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; n\nijayn ---&gt; t\njaynt ---&gt; i\naynti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; y\nsadiy ---&gt; a\nadiya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; o\nmadho ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; i\n.hali ---&gt; m\nhalim ---&gt; a\nalima ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; z\n.shaz ---&gt; a\nshaza ---&gt; d\nhazad ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; y\nshivy ---&gt; a\nhivya ---&gt; l\nivyal ---&gt; a\nvyala ---&gt; m\nyalam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; i\nkarmi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; e\nchate ---&gt; r\nhater ---&gt; p\naterp ---&gt; a\nterpa ---&gt; l\nerpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; d\n..rud ---&gt; m\n.rudm ---&gt; a\nrudma ---&gt; l\nudmal ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; e\n.dave ---&gt; e\ndavee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; m\nkushm ---&gt; a\nushma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; e\n.nade ---&gt; m\nnadem ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; s\njites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; k\njanak ---&gt; i\nanaki ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; i\n.simi ---&gt; l\nsimil ---&gt; a\nimila ---&gt; t\nmilat ---&gt; a\nilata ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; y\n.tany ---&gt; a\ntanya ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; s\n.ayus ---&gt; h\nayush ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; e\n..ile ---&gt; s\n.iles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; a\nashra ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; r\n.neer ---&gt; u\nneeru ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; k\nrupak ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; s\nrajis ---&gt; h\najish ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; m\n..arm ---&gt; a\n.arma ---&gt; n\narman ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; l\n.bhul ---&gt; a\nbhula ---&gt; n\nhulan ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; x\neenax ---&gt; i\nenaxi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; a\n.mona ---&gt; m\nmonam ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; r\n..tir ---&gt; a\n.tira ---&gt; t\ntirat ---&gt; h\nirath ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; e\n.fare ---&gt; e\nfaree ---&gt; n\nareen ---&gt; a\nreena ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; r\nhushr ---&gt; e\nushre ---&gt; e\nshree ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; n\nrupen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; h\n.sauh ---&gt; a\nsauha ---&gt; l\nauhal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; i\nsabbi ---&gt; r\nabbir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; s\n.saks ---&gt; h\nsaksh ---&gt; a\naksha ---&gt; m\nksham ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; k\n..prk ---&gt; e\n.prke ---&gt; s\nprkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; a\n..upa ---&gt; s\n.upas ---&gt; a\nupasa ---&gt; n\npasan ---&gt; a\nasana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; s\n.jais ---&gt; i\njaisi ---&gt; g\naisig ---&gt; h\nisigh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; t\n.mast ---&gt; e\nmaste ---&gt; r\naster ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; i\nshadi ---&gt; k\nhadik ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; r\nshimr ---&gt; a\nhimra ---&gt; n\nimran ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; l\n.akil ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; r\n.poor ---&gt; a\npoora ---&gt; n\nooran ---&gt; m\noranm ---&gt; a\nranma ---&gt; l\nanmal ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; o\n..mho ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; v\ndharv ---&gt; e\nharve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; n\n..jin ---&gt; a\n.jina ---&gt; t\njinat ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; s\ndeves ---&gt; h\nevesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; f\n.asif ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; l\n.babl ---&gt; i\nbabli ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; v\n.triv ---&gt; e\ntrive ---&gt; n\nriven ---&gt; i\niveni ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; i\nkundi ---&gt; n\nundin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; r\n..kur ---&gt; e\n.kure ---&gt; j\nkurej ---&gt; a\nureja ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; e\n.saie ---&gt; s\nsaies ---&gt; t\naiest ---&gt; a\niesta ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; t\n.bant ---&gt; u\nbantu ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; n\nmonin ---&gt; i\nonini ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; n\nshann ---&gt; o\nhanno ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; p\nshrip ---&gt; a\nhripa ---&gt; l\nripal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; l\n.babl ---&gt; o\nbablo ---&gt; o\nabloo ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; i\n..gri ---&gt; s\n.gris ---&gt; h\ngrish ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; u\nanuru ---&gt; d\nnurud ---&gt; h\nurudh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; r\n.mehr ---&gt; u\nmehru ---&gt; d\nehrud ---&gt; d\nhrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; h\n.kosh ---&gt; a\nkosha ---&gt; l\noshal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; b\n.shub ---&gt; h\nshubh ---&gt; m\nhubhm ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; e\n.ompe ---&gt; r\nomper ---&gt; k\nmperk ---&gt; a\nperka ---&gt; s\nerkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; p\natyap ---&gt; a\ntyapa ---&gt; l\nyapal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; a\n.mata ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; p\nchitp ---&gt; r\nhitpr ---&gt; i\nitpri ---&gt; t\ntprit ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; h\n..leh ---&gt; r\n.lehr ---&gt; u\nlehru ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; e\n.ajme ---&gt; r\najmer ---&gt; i\njmeri ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; n\n.gurn ---&gt; a\ngurna ---&gt; m\nurnam ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; h\n..vih ---&gt; s\n.vihs ---&gt; a\nvihsa ---&gt; l\nihsal ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; t\n..not ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; m\nkarim ---&gt; a\narima ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; e\nbhise ---&gt; k\nhisek ---&gt; h\nisekh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; h\namesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; s\naakas ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; s\n..ais ---&gt; h\n.aish ---&gt; w\naishw ---&gt; a\nishwa ---&gt; r\nshwar ---&gt; y\nhwary ---&gt; a\nwarya ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; a\n.aana ---&gt; d\naanad ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; k\n.anik ---&gt; t\nanikt ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; w\n..jaw ---&gt; a\n.jawa ---&gt; h\njawah ---&gt; a\nawaha ---&gt; r\nwahar ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; l\n.akhl ---&gt; e\nakhle ---&gt; s\nkhles ---&gt; h\nhlesh ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; i\nvidhi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; a\n.basa ---&gt; n\nbasan ---&gt; t\nasant ---&gt; a\nsanta ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; s\ntanis ---&gt; h\nanish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; n\n.sugn ---&gt; a\nsugna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; z\n.saiz ---&gt; a\nsaiza ---&gt; d\naizad ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; e\nprite ---&gt; e\nritee ---&gt; .\n..... ---&gt; m\n....m ---&gt; m\n...mm ---&gt; t\n..mmt ---&gt; a\n.mmta ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; b\n..lab ---&gt; b\n.labb ---&gt; h\nlabbh ---&gt; i\nabbhi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; b\n.nirb ---&gt; h\nnirbh ---&gt; a\nirbha ---&gt; y\nrbhay ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; a\nrakha ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; h\n.rith ---&gt; i\nrithi ---&gt; k\nithik ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; s\n..tos ---&gt; i\n.tosi ---&gt; f\ntosif ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; s\nashis ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; h\n.nadh ---&gt; i\nnadhi ---&gt; m\nadhim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; a\n.nana ---&gt; g\nnanag ---&gt; r\nanagr ---&gt; a\nnagra ---&gt; m\nagram ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; i\nsurai ---&gt; y\nuraiy ---&gt; a\nraiya ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; m\nulfam ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; k\n.munk ---&gt; a\nmunka ---&gt; d\nunkad ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; n\namarn ---&gt; a\nmarna ---&gt; t\narnat ---&gt; h\nrnath ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; o\nhhano ---&gt; y\nhanoy ---&gt; a\nanoya ---&gt; r\nnoyar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; w\nhanaw ---&gt; a\nanawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; n\n.teen ---&gt; a\nteena ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; i\nsamsi ---&gt; d\namsid ---&gt; a\nmsida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; n\n.satn ---&gt; a\nsatna ---&gt; m\natnam ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; z\nfiroz ---&gt; a\niroza ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; a\nrinka ---&gt; l\ninkal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; i\nramki ---&gt; s\namkis ---&gt; h\nmkish ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; o\n.kiro ---&gt; r\nkiror ---&gt; i\nirori ---&gt; m\nrorim ---&gt; a\norima ---&gt; l\nrimal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; u\njeetu ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; e\n.fate ---&gt; h\nfateh ---&gt; .\n..... ---&gt; u\n....u ---&gt; s\n...us ---&gt; h\n..ush ---&gt; a\n.usha ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; r\n.meer ---&gt; a\nmeera ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; a\nmaria ---&gt; m\nariam ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; r\n..ser ---&gt; u\n.seru ---&gt; l\nserul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; e\nsarde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; r\nandar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; i\n.muji ---&gt; b\nmujib ---&gt; u\nujibu ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; y\n..soy ---&gt; a\n.soya ---&gt; b\nsoyab ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; p\n..hap ---&gt; p\n.happ ---&gt; y\nhappy ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; s\n.murs ---&gt; i\nmursi ---&gt; d\nursid ---&gt; a\nrsida ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; u\n.ritu ---&gt; r\nritur ---&gt; a\nitura ---&gt; j\nturaj ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; n\n..jen ---&gt; n\n.jenn ---&gt; y\njenny ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; s\ngulas ---&gt; h\nulash ---&gt; a\nlasha ---&gt; n\nashan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; w\n.sahw ---&gt; a\nsahwa ---&gt; j\nahwaj ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; e\nprate ---&gt; e\nratee ---&gt; k\nateek ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; t\n.sart ---&gt; h\nsarth ---&gt; a\nartha ---&gt; k\nrthak ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; j\nfiroj ---&gt; a\niroja ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; w\n.kunw ---&gt; a\nkunwa ---&gt; r\nunwar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; j\npramj ---&gt; o\nramjo ---&gt; t\namjot ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; a\nashra ---&gt; b\nshrab ---&gt; i\nhrabi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; a\n.rata ---&gt; n\nratan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; a\nracha ---&gt; n\nachan ---&gt; a\nchana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; j\nriyaj ---&gt; u\niyaju ---&gt; l\nyajul ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; z\n..anz ---&gt; u\n.anzu ---&gt; m\nanzum ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; o\nhoolo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; a\n.haza ---&gt; r\nhazar ---&gt; a\nazara ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; n\nnayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; i\n.javi ---&gt; t\njavit ---&gt; r\navitr ---&gt; i\nvitri ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; b\n.kulb ---&gt; i\nkulbi ---&gt; r\nulbir ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; l\nbabal ---&gt; u\nabalu ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; l\n.leel ---&gt; a\nleela ---&gt; w\neelaw ---&gt; a\nelawa ---&gt; t\nlawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; w\n.ishw ---&gt; a\nishwa ---&gt; r\nshwar ---&gt; i\nhwari ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; v\nindrv ---&gt; a\nndrva ---&gt; t\ndrvat ---&gt; i\nrvati ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; v\n.ranv ---&gt; e\nranve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; s\ngulfs ---&gt; a\nulfsa ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; i\n.gudi ---&gt; y\ngudiy ---&gt; a\nudiya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; h\nparsh ---&gt; o\narsho ---&gt; t\nrshot ---&gt; a\nshota ---&gt; m\nhotam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; t\n.kamt ---&gt; a\nkamta ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; k\nsurek ---&gt; h\nurekh ---&gt; a\nrekha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; c\n..sac ---&gt; h\n.sach ---&gt; i\nsachi ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; k\n..akk ---&gt; a\n.akka ---&gt; s\nakkas ---&gt; h\nkkash ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; m\nmoham ---&gt; m\nohamm ---&gt; a\nhamma ---&gt; d\nammad ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; l\n..tol ---&gt; a\n.tola ---&gt; r\ntolar ---&gt; a\nolara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; m\n..lam ---&gt; b\n.lamb ---&gt; h\nlambh ---&gt; u\nambhu ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; s\nepans ---&gt; h\npansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; i\n.vidi ---&gt; t\nvidit ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; m\nmoham ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; u\n.ritu ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; j\n..fij ---&gt; a\n.fija ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; t\n..vit ---&gt; a\n.vita ---&gt; n\nvitan ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; a\namana ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; b\nchhab ---&gt; i\nhhabi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; r\n.sehr ---&gt; u\nsehru ---&gt; n\nehrun ---&gt; i\nhruni ---&gt; s\nrunis ---&gt; a\nunisa ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; l\n.pool ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; a\nambha ---&gt; r\nmbhar ---&gt; o\nbharo ---&gt; s\nharos ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; m\n..hom ---&gt; a\n.homa ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; c\neelac ---&gt; h\nelach ---&gt; a\nlacha ---&gt; l\nachal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; y\nraziy ---&gt; a\naziya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; l\nrahil ---&gt; a\nahila ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; i\n.mini ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; i\n.bani ---&gt; y\nbaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; m\n.azam ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; a\n.bina ---&gt; n\nbinan ---&gt; i\ninani ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; w\n.danw ---&gt; a\ndanwa ---&gt; t\nanwat ---&gt; i\nnwati ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; o\n..jho ---&gt; t\n.jhot ---&gt; i\njhoti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; a\n.mega ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; o\n.kuso ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; z\n..niz ---&gt; a\n.niza ---&gt; m\nnizam ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; n\nyogen ---&gt; d\nogend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; a\numara ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; a\nashra ---&gt; f\nshraf ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; e\nbhise ---&gt; k\nhisek ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; i\ngulsi ---&gt; d\nulsid ---&gt; a\nlsida ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; m\nyashm ---&gt; i\nashmi ---&gt; n\nshmin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; s\n.kuls ---&gt; u\nkulsu ---&gt; m\nulsum ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; a\n.yama ---&gt; n\nyaman ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; t\n..alt ---&gt; a\n.alta ---&gt; f\naltaf ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; b\n..pab ---&gt; u\n.pabu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; v\n..biv ---&gt; l\n.bivl ---&gt; a\nbivla ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; i\n..phi ---&gt; l\n.phil ---&gt; o\nphilo ---&gt; m\nhilom ---&gt; i\nilomi ---&gt; n\nlomin ---&gt; a\nomina ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; k\n.tink ---&gt; i\ntinki ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; n\nshyan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; u\nhandu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; a\n.rada ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; a\n.ruka ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; d\narmod ---&gt; h\nrmodh ---&gt; .\n..... ---&gt; e\n....e ---&gt; v\n...ev ---&gt; a\n..eva ---&gt; n\n.evan ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; i\n.vasi ---&gt; m\nvasim ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; k\n.lavk ---&gt; u\nlavku ---&gt; s\navkus ---&gt; h\nvkush ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; g\nubhag ---&gt; i\nbhagi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; n\npriyn ---&gt; k\nriynk ---&gt; a\niynka ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; u\n..gou ---&gt; r\n.gour ---&gt; a\ngoura ---&gt; v\nourav ---&gt; .\n..... ---&gt; v\n....v ---&gt; y\n...vy ---&gt; e\n..vye ---&gt; l\n.vyel ---&gt; e\nvyele ---&gt; t\nyelet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; e\nhanwe ---&gt; r\nanwer ---&gt; i\nnweri ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; g\nchhag ---&gt; a\nhhaga ---&gt; n\nhagan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; m\nashim ---&gt; a\nshima ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; l\nbalal ---&gt; i\nalali ---&gt; a\nlalia ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; t\nnisht ---&gt; h\nishth ---&gt; a\nshtha ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; a\n.sira ---&gt; j\nsiraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; e\n.sate ---&gt; n\nsaten ---&gt; d\natend ---&gt; r\ntendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; g\n.swag ---&gt; t\nswagt ---&gt; i\nwagti ---&gt; k\nagtik ---&gt; a\ngtika ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; m\n.sajm ---&gt; a\nsajma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; b\n.narb ---&gt; a\nnarba ---&gt; d\narbad ---&gt; a\nrbada ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; a\narcha ---&gt; n\nrchan ---&gt; a\nchana ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; w\n..asw ---&gt; a\n.aswa ---&gt; n\naswan ---&gt; i\nswani ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; r\nsayar ---&gt; i\nayari ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; c\n..bec ---&gt; h\n.bech ---&gt; a\nbecha ---&gt; n\nechan ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; a\nprasa ---&gt; n\nrasan ---&gt; t\nasant ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; i\nrishi ---&gt; r\nishir ---&gt; a\nshira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; e\n..nae ---&gt; e\n.naee ---&gt; m\nnaeem ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; c\n..kac ---&gt; h\n.kach ---&gt; a\nkacha ---&gt; n\nachan ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; a\n.joya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; a\nmanga ---&gt; l\nangal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; k\n..sok ---&gt; a\n.soka ---&gt; t\nsokat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; v\npallv ---&gt; i\nallvi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; c\ndeepc ---&gt; h\neepch ---&gt; a\nepcha ---&gt; n\npchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; s\nulfas ---&gt; h\nlfash ---&gt; a\nfasha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; n\n..chn ---&gt; d\n.chnd ---&gt; a\nchnda ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; t\n..art ---&gt; i\n.arti ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; c\n.vicc ---&gt; k\nvicck ---&gt; y\niccky ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; d\n..kud ---&gt; e\n.kude ---&gt; e\nkudee ---&gt; p\nudeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; s\nsumes ---&gt; h\numesh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; h\nfarah ---&gt; a\naraha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; z\n..muz ---&gt; m\n.muzm ---&gt; i\nmuzmi ---&gt; l\nuzmil ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; d\n..bod ---&gt; u\n.bodu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; s\nmanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; l\nishal ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; a\n.tila ---&gt; k\ntilak ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; u\nanchu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; f\n..haf ---&gt; s\n.hafs ---&gt; a\nhafsa ---&gt; .\n..... ---&gt; c\n....c ---&gt; a\n...ca ---&gt; p\n..cap ---&gt; t\n.capt ---&gt; a\ncapta ---&gt; i\naptai ---&gt; n\nptain ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; l\nsahil ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; t\n.amit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; e\nsurje ---&gt; e\nurjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; t\nagwat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; m\nsumem ---&gt; t\numemt ---&gt; r\nmemtr ---&gt; a\nemtra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; d\n.angd ---&gt; a\nangda ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; l\n..all ---&gt; a\n.alla ---&gt; b\nallab ---&gt; a\nllaba ---&gt; k\nlabak ---&gt; s\nabaks ---&gt; h\nbaksh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; a\nmadha ---&gt; n\nadhan ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; a\nbhola ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; r\n..abr ---&gt; a\n.abra ---&gt; r\nabrar ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; i\n.puni ---&gt; a\npunia ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; r\n.namr ---&gt; a\nnamra ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; l\n.bhal ---&gt; a\nbhala ---&gt; r\nhalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; n\n.purn ---&gt; m\npurnm ---&gt; a\nurnma ---&gt; l\nrnmal ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; d\n..bod ---&gt; u\n.bodu ---&gt; r\nbodur ---&gt; a\nodura ---&gt; m\nduram ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; t\n.aaft ---&gt; a\naafta ---&gt; a\naftaa ---&gt; b\nftaab ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; n\nshrin ---&gt; a\nhrina ---&gt; t\nrinat ---&gt; h\ninath ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; v\n.rajv ---&gt; i\nrajvi ---&gt; r\najvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; n\nsaran ---&gt; j\naranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; l\nsunal ---&gt; i\nunali ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; m\nhoolm ---&gt; a\noolma ---&gt; n\nolman ---&gt; i\nlmani ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; e\n.hase ---&gt; e\nhasee ---&gt; n\naseen ---&gt; a\nseena ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; m\nkashm ---&gt; i\nashmi ---&gt; r\nshmir ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; f\ngulaf ---&gt; s\nulafs ---&gt; h\nlafsh ---&gt; a\nafsha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; a\n.nava ---&gt; l\nnaval ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; i\nramsi ---&gt; n\namsin ---&gt; g\nmsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; m\n.musm ---&gt; i\nmusmi ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; a\nfaiza ---&gt; n\naizan ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; i\nnoori ---&gt; n\noorin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; m\nshrim ---&gt; a\nhrima ---&gt; t\nrimat ---&gt; i\nimati ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; t\n..put ---&gt; u\n.putu ---&gt; l\nputul ---&gt; u\nutulu ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; p\n.krip ---&gt; y\nkripy ---&gt; a\nripya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; s\nnajis ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; s\n.bhus ---&gt; h\nbhush ---&gt; a\nhusha ---&gt; n\nushan ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; u\n.menu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; r\n.tajr ---&gt; a\ntajra ---&gt; n\najran ---&gt; i\njrani ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; i\njasvi ---&gt; n\nasvin ---&gt; d\nsvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; i\nmanvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; a\nusuma ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; s\nyoges ---&gt; h\nogesh ---&gt; w\ngeshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; k\n....k ---&gt; n\n...kn ---&gt; h\n..knh ---&gt; e\n.knhe ---&gt; y\nknhey ---&gt; a\nnheya ---&gt; l\nheyal ---&gt; a\neyala ---&gt; l\nyalal ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; e\n..are ---&gt; e\n.aree ---&gt; n\nareen ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; r\n..ibr ---&gt; a\n.ibra ---&gt; h\nibrah ---&gt; e\nbrahe ---&gt; e\nrahee ---&gt; m\naheem ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; r\nmanir ---&gt; a\nanira ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; e\n.sone ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; a\nshada ---&gt; b\nhadab ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; u\nchotu ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; r\n.kher ---&gt; u\nkheru ---&gt; l\nherul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; i\nrakhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; i\n.vidi ---&gt; s\nvidis ---&gt; h\nidish ---&gt; a\ndisha ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; t\n.krit ---&gt; i\nkriti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; e\nhaile ---&gt; s\nailes ---&gt; h\nilesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; w\n.rijw ---&gt; a\nrijwa ---&gt; n\nijwan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; p\n..jap ---&gt; n\n.japn ---&gt; e\njapne ---&gt; e\napnee ---&gt; t\npneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; s\nsahis ---&gt; t\nahist ---&gt; a\nhista ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; a\n..sma ---&gt; r\n.smar ---&gt; t\nsmart ---&gt; i\nmarti ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; a\n.kava ---&gt; l\nkaval ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; s\n.anis ---&gt; h\nanish ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; w\n..diw ---&gt; a\n.diwa ---&gt; n\ndiwan ---&gt; s\niwans ---&gt; i\nwansi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; m\n.gurm ---&gt; e\ngurme ---&gt; l\nurmel ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; p\naramp ---&gt; a\nrampa ---&gt; l\nampal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; y\nsafiy ---&gt; a\nafiya ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; r\nompar ---&gt; s\nmpars ---&gt; a\nparsa ---&gt; d\narsad ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; a\n.kela ---&gt; m\nkelam ---&gt; a\nelama ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; k\nhandk ---&gt; o\nandko ---&gt; r\nndkor ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; t\n..ant ---&gt; a\n.anta ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; h\n.jish ---&gt; a\njisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; r\n..asr ---&gt; f\n.asrf ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; j\n..roj ---&gt; i\n.roji ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; a\n.vira ---&gt; t\nvirat ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; m\nharim ---&gt; o\narimo ---&gt; h\nrimoh ---&gt; a\nimoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; n\n.ragn ---&gt; i\nragni ---&gt; .\n..... ---&gt; r\n....r ---&gt; s\n...rs ---&gt; j\n..rsj ---&gt; e\n.rsje ---&gt; s\nrsjes ---&gt; h\nsjesh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; a\nfaiza ---&gt; l\naizal ---&gt; .\n..... ---&gt; y\n....y ---&gt; e\n...ye ---&gt; s\n..yes ---&gt; h\n.yesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; d\n.sind ---&gt; e\nsinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; a\n.joya ---&gt; t\njoyat ---&gt; e\noyate ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; b\n.rubb ---&gt; i\nrubbi ---&gt; n\nubbin ---&gt; a\nbbina ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; i\nvandi ---&gt; t\nandit ---&gt; a\nndita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; i\nsidhi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; s\n..prs ---&gt; a\n.prsa ---&gt; n\nprsan ---&gt; t\nrsant ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; m\n.neem ---&gt; i\nneemi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; r\nvendr ---&gt; i\nendri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; a\nantra ---&gt; m\nntram ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; b\n..mub ---&gt; a\n.muba ---&gt; s\nmubas ---&gt; h\nubash ---&gt; i\nbashi ---&gt; r\nashir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; e\nmande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; n\nhivan ---&gt; g\nivang ---&gt; i\nvangi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; a\nshaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; h\n..pah ---&gt; a\n.paha ---&gt; l\npahal ---&gt; w\nahalw ---&gt; a\nhalwa ---&gt; n\nalwan ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; t\n.mult ---&gt; a\nmulta ---&gt; n\nultan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; n\ndhann ---&gt; u\nhannu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; n\nsandn ---&gt; a\nandna ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; i\n.kuli ---&gt; n\nkulin ---&gt; a\nulina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; k\n.sank ---&gt; a\nsanka ---&gt; r\nankar ---&gt; l\nnkarl ---&gt; a\nkarla ---&gt; l\narlal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; f\n..gaf ---&gt; f\n.gaff ---&gt; a\ngaffa ---&gt; r\naffar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; g\n.parg ---&gt; a\nparga ---&gt; t\nargat ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; a\ndhana ---&gt; n\nhanan ---&gt; j\nananj ---&gt; a\nnanja ---&gt; i\nanjai ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; h\ndaksh ---&gt; i\nakshi ---&gt; n\nkshin ---&gt; a\nshina ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; v\n..shv ---&gt; a\n.shva ---&gt; n\nshvan ---&gt; i\nhvani ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; m\n.reem ---&gt; i\nreemi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; l\n.jasl ---&gt; e\njasle ---&gt; e\naslee ---&gt; n\nsleen ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; e\npappe ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; a\n.afsa ---&gt; r\nafsar ---&gt; i\nfsari ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; f\n..gaf ---&gt; u\n.gafu ---&gt; r\ngafur ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; i\n.chhi ---&gt; d\nchhid ---&gt; u\nhhidu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; s\n.viks ---&gt; h\nviksh ---&gt; i\nikshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; m\n.susm ---&gt; i\nsusmi ---&gt; t\nusmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; a\n.rima ---&gt; s\nrimas ---&gt; h\nimash ---&gt; a\nmasha ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; t\n.sint ---&gt; a\nsinta ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; n\n.yogn ---&gt; d\nyognd ---&gt; e\nognde ---&gt; r\ngnder ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; a\ngaura ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; v\n..tav ---&gt; v\n.tavv ---&gt; a\ntavva ---&gt; s\navvas ---&gt; u\nvvasu ---&gt; m\nvasum ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; u\n.anou ---&gt; r\nanour ---&gt; a\nnoura ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; n\n.reen ---&gt; u\nreenu ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; i\nnandi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; w\n.shiw ---&gt; a\nshiwa ---&gt; n\nhiwan ---&gt; i\niwani ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; i\ngoldi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; v\n..viv ---&gt; e\n.vive ---&gt; k\nvivek ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; i\n.nadi ---&gt; r\nnadir ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; z\n..ruz ---&gt; i\n.ruzi ---&gt; n\nruzin ---&gt; a\nuzina ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; f\n.julf ---&gt; i\njulfi ---&gt; c\nulfic ---&gt; a\nlfica ---&gt; r\nficar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; i\nramni ---&gt; w\namniw ---&gt; a\nmniwa ---&gt; s\nniwas ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; u\n.ratu ---&gt; l\nratul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; r\nhakir ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; a\nnirma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; y\n.mary ---&gt; a\nmarya ---&gt; n\naryan ---&gt; a\nryana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; e\n.some ---&gt; s\nsomes ---&gt; h\nomesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; m\n..dum ---&gt; a\n.duma ---&gt; n\nduman ---&gt; i\numani ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; a\nmohma ---&gt; d\nohmad ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; a\nbhina ---&gt; v\nhinav ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; t\n.lalt ---&gt; e\nlalte ---&gt; s\naltes ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; p\nnoorp ---&gt; a\noorpa ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; s\n.hams ---&gt; i\nhamsi ---&gt; r\namsir ---&gt; a\nmsira ---&gt; n\nsiran ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; u\n.annu ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; n\n.avin ---&gt; a\navina ---&gt; s\nvinas ---&gt; h\ninash ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; a\n.nika ---&gt; h\nnikah ---&gt; a\nikaha ---&gt; t\nkahat ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; a\n.soma ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; v\n.tanv ---&gt; e\ntanve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; y\n.sury ---&gt; a\nsurya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; l\n.shul ---&gt; e\nshule ---&gt; k\nhulek ---&gt; h\nulekh ---&gt; a\nlekha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; m\n.tanm ---&gt; a\ntanma ---&gt; y\nanmay ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; i\n.rosi ---&gt; n\nrosin ---&gt; a\nosina ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; e\n..ghe ---&gt; e\n.ghee ---&gt; s\nghees ---&gt; a\nheesa ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; l\nrihal ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; l\n.goll ---&gt; u\ngollu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; r\n.jafr ---&gt; i\njafri ---&gt; n\nafrin ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; a\n.reha ---&gt; n\nrehan ---&gt; a\nehana ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; m\n.pamm ---&gt; y\npammy ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; i\nmunni ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; a\n.kaba ---&gt; l\nkabal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; n\nrupan ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; d\n.gend ---&gt; a\ngenda ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; m\n.alim ---&gt; a\nalima ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; i\n..udi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; h\nramdh ---&gt; a\namdha ---&gt; n\nmdhan ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; a\n..zia ---&gt; r\n.ziar ---&gt; u\nziaru ---&gt; l\niarul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; a\nsamsa ---&gt; d\namsad ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; c\n.joyc ---&gt; e\njoyce ---&gt; e\noycee ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; m\nkarim ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; j\n.farj ---&gt; a\nfarja ---&gt; n\narjan ---&gt; u\nrjanu ---&gt; l\njanul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; e\n.rase ---&gt; e\nrasee ---&gt; l\naseel ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; n\nsujan ---&gt; t\nujant ---&gt; i\njanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; h\n.sadh ---&gt; n\nsadhn ---&gt; a\nadhna ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; g\n..pog ---&gt; a\n.poga ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; u\n.niru ---&gt; t\nnirut ---&gt; m\nirutm ---&gt; a\nrutma ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; a\n.nita ---&gt; m\nnitam ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; c\n..roc ---&gt; k\n.rock ---&gt; y\nrocky ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; v\ndeepv ---&gt; h\neepvh ---&gt; a\nepvha ---&gt; n\npvhan ---&gt; d\nvhand ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; e\njeete ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; d\namald ---&gt; e\nmalde ---&gt; e\naldee ---&gt; p\nldeep ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; a\n.husa ---&gt; i\nhusai ---&gt; n\nusain ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; i\narshi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; d\n..pad ---&gt; a\n.pada ---&gt; m\npadam ---&gt; a\nadama ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; d\n..kad ---&gt; i\n.kadi ---&gt; r\nkadir ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; r\n..zar ---&gt; i\n.zari ---&gt; n\nzarin ---&gt; a\narina ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; s\niyans ---&gt; i\nyansi ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; i\n.kiri ---&gt; t\nkirit ---&gt; i\niriti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; n\npreen ---&gt; a\nreena ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; o\n.anno ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; n\nshayn ---&gt; a\nhayna ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; l\nbadal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; n\n.sabn ---&gt; a\nsabna ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; r\n.simr ---&gt; a\nsimra ---&gt; n\nimran ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; i\n.moti ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; a\n.sima ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; a\nrihaa ---&gt; n\nihaan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; a\nvinda ---&gt; r\nindar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; y\nnaray ---&gt; a\naraya ---&gt; n\nrayan ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; s\n..pis ---&gt; t\n.pist ---&gt; a\npista ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; n\n..sen ---&gt; s\n.sens ---&gt; e\nsense ---&gt; r\nenser ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; i\n..twi ---&gt; n\n.twin ---&gt; k\ntwink ---&gt; a\nwinka ---&gt; l\ninkal ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; b\n..nib ---&gt; o\n.nibo ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; w\n..kuw ---&gt; a\n.kuwa ---&gt; r\nkuwar ---&gt; j\nuwarj ---&gt; e\nwarje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; i\n....i ---&gt; j\n...ij ---&gt; h\n..ijh ---&gt; a\n.ijha ---&gt; r\nijhar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; u\nmandu ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; r\n.teer ---&gt; a\nteera ---&gt; t\neerat ---&gt; h\nerath ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; u\n.raju ---&gt; d\nrajud ---&gt; i\najudi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; a\n.baha ---&gt; d\nbahad ---&gt; u\nahadu ---&gt; r\nhadur ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; s\n.baks ---&gt; i\nbaksi ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; a\n.afsa ---&gt; r\nafsar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; n\nsanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; k\nshank ---&gt; e\nhanke ---&gt; r\nanker ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; s\namjas ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; m\nkushm ---&gt; i\nushmi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; o\n.bito ---&gt; o\nbitoo ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; z\n..arz ---&gt; o\n.arzo ---&gt; o\narzoo ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; n\n..din ---&gt; e\n.dine ---&gt; s\ndines ---&gt; h\ninesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; v\nhemav ---&gt; a\nemava ---&gt; n\nmavan ---&gt; t\navant ---&gt; i\nvanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; d\n.sadd ---&gt; i\nsaddi ---&gt; q\naddiq ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; a\n.hara ---&gt; k\nharak ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; u\n.masu ---&gt; m\nmasum ---&gt; a\nasuma ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; a\nuraja ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; u\n.raju ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; p\n..tep ---&gt; u\n.tepu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; m\nnasim ---&gt; a\nasima ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; n\n.kamn ---&gt; i\nkamni ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; r\ngirir ---&gt; a\nirira ---&gt; j\nriraj ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; s\n..irs ---&gt; a\n.irsa ---&gt; d\nirsad ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; e\nkamre ---&gt; e\namree ---&gt; n\nmreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; n\nsujan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; a\nampaa ---&gt; l\nmpaal ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; e\n.teje ---&gt; n\ntejen ---&gt; d\nejend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; h\nharbh ---&gt; a\narbha ---&gt; j\nrbhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; m\n.tohm ---&gt; e\ntohme ---&gt; e\nohmee ---&gt; n\nhmeen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; y\nkalay ---&gt; a\nalaya ---&gt; n\nlayan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; g\n..aag ---&gt; a\n.aaga ---&gt; n\naagan ---&gt; d\nagand ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; u\nvashu ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; a\n.hira ---&gt; m\nhiram ---&gt; a\nirama ---&gt; n\nraman ---&gt; i\namani ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; p\n.somp ---&gt; a\nsompa ---&gt; l\nompal ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; a\n.aara ---&gt; t\naarat ---&gt; i\narati ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; a\nmusta ---&gt; k\nustak ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; v\n.perv ---&gt; e\nperve ---&gt; z\nervez ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; n\n.pran ---&gt; k\nprank ---&gt; u\nranku ---&gt; r\nankur ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; p\n.silp ---&gt; i\nsilpi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; w\n.ratw ---&gt; a\nratwa ---&gt; r\natwar ---&gt; i\ntwari ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; n\nrihan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; f\n.manf ---&gt; u\nmanfu ---&gt; l\nanful ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; w\n.asiw ---&gt; a\nasiwa ---&gt; n\nsiwan ---&gt; i\niwani ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; k\n.sukk ---&gt; a\nsukka ---&gt; .\n..... ---&gt; i\n....i ---&gt; t\n...it ---&gt; w\n..itw ---&gt; a\n.itwa ---&gt; r\nitwar ---&gt; i\ntwari ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; u\n.mamu ---&gt; n\nmamun ---&gt; a\namuna ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; t\n.meet ---&gt; a\nmeeta ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; j\n.girj ---&gt; e\ngirje ---&gt; s\nirjes ---&gt; h\nrjesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; n\narven ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; a\n.mosa ---&gt; m\nmosam ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; i\npravi ---&gt; n\nravin ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; j\ndhanj ---&gt; a\nhanja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; n\nagwan ---&gt; t\ngwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; j\nsarvj ---&gt; e\narvje ---&gt; e\nrvjee ---&gt; t\nvjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; a\n..ada ---&gt; r\n.adar ---&gt; s\nadars ---&gt; h\ndarsh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; s\naakas ---&gt; h\nakash ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; b\n..ikb ---&gt; a\n.ikba ---&gt; l\nikbal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; r\n.shur ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; a\n.pala ---&gt; r\npalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; i\nhursi ---&gt; d\nursid ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; e\n.juge ---&gt; n\njugen ---&gt; d\nugend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; t\nanant ---&gt; r\nnantr ---&gt; a\nantra ---&gt; m\nntram ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; s\n..tis ---&gt; h\n.tish ---&gt; a\ntisha ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; n\n..lon ---&gt; g\n.long ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; z\n..roz ---&gt; y\n.rozy ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; r\ndayar ---&gt; a\nayara ---&gt; m\nyaram ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; x\n..dix ---&gt; y\n.dixy ---&gt; a\ndixya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; w\n.bhuw ---&gt; a\nbhuwa ---&gt; n\nhuwan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; i\nsuhai ---&gt; l\nuhail ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; e\n..ude ---&gt; s\n.udes ---&gt; h\nudesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; r\nshahr ---&gt; u\nhahru ---&gt; k\nahruk ---&gt; h\nhrukh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; e\n.aade ---&gt; s\naades ---&gt; h\nadesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; j\nbiraj ---&gt; p\nirajp ---&gt; a\nrajpa ---&gt; l\najpal ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; p\n..tip ---&gt; u\n.tipu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; a\nkaila ---&gt; k\nailak ---&gt; i\nilaki ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; i\n.anki ---&gt; t\nankit ---&gt; a\nnkita ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; u\n.benu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; c\n.mamc ---&gt; h\nmamch ---&gt; a\namcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; k\nmahak ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; n\n.shen ---&gt; a\nshena ---&gt; z\nhenaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; d\nsazid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; e\n.saie ---&gt; m\nsaiem ---&gt; a\naiema ---&gt; n\nieman ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; m\n..ilm ---&gt; a\n.ilma ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; k\n..dik ---&gt; s\n.diks ---&gt; h\ndiksh ---&gt; a\niksha ---&gt; n\nkshan ---&gt; t\nshant ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; o\nguddo ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; u\n..gru ---&gt; c\n.gruc ---&gt; h\ngruch ---&gt; r\nruchr ---&gt; a\nuchra ---&gt; n\nchran ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; y\n.jeey ---&gt; a\njeeya ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; s\n..dus ---&gt; h\n.dush ---&gt; y\ndushy ---&gt; a\nushya ---&gt; n\nshyan ---&gt; t\nhyant ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; l\n.akhl ---&gt; a\nakhla ---&gt; q\nkhlaq ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; i\n.muni ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; n\n..hin ---&gt; i\n.hini ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; i\nmandi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; r\n.jafr ---&gt; u\njafru ---&gt; d\nafrud ---&gt; d\nfrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; u\n.binu ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; n\ndipan ---&gt; s\nipans ---&gt; h\npansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; k\n..tak ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; n\n.nazn ---&gt; e\nnazne ---&gt; e\naznee ---&gt; n\nzneen ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; s\n..nes ---&gt; h\n.nesh ---&gt; a\nnesha ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; a\n.fira ---&gt; k\nfirak ---&gt; a\niraka ---&gt; t\nrakat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; y\n.pary ---&gt; a\nparya ---&gt; n\naryan ---&gt; k\nryank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; l\nsumal ---&gt; i\numali ---&gt; a\nmalia ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; d\n..had ---&gt; i\n.hadi ---&gt; s\nhadis ---&gt; h\nadish ---&gt; a\ndisha ---&gt; .\n..... ---&gt; n\n....n ---&gt; r\n...nr ---&gt; o\n..nro ---&gt; t\n.nrot ---&gt; a\nnrota ---&gt; m\nrotam ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; a\n.aaya ---&gt; n\naayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; u\nreetu ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; k\n.seek ---&gt; h\nseekh ---&gt; a\neekha ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; m\nabhim ---&gt; a\nbhima ---&gt; n\nhiman ---&gt; u\nimanu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; a\naruna ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; m\n.manm ---&gt; e\nmanme ---&gt; e\nanmee ---&gt; t\nnmeet ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; b\n.parb ---&gt; h\nparbh ---&gt; a\narbha ---&gt; t\nrbhat ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; o\n.faro ---&gt; j\nfaroj ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; s\nmonis ---&gt; h\nonish ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; r\ndivar ---&gt; a\nivara ---&gt; j\nvaraj ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; i\nchuni ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; b\nshahb ---&gt; a\nhahba ---&gt; z\nahbaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; e\nshake ---&gt; e\nhakee ---&gt; l\nakeel ---&gt; a\nkeela ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; i\n.jahi ---&gt; r\njahir ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; k\nrishk ---&gt; a\nishka ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; m\nkaram ---&gt; v\naramv ---&gt; i\nramvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; e\nramde ---&gt; v\namdev ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; n\n.akan ---&gt; s\nakans ---&gt; h\nkansh ---&gt; a\nansha ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; b\n..pab ---&gt; i\n.pabi ---&gt; t\npabit ---&gt; r\nabitr ---&gt; a\nbitra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; f\n.saif ---&gt; a\nsaifa ---&gt; l\naifal ---&gt; i\nifali ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; a\n.shra ---&gt; b\nshrab ---&gt; a\nhraba ---&gt; n\nraban ---&gt; i\nabani ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; y\nsabiy ---&gt; a\nabiya ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; t\n..ket ---&gt; a\n.keta ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; t\n..bht ---&gt; e\n.bhte ---&gt; r\nbhter ---&gt; i\nhteri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; h\nsandh ---&gt; a\nandha ---&gt; y\nndhay ---&gt; a\ndhaya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; b\n.hasb ---&gt; u\nhasbu ---&gt; l\nasbul ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; a\nruksa ---&gt; r\nuksar ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; r\nkeshr ---&gt; i\neshri ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; u\n.gulu ---&gt; r\ngulur ---&gt; a\nulura ---&gt; m\nluram ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; v\n.rajv ---&gt; e\nrajve ---&gt; e\najvee ---&gt; r\njveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; j\n.sahj ---&gt; a\nsahja ---&gt; d\nahjad ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; r\n.susr ---&gt; i\nsusri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; a\n.vina ---&gt; y\nvinay ---&gt; a\ninaya ---&gt; k\nnayak ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; w\nishaw ---&gt; a\nshawa ---&gt; r\nhawar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; u\n.papu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; w\nsharw ---&gt; a\nharwa ---&gt; n\narwan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; e\nshele ---&gt; s\nheles ---&gt; h\nelesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; a\n.maya ---&gt; n\nmayan ---&gt; k\nayank ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; m\n..alm ---&gt; i\n.almi ---&gt; n\nalmin ---&gt; a\nlmina ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; t\n.krit ---&gt; i\nkriti ---&gt; k\nritik ---&gt; a\nitika ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; e\n.dale ---&gt; r\ndaler ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; i\n.udai ---&gt; y\nudaiy ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; h\n..lah ---&gt; i\n.lahi ---&gt; d\nlahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; i\nshali ---&gt; g\nhalig ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; d\n..abd ---&gt; u\n.abdu ---&gt; l\nabdul ---&gt; l\nbdull ---&gt; a\ndulla ---&gt; h\nullah ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; a\noorja ---&gt; m\norjam ---&gt; a\nrjama ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; r\n.makr ---&gt; u\nmakru ---&gt; d\nakrud ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; z\n.farz ---&gt; a\nfarza ---&gt; n\narzan ---&gt; a\nrzana ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; j\n.nooj ---&gt; o\nnoojo ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; i\n..ghi ---&gt; s\n.ghis ---&gt; a\nghisa ---&gt; r\nhisar ---&gt; a\nisara ---&gt; m\nsaram ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; j\n.bhoj ---&gt; a\nbhoja ---&gt; r\nhojar ---&gt; a\nojara ---&gt; m\njaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; n\n.khan ---&gt; c\nkhanc ---&gt; h\nhanch ---&gt; a\nancha ---&gt; n\nnchan ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; l\nbrijl ---&gt; a\nrijla ---&gt; l\nijlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; k\n.bhik ---&gt; i\nbhiki ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; k\n..bik ---&gt; r\n.bikr ---&gt; a\nbikra ---&gt; m\nikram ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; i\n.guni ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; w\n.girw ---&gt; e\ngirwe ---&gt; r\nirwer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; s\n.saks ---&gt; h\nsaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; i\nharvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; a\n.guna ---&gt; n\ngunan ---&gt; i\nunani ---&gt; d\nnanid ---&gt; h\nanidh ---&gt; i\nnidhi ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; y\n..koy ---&gt; a\n.koya ---&gt; l\nkoyal ---&gt; i\noyali ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; s\ntanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; p\n..kap ---&gt; t\n.kapt ---&gt; a\nkapta ---&gt; n\naptan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; a\n.ansa ---&gt; r\nansar ---&gt; .\n..... ---&gt; k\n....k ---&gt; m\n...km ---&gt; l\n..kml ---&gt; e\n.kmle ---&gt; s\nkmles ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; t\nemant ---&gt; i\nmanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; i\n.pavi ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; i\n..uji ---&gt; r\n.ujir ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; o\nveero ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; d\n..aid ---&gt; t\n.aidt ---&gt; y\naidty ---&gt; a\nidtya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; a\nsarda ---&gt; r\nardar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; i\n.aati ---&gt; f\naatif ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; t\nanget ---&gt; a\nngeta ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; d\n..jod ---&gt; h\n.jodh ---&gt; i\njodhi ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; j\n.fulj ---&gt; h\nfuljh ---&gt; a\nuljha ---&gt; d\nljhad ---&gt; i\njhadi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; i\n.vani ---&gt; s\nvanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; t\n.swat ---&gt; r\nswatr ---&gt; i\nwatri ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; v\n.tanv ---&gt; i\ntanvi ---&gt; r\nanvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; e\nangee ---&gt; t\nngeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; p\nshimp ---&gt; i\nhimpi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; a\n.sala ---&gt; m\nsalam ---&gt; a\nalama ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; d\n..pad ---&gt; m\n.padm ---&gt; a\npadma ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; o\n.kajo ---&gt; l\nkajol ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; d\nsajid ---&gt; a\najida ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; l\n..nel ---&gt; a\n.nela ---&gt; m\nnelam ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; a\n.nima ---&gt; h\nnimah ---&gt; e\nimahe ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; y\n..siy ---&gt; a\n.siya ---&gt; r\nsiyar ---&gt; a\niyara ---&gt; m\nyaram ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; a\n.akha ---&gt; t\nakhat ---&gt; a\nkhata ---&gt; r\nhatar ---&gt; i\natari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; l\nsheel ---&gt; a\nheela ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; w\ntabbw ---&gt; u\nabbwu ---&gt; m\nbbwum ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; t\nshakt ---&gt; i\nhakti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; k\nriyak ---&gt; a\niyaka ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; s\n..las ---&gt; h\n.lash ---&gt; m\nlashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; e\n.bije ---&gt; n\nbijen ---&gt; d\nijend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; u\ndishu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; m\n.samm ---&gt; a\nsamma ---&gt; n\namman ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; l\nabhil ---&gt; a\nbhila ---&gt; s\nhilas ---&gt; h\nilash ---&gt; a\nlasha ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; s\nashis ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; w\n.yasw ---&gt; a\nyaswa ---&gt; n\naswan ---&gt; i\nswani ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; n\n..ajn ---&gt; o\n.ajno ---&gt; o\najnoo ---&gt; r\njnoor ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; j\nrabhj ---&gt; i\nabhji ---&gt; t\nbhjit ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; d\n.kand ---&gt; a\nkanda ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; f\n..asf ---&gt; a\n.asfa ---&gt; k\nasfak ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; a\n..ila ---&gt; y\n.ilay ---&gt; a\nilaya ---&gt; t\nlayat ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; m\nnshum ---&gt; a\nshuma ---&gt; n\nhuman ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; y\nchhay ---&gt; a\nhhaya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; e\nramke ---&gt; s\namkes ---&gt; h\nmkesh ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; k\n..gok ---&gt; u\n.goku ---&gt; l\ngokul ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; a\nbhura ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; h\ndilsh ---&gt; a\nilsha ---&gt; d\nlshad ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; a\nmrita ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; t\n.noot ---&gt; a\nnoota ---&gt; n\nootan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; l\n..vil ---&gt; r\n.vilr ---&gt; a\nvilra ---&gt; m\nilram ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; k\n.malk ---&gt; i\nmalki ---&gt; a\nalkia ---&gt; t\nlkiat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; e\n.sule ---&gt; k\nsulek ---&gt; h\nulekh ---&gt; a\nlekha ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; m\n.panm ---&gt; a\npanma ---&gt; t\nanmat ---&gt; i\nnmati ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; m\nusham ---&gt; v\nshamv ---&gt; i\nhamvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; r\nshabr ---&gt; a\nhabra ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; r\n.anjr ---&gt; e\nanjre ---&gt; j\nnjrej ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; k\nantok ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; a\n.gora ---&gt; v\ngorav ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; l\n.batl ---&gt; o\nbatlo ---&gt; o\natloo ---&gt; n\ntloon ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; m\n..shm ---&gt; e\n.shme ---&gt; j\nshmej ---&gt; h\nhmejh ---&gt; a\nmejha ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; m\nmahim ---&gt; a\nahima ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; i\n.aani ---&gt; k\naanik ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; s\n.amas ---&gt; i\namasi ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; i\n.fazi ---&gt; a\nfazia ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; i\n..khi ---&gt; m\n.khim ---&gt; a\nkhima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; l\nsaral ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; y\nmariy ---&gt; a\nariya ---&gt; m\nriyam ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; h\n..wah ---&gt; e\n.wahe ---&gt; e\nwahee ---&gt; d\naheed ---&gt; a\nheeda ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; e\npuspe ---&gt; n\nuspen ---&gt; d\nspend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; i\nrishi ---&gt; p\niship ---&gt; a\nshipa ---&gt; l\nhipal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; o\n.majo ---&gt; o\nmajoo ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; c\namarc ---&gt; h\nmarch ---&gt; a\narcha ---&gt; n\nrchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; s\n.chos ---&gt; h\nchosh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; h\nmansh ---&gt; e\nanshe ---&gt; e\nnshee ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; o\n..kho ---&gt; o\n.khoo ---&gt; s\nkhoos ---&gt; h\nhoosh ---&gt; b\nooshb ---&gt; o\noshbo ---&gt; o\nshboo ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; r\n..isr ---&gt; a\n.isra ---&gt; i\nisrai ---&gt; l\nsrail ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; n\nshivn ---&gt; a\nhivna ---&gt; t\nivnat ---&gt; h\nvnath ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; s\n.tams ---&gt; a\ntamsa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; a\n.nasa ---&gt; r\nnasar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; a\nroopa ---&gt; l\noopal ---&gt; i\nopali ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; p\n.devp ---&gt; a\ndevpa ---&gt; l\nevpal ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; e\n.dipe ---&gt; n\ndipen ---&gt; d\nipend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; g\n..jog ---&gt; a\n.joga ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; a\n.jita ---&gt; n\njitan ---&gt; d\nitand ---&gt; e\ntande ---&gt; r\nander ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; y\n..gay ---&gt; a\n.gaya ---&gt; t\ngayat ---&gt; r\nayatr ---&gt; i\nyatri ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; c\n..nac ---&gt; h\n.nach ---&gt; i\nnachi ---&gt; t\nachit ---&gt; a\nchita ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; a\nrukha ---&gt; r\nukhar ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; d\n..awd ---&gt; h\n.awdh ---&gt; e\nawdhe ---&gt; s\nwdhes ---&gt; h\ndhesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; a\nkamla ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; v\nvishv ---&gt; n\nishvn ---&gt; a\nshvna ---&gt; t\nhvnat ---&gt; h\nvnath ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; a\n.mada ---&gt; n\nmadan ---&gt; l\nadanl ---&gt; a\ndanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; e\nbhole ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; i\nramvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; t\n..imt ---&gt; y\n.imty ---&gt; a\nimtya ---&gt; z\nmtyaz ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; i\nhandi ---&gt; n\nandin ---&gt; i\nndini ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; t\nravit ---&gt; a\navita ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; i\njagdi ---&gt; s\nagdis ---&gt; h\ngdish ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; a\n.vipa ---&gt; s\nvipas ---&gt; h\nipash ---&gt; a\npasha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; h\nprith ---&gt; v\nrithv ---&gt; i\nithvi ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; l\n..aal ---&gt; i\n.aali ---&gt; y\naaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; z\narvez ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; u\n.janu ---&gt; k\njanuk ---&gt; a\nanuka ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; h\n.jhuh ---&gt; i\njhuhi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; m\n.jaim ---&gt; a\njaima ---&gt; t\naimat ---&gt; i\nimati ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; l\n.akil ---&gt; e\nakile ---&gt; s\nkiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; p\n.mehp ---&gt; h\nmehph ---&gt; a\nehpha ---&gt; l\nhphal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; r\n.gulr ---&gt; e\ngulre ---&gt; g\nulreg ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; u\n..gou ---&gt; t\n.gout ---&gt; a\ngouta ---&gt; m\noutam ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; k\n..yak ---&gt; s\n.yaks ---&gt; h\nyaksh ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; m\nbiram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; b\nhakib ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; t\n.kart ---&gt; a\nkarta ---&gt; r\nartar ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; i\nruchi ---&gt; t\nuchit ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; t\nchett ---&gt; a\nhetta ---&gt; n\nettan ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; r\nharir ---&gt; a\narira ---&gt; m\nriram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; v\nshahv ---&gt; a\nhahva ---&gt; j\nahvaj ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; k\n.mink ---&gt; a\nminka ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; i\n.bini ---&gt; t\nbinit ---&gt; a\ninita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; i\nsarai ---&gt; n\narain ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; e\nparde ---&gt; p\nardep ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; u\n.baju ---&gt; l\nbajul ---&gt; a\najula ---&gt; l\njulal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; i\nlaxmi ---&gt; k\naxmik ---&gt; a\nxmika ---&gt; n\nmikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; z\n..muz ---&gt; i\n.muzi ---&gt; m\nmuzim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; i\n.rati ---&gt; m\nratim ---&gt; a\natima ---&gt; n\ntiman ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; l\n.parl ---&gt; a\nparla ---&gt; d\narlad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; d\narmod ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; v\n..rev ---&gt; a\n.reva ---&gt; k\nrevak ---&gt; s\nevaks ---&gt; h\nvaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; m\nsushm ---&gt; i\nushmi ---&gt; t\nshmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; h\nsarah ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; n\ntaran ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; a\nanura ---&gt; j\nnuraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; s\n.sans ---&gt; a\nsansa ---&gt; a\nansaa ---&gt; r\nnsaar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; k\n.prak ---&gt; a\npraka ---&gt; s\nrakas ---&gt; h\nakash ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; u\nbadru ---&gt; d\nadrud ---&gt; e\ndrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; e\n.nage ---&gt; n\nnagen ---&gt; d\nagend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; n\n..hin ---&gt; a\n.hina ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; a\n.kawa ---&gt; l\nkawal ---&gt; j\nawalj ---&gt; e\nwalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; i\ngauri ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; n\nnaran ---&gt; d\narand ---&gt; e\nrande ---&gt; r\nander ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; r\nsawar ---&gt; m\nawarm ---&gt; a\nwarma ---&gt; l\narmal ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; v\n.nirv ---&gt; a\nnirva ---&gt; t\nirvat ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; d\n.gand ---&gt; h\ngandh ---&gt; i\nandhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; i\n.sobi ---&gt; t\nsobit ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; s\n..ais ---&gt; h\n.aish ---&gt; a\naisha ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; y\n.vidy ---&gt; a\nvidya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; i\n.pavi ---&gt; t\npavit ---&gt; r\navitr ---&gt; a\nvitra ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; r\n.peer ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; z\n..fiz ---&gt; a\n.fiza ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; i\n.rafi ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; u\n.riju ---&gt; l\nrijul ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; a\nprema ---&gt; w\nremaw ---&gt; a\nemawa ---&gt; t\nmawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; e\n.june ---&gt; b\njuneb ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; m\nbhanm ---&gt; a\nhanma ---&gt; t\nanmat ---&gt; i\nnmati ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; i\nhandi ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; m\n..ahm ---&gt; e\n.ahme ---&gt; d\nahmed ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; a\nparva ---&gt; t\narvat ---&gt; i\nrvati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; z\n..soz ---&gt; i\n.sozi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; k\n.vikk ---&gt; y\nvikky ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; k\nneelk ---&gt; a\neelka ---&gt; n\nelkan ---&gt; t\nlkant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; w\n.satw ---&gt; i\nsatwi ---&gt; n\natwin ---&gt; d\ntwind ---&gt; e\nwinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; r\n..mir ---&gt; a\n.mira ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; u\n..phu ---&gt; l\n.phul ---&gt; b\nphulb ---&gt; i\nhulbi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; e\n..mhe ---&gt; n\n.mhen ---&gt; d\nmhend ---&gt; a\nhenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; w\n.ramw ---&gt; a\nramwa ---&gt; t\namwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; j\n.barj ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; b\n..vib ---&gt; h\n.vibh ---&gt; a\nvibha ---&gt; s\nibhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; k\n....k ---&gt; l\n...kl ---&gt; a\n..kla ---&gt; s\n.klas ---&gt; h\nklash ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; s\nhimas ---&gt; h\nimash ---&gt; u\nmashu ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; b\n.jhab ---&gt; a\njhaba ---&gt; n\nhaban ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; w\n.gulw ---&gt; a\ngulwa ---&gt; s\nulwas ---&gt; h\nlwash ---&gt; a\nwasha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; m\n..gom ---&gt; a\n.goma ---&gt; t\ngomat ---&gt; i\nomati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; i\n.sobi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; a\n.nisa ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; a\nsurja ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; n\nsudan ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; r\n.ishr ---&gt; a\nishra ---&gt; t\nshrat ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; c\n..luc ---&gt; k\n.luck ---&gt; y\nlucky ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; v\n.arav ---&gt; i\naravi ---&gt; n\nravin ---&gt; d\navind ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; a\nsusha ---&gt; i\nushai ---&gt; l\nshail ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; a\n.muna ---&gt; j\nmunaj ---&gt; i\nunaji ---&gt; r\nnajir ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; a\nehnaa ---&gt; z\nhnaaz ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; a\nkausa ---&gt; l\nausal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; a\n.haza ---&gt; r\nhazar ---&gt; i\nazari ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; w\najesw ---&gt; a\njeswa ---&gt; r\neswar ---&gt; y\nswary ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; e\nrende ---&gt; r\nender ---&gt; a\nndera ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; t\n.roht ---&gt; h\nrohth ---&gt; a\nohtha ---&gt; s\nhthas ---&gt; h\nthash ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; r\n.ajar ---&gt; a\najara ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; d\n..pad ---&gt; a\n.pada ---&gt; m\npadam ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; i\n..twi ---&gt; n\n.twin ---&gt; k\ntwink ---&gt; l\nwinkl ---&gt; e\ninkle ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; p\n.rajp ---&gt; u\nrajpu ---&gt; t\najput ---&gt; r\njputr ---&gt; a\nputra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; n\n.pann ---&gt; a\npanna ---&gt; l\nannal ---&gt; a\nnnala ---&gt; l\nnalal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; a\n.hara ---&gt; n\nharan ---&gt; a\narana ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; j\n.pooj ---&gt; a\npooja ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; b\n.shib ---&gt; a\nshiba ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; a\n.juga ---&gt; l\njugal ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; p\nakshp ---&gt; a\nkshpa ---&gt; a\nshpaa ---&gt; t\nhpaat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; w\n..paw ---&gt; n\n.pawn ---&gt; i\npawni ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; q\n..yaq ---&gt; o\n.yaqo ---&gt; o\nyaqoo ---&gt; b\naqoob ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; v\neshav ---&gt; e\nshave ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; r\n.mamr ---&gt; a\nmamra ---&gt; j\namraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; r\n.satr ---&gt; o\nsatro ---&gt; h\natroh ---&gt; a\ntroha ---&gt; n\nrohan ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; k\n.jhak ---&gt; a\njhaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; y\n.asiy ---&gt; a\nasiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; o\n.asho ---&gt; k\nashok ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; s\n.niks ---&gt; i\nniksi ---&gt; y\niksiy ---&gt; a\nksiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; y\n..ary ---&gt; a\n.arya ---&gt; n\naryan ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; e\n..ade ---&gt; s\n.ades ---&gt; h\nadesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; t\nmohat ---&gt; t\nohatt ---&gt; e\nhatte ---&gt; r\natter ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; p\n..nip ---&gt; a\n.nipa ---&gt; m\nnipam ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; h\nprath ---&gt; a\nratha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; p\n.satp ---&gt; a\nsatpa ---&gt; l\natpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; a\noorja ---&gt; h\norjah ---&gt; a\nrjaha ---&gt; n\njahan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; i\n.riti ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; o\n.yaso ---&gt; d\nyasod ---&gt; a\nasoda ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; i\n.aadi ---&gt; s\naadis ---&gt; h\nadish ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; a\n.nama ---&gt; n\nnaman ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; z\n..amz ---&gt; a\n.amza ---&gt; d\namzad ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; r\n.poor ---&gt; a\npoora ---&gt; n\nooran ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; i\nneeli ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; j\n.balj ---&gt; e\nbalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; i\n.shoi ---&gt; b\nshoib ---&gt; a\nhoiba ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; v\n.nanv ---&gt; e\nnanve ---&gt; e\nanvee ---&gt; t\nnveet ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; a\n.pala ---&gt; k\npalak ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; e\n.late ---&gt; e\nlatee ---&gt; f\nateef ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; x\n..lux ---&gt; m\n.luxm ---&gt; i\nluxmi ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; a\nfoola ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; r\n..dor ---&gt; i\n.dori ---&gt; l\ndoril ---&gt; a\norila ---&gt; l\nrilal ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; i\narchi ---&gt; t\nrchit ---&gt; a\nchita ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; i\n.aari ---&gt; f\naarif ---&gt; u\narifu ---&gt; n\nrifun ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; a\n.asma ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; d\ndevid ---&gt; e\nevide ---&gt; e\nvidee ---&gt; n\nideen ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; i\n.gori ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; a\n.kuna ---&gt; l\nkunal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; a\n.pava ---&gt; n\npavan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; e\nsamse ---&gt; r\namser ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; n\n..hon ---&gt; e\n.hone ---&gt; y\nhoney ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; k\n.hark ---&gt; e\nharke ---&gt; s\narkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; e\nshare ---&gt; e\nharee ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; n\nubhan ---&gt; g\nbhang ---&gt; i\nhangi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; e\nmanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; l\n.akhl ---&gt; i\nakhli ---&gt; s\nkhlis ---&gt; h\nhlish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; d\n.saad ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; t\n.gyat ---&gt; r\ngyatr ---&gt; i\nyatri ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; i\nshobi ---&gt; n\nhobin ---&gt; a\nobina ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; m\n..num ---&gt; e\n.nume ---&gt; s\nnumes ---&gt; h\numesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; k\n..kok ---&gt; i\n.koki ---&gt; l\nkokil ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; h\nmansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; u\n.vimu ---&gt; k\nvimuk ---&gt; t\nimukt ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; d\n.gend ---&gt; i\ngendi ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; n\n.veen ---&gt; i\nveeni ---&gt; t\neenit ---&gt; a\nenita ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; a\nlakha ---&gt; m\nakham ---&gt; i\nkhami ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; a\n.suba ---&gt; s\nsubas ---&gt; h\nubash ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; b\nratib ---&gt; h\natibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; j\n.chaj ---&gt; j\nchajj ---&gt; u\nhajju ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; a\n.fora ---&gt; n\nforan ---&gt; t\norant ---&gt; a\nranta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; b\n.chab ---&gt; i\nchabi ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; o\nushbo ---&gt; o\nshboo ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; h\nanchh ---&gt; i\nnchhi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; s\n.taps ---&gt; a\ntapsa ---&gt; m\napsam ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; e\nmunne ---&gt; .\n..... ---&gt; s\n....s ---&gt; y\n...sy ---&gt; e\n..sye ---&gt; d\n.syed ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; u\n.jamu ---&gt; n\njamun ---&gt; a\namuna ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; t\n..gut ---&gt; a\n.guta ---&gt; m\ngutam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; p\n.ship ---&gt; r\nshipr ---&gt; a\nhipra ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; g\n..big ---&gt; a\n.biga ---&gt; n\nbigan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; h\narish ---&gt; m\nrishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; f\n..avf ---&gt; e\n.avfe ---&gt; s\navfes ---&gt; h\nvfesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; z\nshahz ---&gt; a\nhahza ---&gt; d\nahzad ---&gt; i\nhzadi ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; a\n.kela ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; a\n.lala ---&gt; r\nlalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; y\n.maay ---&gt; a\nmaaya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; f\n.tauf ---&gt; i\ntaufi ---&gt; k\naufik ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; s\nraves ---&gt; h\navesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; a\n.pana ---&gt; j\npanaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; h\n..aph ---&gt; a\n.apha ---&gt; s\naphas ---&gt; a\nphasa ---&gt; n\nhasan ---&gt; a\nasana ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; j\n.gunj ---&gt; a\ngunja ---&gt; n\nunjan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; s\n.aans ---&gt; i\naansi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; e\nmithe ---&gt; l\nithel ---&gt; a\nthela ---&gt; s\nhelas ---&gt; h\nelash ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; t\n.khet ---&gt; r\nkhetr ---&gt; a\nhetra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; r\nsumer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; g\n..gug ---&gt; n\n.gugn ---&gt; a\ngugna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; m\n.saim ---&gt; u\nsaimu ---&gt; n\naimun ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; i\nprami ---&gt; l\nramil ---&gt; a\namila ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; l\n.anjl ---&gt; i\nanjli ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; h\n.nikh ---&gt; i\nnikhi ---&gt; l\nikhil ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; a\njeeta ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; r\n.beer ---&gt; a\nbeera ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; u\nsanju ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; i\nrajbi ---&gt; r\najbir ---&gt; i\njbiri ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; r\n.chor ---&gt; a\nchora ---&gt; g\nhorag ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; s\nshans ---&gt; h\nhansh ---&gt; a\nansha ---&gt; k\nnshak ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; k\namrik ---&gt; a\nmrika ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; a\njagda ---&gt; m\nagdam ---&gt; b\ngdamb ---&gt; a\ndamba ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; a\nbitta ---&gt; n\nittan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; a\nruksa ---&gt; a\nuksaa ---&gt; r\nksaar ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; s\n..yus ---&gt; a\n.yusa ---&gt; f\nyusaf ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; d\nsarid ---&gt; e\naride ---&gt; v\nridev ---&gt; i\nidevi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; d\n.ladd ---&gt; h\nladdh ---&gt; a\naddha ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; i\n.jahi ---&gt; d\njahid ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; e\n.suke ---&gt; n\nsuken ---&gt; t\nukent ---&gt; a\nkenta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; i\nambai ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; k\n.nikk ---&gt; y\nnikky ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; a\n.mala ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; n\ndhann ---&gt; a\nhanna ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; m\n.tajm ---&gt; a\ntajma ---&gt; h\najmah ---&gt; a\njmaha ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; i\n.neni ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; k\n.lakk ---&gt; y\nlakky ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; u\n.sahu ---&gt; n\nsahun ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; e\n..ome ---&gt; s\n.omes ---&gt; h\nomesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; a\n.aasa ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; h\nparsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; t\nshant ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; w\npremw ---&gt; a\nremwa ---&gt; t\nemwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; b\n..mob ---&gt; a\n.moba ---&gt; r\nmobar ---&gt; k\nobark ---&gt; a\nbarka ---&gt; r\narkar ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; s\n..his ---&gt; h\n.hish ---&gt; a\nhisha ---&gt; m\nisham ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; i\n.mosi ---&gt; m\nmosim ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; b\nshabb ---&gt; o\nhabbo ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; p\n.virp ---&gt; a\nvirpa ---&gt; l\nirpal ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; a\n..ada ---&gt; n\n.adan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; s\n..rus ---&gt; t\n.rust ---&gt; a\nrusta ---&gt; m\nustam ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; i\n.ajmi ---&gt; t\najmit ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; z\n..maz ---&gt; h\n.mazh ---&gt; a\nmazha ---&gt; r\nazhar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; i\n.sani ---&gt; t\nsanit ---&gt; a\nanita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; t\n..cht ---&gt; a\n.chta ---&gt; r\nchtar ---&gt; p\nhtarp ---&gt; a\ntarpa ---&gt; l\narpal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; g\n..gag ---&gt; a\n.gaga ---&gt; n\ngagan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; k\n.mink ---&gt; a\nminka ---&gt; s\ninkas ---&gt; h\nnkash ---&gt; i\nkashi ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; t\n.amrt ---&gt; a\namrta ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; t\nijayt ---&gt; a\njayta ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; w\n.banw ---&gt; a\nbanwa ---&gt; r\nanwar ---&gt; i\nnwari ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; b\nhramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; j\narvej ---&gt; .\n..... ---&gt; o\n....o ---&gt; n\n...on ---&gt; e\n..one ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; n\n.peen ---&gt; t\npeent ---&gt; u\neentu ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; l\n.udal ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; e\n..aje ---&gt; e\n.ajee ---&gt; m\najeem ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; z\n..maz ---&gt; i\n.mazi ---&gt; d\nmazid ---&gt; a\nazida ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; s\n....s ---&gt; t\n...st ---&gt; i\n..sti ---&gt; f\n.stif ---&gt; e\nstife ---&gt; n\ntifen ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; m\naseem ---&gt; a\nseema ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; i\n.suni ---&gt; l\nsunil ---&gt; .\n..... ---&gt; e\n....e ---&gt; z\n...ez ---&gt; a\n..eza ---&gt; z\n.ezaz ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; m\n.jasm ---&gt; e\njasme ---&gt; n\nasmen ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; u\n.batu ---&gt; l\nbatul ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; a\n.kela ---&gt; s\nkelas ---&gt; h\nelash ---&gt; i\nlashi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; h\n..alh ---&gt; a\n.alha ---&gt; m\nalham ---&gt; d\nlhamd ---&gt; i\nhamdi ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; r\n.busr ---&gt; a\nbusra ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; a\n..rua ---&gt; n\n.ruan ---&gt; b\nruanb ---&gt; z\nuanbz ---&gt; a\nanbza ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; r\n..bhr ---&gt; a\n.bhra ---&gt; t\nbhrat ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; v\n..bav ---&gt; i\n.bavi ---&gt; t\nbavit ---&gt; a\navita ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; s\namris ---&gt; h\nmrish ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; r\nhawar ---&gt; i\nawari ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; k\n.prak ---&gt; u\npraku ---&gt; l\nrakul ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; e\namare ---&gt; n\nmaren ---&gt; d\narend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; d\n..yad ---&gt; r\n.yadr ---&gt; a\nyadra ---&gt; m\nadram ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; u\n.satu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; r\n.hazr ---&gt; a\nhazra ---&gt; t\nazrat ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; b\n.mehb ---&gt; o\nmehbo ---&gt; o\nehboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; w\n..anw ---&gt; a\n.anwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; m\n.jaim ---&gt; a\njaima ---&gt; t\naimat ---&gt; a\nimata ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; d\n..nid ---&gt; a\n.nida ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; t\nrabit ---&gt; a\nabita ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; i\n.subi ---&gt; n\nsubin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; g\n.jang ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; u\n.anju ---&gt; m\nanjum ---&gt; a\nnjuma ---&gt; n\njuman ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; y\n.devy ---&gt; a\ndevya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; r\nhawar ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; s\n..lis ---&gt; h\n.lish ---&gt; a\nlisha ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; p\n.sunp ---&gt; r\nsunpr ---&gt; e\nunpre ---&gt; e\nnpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; a\naksha ---&gt; y\nkshay ---&gt; a\nshaya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; n\nnayan ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; m\n..udm ---&gt; a\n.udma ---&gt; i\nudmai ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; p\n.narp ---&gt; e\nnarpe ---&gt; n\narpen ---&gt; d\nrpend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; f\n.saif ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; p\nsadip ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; e\n.sawe ---&gt; t\nsawet ---&gt; a\naweta ---&gt; .\n..... ---&gt; u\n....u ---&gt; n\n...un ---&gt; k\n..unk ---&gt; a\n.unka ---&gt; r\nunkar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; s\n..las ---&gt; k\n.lask ---&gt; s\nlasks ---&gt; h\nasksh ---&gt; i\nskshi ---&gt; t\nkshit ---&gt; a\nshita ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; o\n..amo ---&gt; l\n.amol ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; i\n..udi ---&gt; t\n.udit ---&gt; a\nudita ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; a\n..taa ---&gt; r\n.taar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; e\nshahe ---&gt; r\nhaher ---&gt; a\nahera ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; i\nrajki ---&gt; r\najkir ---&gt; a\njkira ---&gt; n\nkiran ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; f\n..sif ---&gt; a\n.sifa ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; t\n.anit ---&gt; a\nanita ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; s\n..sos ---&gt; a\n.sosa ---&gt; n\nsosan ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; m\n.swam ---&gt; i\nswami ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; u\n..lau ---&gt; k\n.lauk ---&gt; u\nlauku ---&gt; s\naukus ---&gt; h\nukush ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; o\n.vero ---&gt; n\nveron ---&gt; i\neroni ---&gt; k\nronik ---&gt; a\nonika ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; s\n.dars ---&gt; h\ndarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; n\n.sajn ---&gt; i\nsajni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; a\nahaja ---&gt; h\nhajah ---&gt; a\najaha ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; e\n.sohe ---&gt; b\nsoheb ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; p\n.swap ---&gt; a\nswapa ---&gt; m\nwapam ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; i\nsohai ---&gt; l\nohail ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; r\n..umr ---&gt; a\n.umra ---&gt; w\numraw ---&gt; t\nmrawt ---&gt; i\nrawti ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; f\n..tuf ---&gt; e\n.tufe ---&gt; l\ntufel ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; i\nhushi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; b\n.gulb ---&gt; a\ngulba ---&gt; s\nulbas ---&gt; h\nlbash ---&gt; a\nbasha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; a\n.naha ---&gt; n\nnahan ---&gt; i\nahani ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; u\ngyanu ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; a\n.pera ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; r\n.girr ---&gt; a\ngirra ---&gt; j\nirraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; r\n.majr ---&gt; u\nmajru ---&gt; l\najrul ---&gt; a\njrula ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; m\nsanam ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; f\n..sef ---&gt; a\n.sefa ---&gt; l\nsefal ---&gt; i\nefali ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; s\n.kars ---&gt; h\nkarsh ---&gt; m\narshm ---&gt; a\nrshma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; b\n.mahb ---&gt; o\nmahbo ---&gt; o\nahboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; p\n.jasp ---&gt; r\njaspr ---&gt; e\naspre ---&gt; e\nspree ---&gt; t\npreet ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; o\n.maso ---&gt; o\nmasoo ---&gt; m\nasoom ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; g\nubhag ---&gt; y\nbhagy ---&gt; a\nhagya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; i\nsanji ---&gt; v\nanjiv ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; s\n.hans ---&gt; r\nhansr ---&gt; a\nansra ---&gt; j\nnsraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; z\nahnaz ---&gt; a\nhnaza ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; n\nnaven ---&gt; d\navend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; l\n.khal ---&gt; i\nkhali ---&gt; d\nhalid ---&gt; u\nalidu ---&gt; r\nlidur ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; d\n.rand ---&gt; h\nrandh ---&gt; i\nandhi ---&gt; r\nndhir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; r\n.gulr ---&gt; a\ngulra ---&gt; n\nulran ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; h\n.mush ---&gt; k\nmushk ---&gt; a\nushka ---&gt; n\nshkan ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; j\n.trij ---&gt; u\ntriju ---&gt; g\nrijug ---&gt; i\nijugi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; o\n.vipo ---&gt; l\nvipol ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; l\nhabil ---&gt; a\nabila ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; n\n.maan ---&gt; m\nmaanm ---&gt; a\naanma ---&gt; t\nanmat ---&gt; i\nnmati ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; u\nmanju ---&gt; s\nanjus ---&gt; h\nnjush ---&gt; a\njusha ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; i\n.mohi ---&gt; t\nmohit ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; m\n.pamm ---&gt; i\npammi ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; k\n..wak ---&gt; a\n.waka ---&gt; r\nwakar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; v\nbhagv ---&gt; a\nhagva ---&gt; a\nagvaa ---&gt; n\ngvaan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; c\n.jaic ---&gt; h\njaich ---&gt; a\naicha ---&gt; n\nichan ---&gt; d\nchand ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; m\nhushm ---&gt; a\nushma ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; a\nanupa ---&gt; m\nnupam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; l\n.sarl ---&gt; a\nsarla ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; r\n..ser ---&gt; a\n.sera ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; e\nparde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; k\n..sok ---&gt; i\n.soki ---&gt; n\nsokin ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; o\n.naro ---&gt; o\nnaroo ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; i\nchoti ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; r\nsugar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; g\n.rang ---&gt; e\nrange ---&gt; e\nangee ---&gt; t\nngeet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; b\n.ayub ---&gt; e\nayube ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; i\n.jiti ---&gt; n\njitin ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; n\n.punn ---&gt; i\npunni ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; a\nparsa ---&gt; n\narsan ---&gt; n\nrsann ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; d\nmoold ---&gt; a\noolda ---&gt; n\noldan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; a\n.nawa ---&gt; b\nnawab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; m\n.sajm ---&gt; e\nsajme ---&gt; e\najmee ---&gt; n\njmeen ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; n\n..pen ---&gt; z\n.penz ---&gt; i\npenzi ---&gt; n\nenzin ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; r\n..ibr ---&gt; a\n.ibra ---&gt; h\nibrah ---&gt; a\nbraha ---&gt; m\nraham ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; m\n.rehm ---&gt; a\nrehma ---&gt; t\nehmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; w\n.sarw ---&gt; i\nsarwi ---&gt; t\narwit ---&gt; a\nrwita ---&gt; v\nwitav ---&gt; m\nitavm ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; r\nshamr ---&gt; i\nhamri ---&gt; n\namrin ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; i\n.niki ---&gt; l\nnikil ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; a\n.mata ---&gt; d\nmatad ---&gt; e\natade ---&gt; e\ntadee ---&gt; n\nadeen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; i\nsurji ---&gt; y\nurjiy ---&gt; a\nrjiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; a\n.jaga ---&gt; n\njagan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; a\n.visa ---&gt; n\nvisan ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; l\n.deel ---&gt; i\ndeeli ---&gt; p\neelip ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; p\nshilp ---&gt; i\nhilpi ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; s\n..ves ---&gt; h\n.vesh ---&gt; a\nvesha ---&gt; l\neshal ---&gt; i\nshali ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; i\n.beni ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; g\n.bagg ---&gt; a\nbagga ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; g\n.pang ---&gt; i\npangi ---&gt; t\nangit ---&gt; a\nngita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; t\nhhant ---&gt; u\nhantu ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; l\n.phol ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; a\nramva ---&gt; t\namvat ---&gt; i\nmvati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; j\n.navj ---&gt; o\nnavjo ---&gt; o\navjoo ---&gt; t\nvjoot ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; a\n..ava ---&gt; d\n.avad ---&gt; h\navadh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; n\nashan ---&gt; j\nshanj ---&gt; a\nhanja ---&gt; l\nanjal ---&gt; i\nnjali ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; o\npremo ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; a\n.gaja ---&gt; n\ngajan ---&gt; a\najana ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; n\nrajan ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; j\n.sehj ---&gt; a\nsehja ---&gt; m\nehjam ---&gt; a\nhjama ---&gt; l\njamal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; u\n.mamu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; a\n.sapa ---&gt; n\nsapan ---&gt; a\napana ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; a\n.mura ---&gt; d\nmurad ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; r\n.vikr ---&gt; a\nvikra ---&gt; n\nikran ---&gt; t\nkrant ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; d\nshaid ---&gt; .\n..... ---&gt; f\n....f ---&gt; e\n...fe ---&gt; r\n..fer ---&gt; u\n.feru ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; t\n.kant ---&gt; i\nkanti ---&gt; l\nantil ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; i\n.yami ---&gt; n\nyamin ---&gt; i\namini ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; u\n.raju ---&gt; d\nrajud ---&gt; d\najudd ---&gt; i\njuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; e\nsarve ---&gt; s\narves ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; f\n..tuf ---&gt; a\n.tufa ---&gt; i\ntufai ---&gt; l\nufail ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; n\n..kin ---&gt; g\n.king ---&gt; k\nkingk ---&gt; a\ningka ---&gt; r\nngkar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; t\n.jant ---&gt; a\njanta ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; h\n..buh ---&gt; a\n.buha ---&gt; n\nbuhan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; s\n.jass ---&gt; i\njassi ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; r\n.jeer ---&gt; e\njeere ---&gt; n\neeren ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; b\n..job ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; b\n..arb ---&gt; a\n.arba ---&gt; z\narbaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; r\nsagir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; m\nnazim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; v\n.malv ---&gt; i\nmalvi ---&gt; k\nalvik ---&gt; a\nlvika ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; a\nulnaa ---&gt; z\nlnaaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; r\n.savr ---&gt; i\nsavri ---&gt; n\navrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; m\n.sarm ---&gt; e\nsarme ---&gt; e\narmee ---&gt; l\nrmeel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; i\n.tohi ---&gt; d\ntohid ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; v\n.souv ---&gt; i\nsouvi ---&gt; k\nouvik ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; u\nnanku ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; i\nshali ---&gt; d\nhalid ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; b\nadhub ---&gt; a\ndhuba ---&gt; l\nhubal ---&gt; a\nubala ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; l\n.chel ---&gt; a\nchela ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; j\n..ajj ---&gt; u\n.ajju ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; e\nsande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; a\ndeepa ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; e\nsuche ---&gt; t\nuchet ---&gt; a\ncheta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; l\nrahil ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; m\n.aasm ---&gt; i\naasmi ---&gt; n\nasmin ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; i\n.hami ---&gt; d\nhamid ---&gt; .\n..... ---&gt; r\n....r ---&gt; v\n...rv ---&gt; i\n..rvi ---&gt; n\n.rvin ---&gt; a\nrvina ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; t\n.maht ---&gt; a\nmahta ---&gt; b\nahtab ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; i\npriti ---&gt; k\nritik ---&gt; a\nitika ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; d\nlilad ---&gt; e\nilade ---&gt; v\nladev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; n\nrajin ---&gt; a\najina ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; p\ndurgp ---&gt; a\nurgpa ---&gt; l\nrgpal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; s\nmanes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; a\n.jaga ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; w\n.jhaw ---&gt; a\njhawa ---&gt; r\nhawar ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; t\n..kat ---&gt; y\n.katy ---&gt; a\nkatya ---&gt; y\natyay ---&gt; a\ntyaya ---&gt; n\nyayan ---&gt; i\nayani ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; o\n.kumo ---&gt; d\nkumod ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; l\n.niml ---&gt; a\nnimla ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; i\nsangi ---&gt; t\nangit ---&gt; a\nngita ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; i\n.jari ---&gt; f\njarif ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; a\n.tama ---&gt; n\ntaman ---&gt; a\namana ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; m\n..arm ---&gt; a\n.arma ---&gt; a\narmaa ---&gt; n\nrmaan ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; e\n.ajme ---&gt; r\najmer ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; a\n.aara ---&gt; v\naarav ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; a\ntania ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; h\nramph ---&gt; e\namphe ---&gt; r\nmpher ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; a\n.sada ---&gt; b\nsadab ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; h\n..teh ---&gt; m\n.tehm ---&gt; i\ntehmi ---&gt; n\nehmin ---&gt; a\nhmina ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; u\nchotu ---&gt; m\nhotum ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; a\n.kana ---&gt; k\nkanak ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; t\n.swat ---&gt; a\nswata ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; j\n.jaij ---&gt; a\njaija ---&gt; i\naijai ---&gt; r\nijair ---&gt; a\njaira ---&gt; m\nairam ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; r\nishar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; m\n.sukm ---&gt; a\nsukma ---&gt; n\nukman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; d\n.anad ---&gt; i\nanadi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; w\n.jasw ---&gt; i\njaswi ---&gt; n\naswin ---&gt; d\nswind ---&gt; e\nwinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; i\nmanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; u\n.anju ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; n\n.husn ---&gt; o\nhusno ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; k\n..mok ---&gt; a\n.moka ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; p\nmahip ---&gt; a\nahipa ---&gt; l\nhipal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; i\n.jati ---&gt; n\njatin ---&gt; d\natind ---&gt; e\ntinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; n\n.sunn ---&gt; y\nsunny ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; n\ndevan ---&gt; t\nevant ---&gt; i\nvanti ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; a\n.joya ---&gt; a\njoyaa ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; s\n.ghas ---&gt; i\nghasi ---&gt; r\nhasir ---&gt; a\nasira ---&gt; m\nsiram ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; y\n.devy ---&gt; a\ndevya ---&gt; n\nevyan ---&gt; i\nvyani ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; e\n.mome ---&gt; n\nmomen ---&gt; a\nomena ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; u\n..phu ---&gt; l\n.phul ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; m\n.fulm ---&gt; i\nfulmi ---&gt; y\nulmiy ---&gt; a\nlmiya ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; l\nnirml ---&gt; a\nirmla ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; v\nnjeev ---&gt; a\njeeva ---&gt; n\neevan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; z\nahnaz ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; m\nchham ---&gt; o\nhhamo ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; d\n.aard ---&gt; h\naardh ---&gt; n\nardhn ---&gt; a\nrdhna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; j\n.jagj ---&gt; e\njagje ---&gt; e\nagjee ---&gt; t\ngjeet ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; k\n.chak ---&gt; r\nchakr ---&gt; a\nhakra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; r\nvindr ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; i\nshani ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; m\nsukhm ---&gt; i\nukhmi ---&gt; t\nkhmit ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; u\ndeepu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; i\nparsi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; .\n\n\n\nX.shape, X.dtype, Y.shape, Y.dtype\n\n(torch.Size([44325, 5]), torch.int64, torch.Size([44325]), torch.int64)\n\n\n\n# Embedding layer for the context\n\nemb_dim = 4\nemb = torch.nn.Embedding(len(stoi), emb_dim)\n\n\nemb.weight\n\nParameter containing:\ntensor([[ 0.7694, -0.0425,  0.4451, -0.5473],\n        [-1.4513,  2.3222,  0.5003, -1.9549],\n        [-1.2120,  2.3934,  0.3531, -1.4271],\n        [-1.1517, -0.7280, -0.9900, -1.0232],\n        [ 0.5035, -0.0431, -1.4582,  1.7057],\n        [ 1.1632,  0.8851, -0.0941,  0.6955],\n        [ 0.7277,  0.8689, -2.3394, -0.1301],\n        [ 0.6960,  0.9702,  0.6731,  1.3605],\n        [ 0.4484,  0.5055,  0.2673,  0.8483],\n        [-1.9781,  0.9819, -1.0400,  0.1364],\n        [-0.3586,  0.1020, -0.2296,  0.0335],\n        [-0.7084,  0.4625, -0.8382,  0.4765],\n        [-0.1664,  0.9483, -0.8406,  1.2484],\n        [ 2.3739,  1.0939,  0.3967, -1.0831],\n        [ 0.1758,  0.6867, -0.2124, -1.3171],\n        [-1.7160,  0.1922, -0.9100,  0.2590],\n        [-0.6906,  0.6579, -1.8157,  0.7363],\n        [ 0.6869,  1.0605,  0.3869,  2.5261],\n        [-1.7279, -0.5944,  0.6080, -2.3156],\n        [-0.0682, -0.8245, -0.0153,  1.1720],\n        [-0.0647, -1.4478,  0.7775, -0.6562],\n        [ 0.5954,  0.0164, -0.2450, -0.1502],\n        [ 0.0467,  0.4786, -0.7976, -0.5191],\n        [ 0.4017, -1.2129,  1.4015, -0.9391],\n        [-0.9076,  1.1577,  0.6700,  1.2061],\n        [-1.3892, -0.1162, -0.1681,  0.3348],\n        [-0.7747, -1.5393,  0.9535,  0.5241]], requires_grad=True)\n\n\n\nemb.weight.shape\n\ntorch.Size([27, 4])\n\n\n\n# Function to visualize the embedding in 2d space\n\ndef plot_emb(emb, itos, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    for i in range(len(itos)):\n        x, y = emb.weight[i].detach().cpu().numpy()\n        ax.scatter(x, y, color='k')\n        ax.text(x + 0.05, y + 0.05, itos[i])\n    return ax\n\nplot_emb(emb, itos)\n\n\n\n\n\n\n\n\n\nclass NextChar(nn.Module):\n  def __init__(self, block_size, vocab_size, emb_dim, hidden_size):\n    super().__init__()\n    self.emb = nn.Embedding(vocab_size, emb_dim)\n    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n    self.lin2 = nn.Linear(hidden_size, vocab_size)\n\n  def forward(self, x):\n    x = self.emb(x)\n    x = x.view(x.shape[0], -1)\n    x = torch.sin(self.lin1(x))\n    x = self.lin2(x)\n    return x\n    \n\n\n# Generate names from untrained model\n\n\nmodel = NextChar(block_size, len(stoi), emb_dim, 10).to(device)\nmodel = torch.compile(model)\n\ng = torch.Generator()\ng.manual_seed(4000002)\ndef generate_name(model, itos, stoi, block_size, max_len=10):\n    context = [0] * block_size\n    name = ''\n    for i in range(max_len):\n        x = torch.tensor(context).view(1, -1).to(device)\n        y_pred = model(x)\n        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n        ch = itos[ix]\n        if ch == '.':\n            break\n        name += ch\n        context = context[1:] + [ix]\n    return name\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\necxoxdwsrd\nsfeeukszll\nnuprryrzgx\ngkiktjbsj\ngovlmjrzgg\nllqslrttsz\ngw\nuchbnxysaj\nrwgswujgse\nspbohliyft\n\n\n\nfor param_name, param in model.named_parameters():\n    print(param_name, param.shape)\n\n_orig_mod.emb.weight torch.Size([27, 4])\n_orig_mod.lin1.weight torch.Size([10, 20])\n_orig_mod.lin1.bias torch.Size([10])\n_orig_mod.lin2.weight torch.Size([27, 10])\n_orig_mod.lin2.bias torch.Size([27])\n\n\n\n# Train the model\n\nloss_fn = nn.CrossEntropyLoss()\nopt = torch.optim.AdamW(model.parameters(), lr=0.01)\nimport time\n# Mini-batch training\nbatch_size = 4096\nprint_every = 100\nelapsed_time = []\nfor epoch in range(10000):\n    start_time = time.time()\n    for i in range(0, X.shape[0], batch_size):\n        x = X[i:i+batch_size]\n        y = Y[i:i+batch_size]\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    end_time = time.time()\n    elapsed_time.append(end_time - start_time)\n    if epoch % print_every == 0:\n        print(epoch, loss.item())\n\n0 2.9544246196746826\n100 2.1499083042144775\n200 2.136932849884033\n300 2.132302761077881\n400 2.1297848224639893\n500 2.127420425415039\n\n\nKeyboardInterrupt: \n\n\n\n# Visualize the embedding\n\nplot_emb(model.emb, itos)\n\n\n\n\n\n\n\n\n\n# Generate names from trained model\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\nkoyani\nmantit\nsukaaki\nchawen\nshahi\nchajil\nmahibh\nrazag\nanpara\nsabka\n\n\nTuning knobs\n\nEmbedding size\nMLP\nContext length"
  },
  {
    "objectID": "notebooks/Linear Regression Notebook.html",
    "href": "notebooks/Linear Regression Notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "notebooks/perceptron-learning.html",
    "href": "notebooks/perceptron-learning.html",
    "title": "Perceptron learning algorithm",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_and = np.array([0, 0, 0, 1])\ny_or = np.array([0, 1, 1, 1])\ny_xor = np.array([0, 1, 1, 0])\n\n\nclass Perceptron(object):\n    def __init__(self, lr=0.01, iterations=100):\n        self.lr = lr\n        self.iterations = iterations\n        \n    def activation(self, z):\n        ac = np.zeros_like(z)\n        ac[z&gt;0] = 1\n        return ac\n    \n    def fit(self, X, y):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        self.W = np.zeros((X_with_one.shape[1], 1))\n        for i in range(self.iterations):\n            for j in range(len(X)):\n                summation = (X_with_one@self.W).flatten()\n                y_hat = self.activation(summation)\n                err = y - y_hat.flatten() \n                self.W = self.W + (self.lr*err[j]*X_with_one[j]).reshape(*(self.W.shape))\n        \n    def predict(self, X):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        summation = (X_with_one@self.W).flatten()\n        y_hat = self.activation(summation)           \n        return y_hat\n\n    \n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X, y_or)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 1., 1., 1.])\n\n\n\nperceptron.fit(X, y_and)\n\n\nperceptron.W\n\narray([[-0.02],\n       [ 0.02],\n       [ 0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 0., 0., 1.])\n\n\n\nperceptron.fit(X, y_xor)\n\n\nperceptron.W\n\narray([[ 0.01],\n       [-0.01],\n       [ 0.  ]])\n\n\n\nperceptron.predict(X)\n\narray([1., 1., 0., 0.])\n\n\n\nXOR using feature transformation\n\n# Transformation: 1 \n# x1, x2, x1x2\nX_xor_1 = np.append(X, (X[:, 0]*X[:, 1]).reshape(-1, 1), axis=1)\n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X_xor_1, y_xor)\n\n\nperceptron.W\n\narray([[ 0.  ],\n       [ 0.01],\n       [ 0.01],\n       [-0.04]])\n\n\n\nnp.allclose(perceptron.predict(X_xor_1), y_xor)\n\nTrue\n\n\n\n(X[:, 0]*X[:, 1]).reshape(-1, 1)\n\narray([[0],\n       [0],\n       [0],\n       [1]])\n\n\n\n# Transformation: 1 \n# x1, x2, x1x2\n\n\nX_xor_2 = np.array([(1-X[:, 0])*X[:,1], (1-X[:, 1])*X[:,0]]).T\n\n\nperceptron = Perceptron()\nperceptron.fit(X_xor_2, y_xor)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])"
  },
  {
    "objectID": "notebooks/classes-trees.html",
    "href": "notebooks/classes-trees.html",
    "title": "Basics of Classes and Plotting Trees",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport graphviz\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nclass TreeNode:\n    def __init__(self, name, value=None, shape='rectangle'):\n        self.name = name\n        self.value = value\n        self.children = []\n        self.shape = shape\n\n    def add_child(self, child_node):\n        self.children.append(child_node)\n\n    def display_tree_text(self, level=0):\n        indent = \"  \" * level\n        print(f\"{indent}|- {self.name}: {self.value}\")\n        for child in self.children:\n            child.display_tree_text(level + 1)\n\n    def display_tree_graphviz(self, dot=None, parent_name=None, graph=None):\n        if graph is None:\n            graph = graphviz.Digraph(format='png')\n        graph.node(str(id(self)), str(self.name), shape=self.shape)\n\n        if parent_name is not None:\n            graph.edge(str(id(parent_name)), str(id(self)))\n\n        for child in self.children:\n            child.display_tree_graphviz(dot, self, graph)\n\n        return graph\n\n    def display_tree_directly(self):\n        graph = self.display_tree_graphviz()\n        src = graph.source\n        display(graphviz.Source(src, format='png'))\n\n\n# Creating nodes\nroot = TreeNode(\"Root\")\nchild1 = TreeNode(\"Child 1\")\nchild2 = TreeNode(\"Child 2\")\nchild3 = TreeNode(\"Child 3\")\n\n# Building the tree structure\nroot.add_child(child1)\nroot.add_child(child2)\nchild2.add_child(child3)\n\n\n# Displaying the tree in text format\nroot.display_tree_text()\n\n|- Root: None\n  |- Child 1: None\n  |- Child 2: None\n    |- Child 3: None\n\n\n\ngraph = root.display_tree_graphviz()\ngraph\n\n\n\n\n\n\n\n\n\nclass DecisionTreeNode:\n    def __init__(self, feature, threshold, decision=None, left=None, right=None, shape='box'):\n        self.feature = feature\n        self.threshold = threshold\n        self.decision = decision\n        self.left = left\n        self.right = right\n        self.shape = shape\n\n    def display_tree_graphviz(self, dot=None, parent_name=None, graph=None, edge_label=None):\n        if graph is None:\n            graph = graphviz.Digraph(format='png')\n\n        node_label = self.feature\n        \n        if self.threshold is not None:\n            node_label += f\" &lt;= {self.threshold}\"\n        \n        if self.decision is not None:\n            node_label += f\"\\nDecision: {self.decision}\"\n        \n        graph.node(str(id(self)), node_label, shape=self.shape)\n\n        if parent_name is not None:\n            if edge_label is not None:\n                graph.edge(str(id(parent_name)), str(id(self)), label=edge_label)\n            else:\n                graph.edge(str(id(parent_name)), str(id(self)))\n\n        if self.left is not None:\n            self.left.display_tree_graphviz(dot, self, graph, edge_label=\"True\")\n        if self.right is not None:\n            self.right.display_tree_graphviz(dot, self, graph, edge_label=\"False\")\n\n        return graph\n\n\nroot = DecisionTreeNode(\"Feature A\", 5.0, decision=None)\nleft_child = DecisionTreeNode(\"Feature B\", 3.0, decision=None)\nright_child = DecisionTreeNode(\"Feature C\", 8.0, decision=None)\nroot.left = left_child\nroot.right = right_child\n\nleft_left = DecisionTreeNode(\"\", None, decision = 20.0)\nleft_right = DecisionTreeNode(\"\", None, decision = 10.0)\n\nleft_child.left = left_left\nleft_child.right = left_right\n\nright_left = DecisionTreeNode(\"\", None, decision = 30.0)\nright_right = DecisionTreeNode(\"\", None, decision = 40.0)\n\nright_child.left = right_left\nright_child.right = right_right\n\n\nroot.display_tree_graphviz()"
  },
  {
    "objectID": "notebooks/boosting-explanation.html",
    "href": "notebooks/boosting-explanation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\n\nx = np.linspace(0.04, 0.51, 1000)\n\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height &gt; MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': True,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=3)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\n\nText(0, 0.5, '$\\\\alpha_m$')\n\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=2)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/ensemble/alpha-boosting.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, np.exp(0.5*np.log((1-x)/x)), color='r', linewidth=2, label=r'$e^{\\alpha_m}$ ')\nplt.plot(x, np.exp(-0.5*np.log((1-x)/x)), color='g', linewidth=2, label=r'$e^{-\\alpha_m}$')\n\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(\"Weight Multiplier\")\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/ensemble/alpha-boosting-weight.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/Sklearn_on_GPU.html",
    "href": "notebooks/Sklearn_on_GPU.html",
    "title": "Sklearn on GPU",
    "section": "",
    "text": "https://scikit-learn.org/stable/modules/array_api.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\nhttps://github.com/data-apis/array-api-compat\nhttps://labs.quansight.org/blog/array-api-support-scikit-learn"
  },
  {
    "objectID": "notebooks/Sklearn_on_GPU.html#references",
    "href": "notebooks/Sklearn_on_GPU.html#references",
    "title": "Sklearn on GPU",
    "section": "",
    "text": "https://scikit-learn.org/stable/modules/array_api.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\nhttps://github.com/data-apis/array-api-compat\nhttps://labs.quansight.org/blog/array-api-support-scikit-learn"
  },
  {
    "objectID": "notebooks/autoregressive_model.html",
    "href": "notebooks/autoregressive_model.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "%pip install numpy pandas matplotlib statsmodels\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\nRequirement already satisfied: patsy&gt;=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy&gt;=0.5.4-&gt;statsmodels) (1.16.0)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Interactive widget\nfrom ipywidgets import interact\n\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n# Download CO2 data from NOAA\nurl = 'https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv'\n\nnames = 'year,month,decimal date,average,deseasonalized,ndays,sdev,unc'.split(',')\n\n# no index\ndf = pd.read_csv(url, skiprows=72, names=names, index_col=False)\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n# Create X and y\n\n# X = months since first measurement\n\nX = np.array(range(len(df)))\ny = df.average.values\nplt.plot(X, y)\nplt.xlabel('Months since first measurement')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')"
  },
  {
    "objectID": "notebooks/autoregressive_model.html#autocorrelation-function-acf",
    "href": "notebooks/autoregressive_model.html#autocorrelation-function-acf",
    "title": "Machine Learning Resources",
    "section": "Autocorrelation Function (ACF):",
    "text": "Autocorrelation Function (ACF):\nThe Autocorrelation Function (ACF) measures the correlation between a time series and its lagged values. It quantifies how well the previous observations at different lags explain the current observation. ## Partial Autocorrelation Function (PACF):\nThe Partial Autocorrelation Function (PACF) measures the correlation between a time series and its lagged values, while controlling for the intermediate lags. It helps identify the direct relationship between two time points, excluding the influence of other time points in between.\n\ny_co = pd.Series(y)\n\n\n\n# Plot the autocorrelation function (ACF) using statsmodels\nplt.figure(figsize=(10, 6))\nplot_acf(y_co, lags=80, title='Autocorrelation Plot')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(y_co, lags=50)\nplt.show()\n\n\n\n\n\n\n\n\n\nAuto Regressive Model\n\nWe convert the CO2 level time sequence into an input feature matrix and the corresponding output vector by setting a lag value. A sliding window of lag length will be used across the sequence to create the feature matrix and the output.\n\n# making the features using previous values according to lag\ndef make_dataset(y, X, lag):\n    X_train = []\n    y_train = []\n    months = []\n    for i in range(len(y)-lag):\n        row = y[i:i+lag]\n        y_train.append(y[i+lag])\n        X_train.append(row)\n\n        month_value = X[i+lag]\n        months.append(month_value)\n\n    X_train = pd.DataFrame(X_train)\n    y_train = pd.Series(y_train)\n    return X_train, y_train, months\n\n\n# adding the month value as a feature\ndef make_dataset_with_month(y, X, lag):\n    X_train = []\n    y_train = []\n    months_train = []\n\n    for i in range(len(y)-lag):\n        row = y[i:i+lag]\n        y_train.append(y[i+lag])\n        X_train.append(row)\n\n        month_value = X[i+lag]\n        months_train.append(month_value)\n\n    X_train = pd.DataFrame(X_train)\n    X_train[10] = months_train\n    y_train = pd.Series(y_train)\n\n    return X_train, y_train, months_train\n\n\n# example run of the model using a lag of 10\nlag = 10\nX_train, y_train, months_train = make_dataset(train_data, X_train_data, lag)\nreg = LinearRegression().fit(X_train, y_train)\nX_test, y_test, months_test = make_dataset(test_data,X_test_data, lag)\ny_hat = reg.predict(X_test)\nmse = mean_squared_error(y_test, y_hat)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\n\nMean Squared Error (MSE): 0.2099165801507042\n\n\n\n# Ploting the predicted output for lag=10 for the test sequence\nplt.plot(months_test, y_test, label='Actual Values (y_test)', marker='o')\nplt.plot(months_test, y_hat, label='Predicted Values (y_hat)', marker='o')\nplt.xlabel('Months Since first measurement')\nplt.ylabel('CO2 levels')\nplt.title('Actual vs Predicted Values')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating a custom class for training and testing the AR model for different lag values\nclass AR_model:\n\n  def __init__(self, lag, add_month = False):\n    self.reg = LinearRegression()\n    self.lag = lag\n    self.add_month = add_month\n\n  def train_AR_model(self, train_data, X_train_data):\n\n    if(self.add_month):\n      X_train, y_train, month_train = make_dataset_with_month(train_data, X_train_data, self.lag)\n      self.reg.fit(X_train, y_train)\n\n    else:\n      X_train, y_train, months_train = make_dataset(train_data, X_train_data, self.lag)\n      self.reg.fit(X_train, y_train)\n\n    return self.reg\n\n  def test_AR_model(self, test_data,X_test_data):\n    if(self.add_month):\n      X_test, y_test, month_test = make_dataset_with_month(test_data,X_test_data, self.lag)\n      y_hat = self.reg.predict(X_test)\n      mse = mean_squared_error(y_test, y_hat)\n\n\n    else:\n      X_test, y_test, months_test = make_dataset(test_data,X_test_data, self.lag)\n      y_hat = self.reg.predict(X_test)\n      mse = mean_squared_error(y_test, y_hat)\n\n    return mse\n\n\n\n# Finding the variation in the mse values with increasing lag\n\nlag_values = [1, 5, 10, 15, 20, 30, 35, 40, 50, 55, 65]\n\n# Compairing the accuracy of the models with and without the month feature\nloss_table_1 = []\nloss_table_2 = []\n\nfor lag in lag_values:\n    model_1 = AR_model(lag)\n    model_2 = AR_model(lag, True)\n    reg1 = model_1.train_AR_model(train_data, X_train_data)\n    reg2 = model_2.train_AR_model(train_data, X_train_data)\n\n    mse1 = model_1.test_AR_model(test_data,X_test_data)\n    mse2 = model_2.test_AR_model(test_data,X_test_data)\n    loss_table_1.append((lag, mse1))\n    loss_table_2.append((lag, mse2))\n\n\n# Visualize the loss values\nlags, losses1 = zip(*loss_table_1)\nlags, losses2 = zip(*loss_table_2)\nplt.plot(lags, losses1,label='Without month as a feature', marker='o')\nplt.plot(lags, losses2,label='With month as a feature', marker='o')\nplt.xlabel('Lag')\nplt.ylabel('Mean Squared Error (MSE)')\nplt.title('Loss vs Lag')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUsing a more complicated dataset : Air passengers dataset\n\n# downloading and plotting the dataset\nurl_ap = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\ndf_ap = pd.read_csv(url_ap)\ndf_ap['Month'] = pd.to_datetime(df_ap['Month'])\ndf_ap.set_index('Month', inplace=True)\ndf = df_ap\nplt.figure(figsize=(10, 6))\nplt.plot(np.array(range(len(df_ap))), df_ap['Passengers'], label='Air Passengers')\nplt.title('Air Passengers Over Time')\nplt.xlabel('Months')\nplt.ylabel('Number of Passengers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Extract features (X) and target variable (y)\nX_ap = np.array(range(len(df_ap)))\ny_ap= df_ap['Passengers'].values.flatten().tolist()\n\nsplit_index_ap = int(0.8 * len(y_ap))\ntrain_data_ap = y_ap[:split_index_ap]\nX_train_data_ap = X_ap[:split_index_ap]\ntest_data_ap = y_ap[split_index_ap:]\nX_test_data_ap = X_ap[split_index_ap:]\n\n\ny_co = pd.Series(y_ap)\n\n\n# Plot the autocorrelation function (ACF) using statsmodels\nplt.figure(figsize=(10, 6))\nplot_acf(y_co, lags=80, title='Autocorrelation Plot')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(y_co, lags=50)\nplt.show()\n\n\n\n\n\n\n\n\n\n# example run of the model using a lag of 20\nlag_ap = 20\nX_train_ap, y_train_ap, months_train_ap = make_dataset(train_data_ap, X_train_data_ap, lag_ap)\nreg_ap = LinearRegression().fit(X_train_ap, y_train_ap)\nX_test_ap, y_test_ap, months_test_ap = make_dataset(test_data_ap,X_test_data_ap, lag_ap)\ny_hat_ap = reg_ap.predict(X_test_ap)\nrmse_ap = np.sqrt(mean_squared_error(y_test_ap, y_hat_ap))\n\nprint(f\"Mean Squared Error (RMSE): {rmse_ap}\")\n\nMean Squared Error (RMSE): 18.798515889301697\n\n\n\n# Ploting the predicted output for lag=10 for the test sequence\nplt.plot(months_test_ap, y_test_ap, label='Actual Values (y_test)', marker='o')\nplt.plot(months_test_ap, y_hat_ap, label='Predicted Values (y_hat)', marker='o')\nplt.xlabel('Month')\nplt.ylabel('Number of passangers')\nplt.title('Actual vs Predicted Values')\n\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nComparison with another time series model : ConvLSTM\n\n# univariate convlstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import ConvLSTM2D\n\n\nlag = 20\n# split into samples\nX, y,_ = make_dataset(train_data_ap, X_train_data_ap, lag)\n# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\nn_features = 1\nn_seq = 4\nn_steps = 5\nX = array(X)\nX = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\nmodel.add(Flatten())\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nX_test, y_test, months_test = make_dataset(test_data_ap, X_test_data_ap, lag)\nX_test = array(X_test)\ny_test = array(y_test)\nX_test = X_test.reshape((X_test.shape[0], n_seq, 1, n_steps, n_features))\ny_hat_lstm = model.predict(X_test)\ny_hat_lstm = y_hat_lstm.flatten()\n\n1/1 [==============================] - 1s 589ms/step\n\n\n\n# Ploting the predicted output for lag=10 for the test sequence\nplt.plot(months_test_ap, y_test_ap, label='Actual Values (y_test)', marker='o')\nplt.plot(months_test_ap, y_hat_ap, label='Predicted Values : ARM', marker='o')\nplt.plot(months_test_ap, y_hat_lstm, label='Predicted Values : ConvLSTM', marker='o')\nplt.xlabel('Month')\nplt.ylabel('Number of passangers')\nplt.title('ARM vs ConvLSTM')\n\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating a class to train and test the ConvLSTM model\n\nclass custom_ConvLSTM:\n\n  def __init__(self, lag,n_seq, n_steps, n_features=1):\n    self.lag = lag\n    self.n_features = n_features\n    self.n_steps = n_steps\n    self.n_seq = n_seq\n    self.model = Sequential()\n    self.model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n    self.model.add(Flatten())\n    self.model.add(Dense(1))\n    self.model.compile(optimizer='adam', loss='mse')\n\n  def train(self, train_data, X_train_data):\n    X_train, y_train, months_train = make_dataset(train_data, X_train_data, self.lag)\n    X_train = array(X_train)\n    y_train = array(y_train)\n    X_train = X_train.reshape((X_train.shape[0], self.n_seq, 1, self.n_steps, self.n_features))\n    self.model.fit(X_train, y_train, epochs=500, verbose=0)\n\n  def test(self, test_data, X_test_data):\n    X_test, y_test, months_test = make_dataset(test_data, X_test_data, self.lag)\n    X_test = array(X_test)\n    y_test = array(y_test)\n    X_test = X_test.reshape((X_test.shape[0], self.n_seq, 1, self.n_steps, self.n_features))\n    y_hat_lstm = self.model.predict(X_test)\n    y_hat_lstm = y_hat_lstm.flatten()\n\n    mse_lstm = mean_squared_error(y_test, y_hat_lstm)\n    return mse_lstm\n\n\n\nimport random\n\ndef random_split(original_number):\n    # Find factors of the original number\n    factors = [i for i in range(2, original_number) if original_number % i == 0]\n\n    # Select one factor randomly\n    factor = random.choice(factors)\n\n    # Calculate the other factor\n    other_factor = original_number // factor\n\n    return factor, other_factor"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#imports",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#imports",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#convention",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#convention",
    "title": "Introduction to Neural Networks",
    "section": "Convention",
    "text": "Convention\n\nn0 = 3\nn1 = 2\nlayer = nn.Linear(n0, n1)\nlayer\n\nLinear(in_features=3, out_features=2, bias=True)\n\n\n\nlayer.weight.shape\n\ntorch.Size([2, 3])\n\n\n\nlayer.bias.shape\n\ntorch.Size([2])\n\n\n\nfor i in range(n0):\n    print(layer.weight[0, i])\n\ntensor(-0.0279, grad_fn=&lt;SelectBackward0&gt;)\ntensor(0.0743, grad_fn=&lt;SelectBackward0&gt;)\ntensor(-0.1339, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nA 2-layer network\n\nmlp_2layers = nn.Sequential(\n    nn.Linear(4, 3),\n    nn.ReLU(),\n    nn.Linear(3, 2),\n    nn.ReLU(),\n    nn.Linear(2, 1)\n)\n\n\nparams = dict(mlp_2layers.named_parameters())\n{name: param.shape for name, param in params.items()}\n\n{'0.weight': torch.Size([3, 4]),\n '0.bias': torch.Size([3]),\n '2.weight': torch.Size([2, 3]),\n '2.bias': torch.Size([2]),\n '4.weight': torch.Size([1, 2]),\n '4.bias': torch.Size([1])}"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#vectorization-with-torch.vmap",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#vectorization-with-torch.vmap",
    "title": "Introduction to Neural Networks",
    "section": "Vectorization with torch.vmap",
    "text": "Vectorization with torch.vmap\n\nfrom jaxtyping import Float\nfrom torch import Tensor\nfrom beartype import beartype\n\n\nQuick intro to jaxtyping\n\nScalars\n\nscalar_type = Float[Tensor, \"\"]\n\n\nscalar = torch.tensor(1.0)\nscalar.shape\n\ntorch.Size([])\n\n\n\nisinstance(scalar, scalar_type)\n\nTrue\n\n\n\nnon_scalar = torch.tensor([1.0])\nnon_scalar.shape\n\ntorch.Size([1])\n\n\n\nisinstance(non_scalar, scalar_type)\n\nFalse\n\n\n\n\nVectors\n\nvector_type = Float[Tensor, \"n0\"]\n\n\nisinstance(non_scalar, vector_type)\n\nTrue\n\n\n\nvector = torch.tensor([1.0, 2.0])\nvector.shape\n\ntorch.Size([2])\n\n\n\nisinstance(vector, vector_type)\n\nTrue\n\n\n\n\nMatrices\n\nmatrix_type = Float[Tensor, \"n0 n1\"]\n\n\nisinstance(vector, matrix_type)\n\nFalse\n\n\n\nmatrix = torch.tensor([[1.0, 2.0]])\nmatrix.shape\n\ntorch.Size([1, 2])\n\n\n\nisinstance(matrix, matrix_type)\n\nTrue\n\n\n\nanother_matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nanother_matrix.shape\n\ntorch.Size([2, 3])\n\n\n\nisinstance(another_matrix, matrix_type)\n\nTrue\n\n\n\n\nTensors\n\ntensor_type = Float[Tensor, \"n0 n1 n2\"]\n\n\ntensor = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\ntensor.shape\n\ntorch.Size([2, 2, 2])\n\n\n\nisinstance(tensor, tensor_type)\n\nTrue\n\n\n\n\n\nQuick intro to beartype\n\ndef call_my_name(name):\n    return f\"Hello, {name}!\"\n\n\ncall_my_name(\"John\")\n\n'Hello, John!'\n\n\n\ncall_my_name(123)\n\n'Hello, 123!'\n\n\n\n@beartype\ndef secured_call_my_name(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\n\nsecured_call_my_name(\"John\")\n\n'Hello, John!'\n\n\n\nsecured_call_my_name(123)\n\nBeartypeCallHintParamViolation: Function __main__.secured_call_my_name() parameter name=123 violates type hint &lt;class 'str'&gt;, as int 123 not instance of str.\n\n\n\n\nVectorization\nOn which dimensions should we apply the vectorization? - Current layers neurons size - Number of examples\n\nn = 50\na = torch.rand(n, n0)\na.shape\n\ntorch.Size([50, 3])\n\n\n\nactivation = F.relu\n\n@beartype\ndef forward(a: Float[Tensor, \"n0\"], w: Float[Tensor, \"n0\"], b: Float[Tensor, \"\"]) -&gt; Float[Tensor, \"\"]:\n    z = (a * w).sum() + b  # () + () -&gt; ()\n    a = activation(z)  # () -&gt; ()\n    return a  # ()\n\n\ndummy_a = torch.rand(n0)\ndummy_w = torch.rand(n0)\ndummy_b = torch.rand(())\nprint(dummy_a.shape, dummy_w.shape, dummy_b.shape)\n\ntorch.Size([3]) torch.Size([3]) torch.Size([])\n\n\n\nforward(dummy_a, dummy_w, dummy_b).shape\n\ntorch.Size([])\n\n\n\nforward(a[0], layer.weight[0], layer.bias[0])\n\ntensor(0.1544, grad_fn=&lt;ReluBackward0&gt;)\n\n\n\nVectorization over current layers neurons size\n\n\n\ninput\nshape in forward\nshape in vectorized forward\n\n\n\n\na\n[n0=3]\n[n0=3]\n\n\nw\n[n0=3]\n[n1=2, n0=3]\n\n\nb\n[]\n[n1=2]\n\n\noutput\n[]\n[n1=2]\n\n\n\n\nv1_forward = torch.vmap(forward, in_dims=(None, 0, 0), out_dims=0)\n\n\nlayer.weight.shape\n\ntorch.Size([2, 3])\n\n\n\nlayer.bias.shape\n\ntorch.Size([2])\n\n\n\nout = v1_forward(a[0], layer.weight, layer.bias)\nout.shape\n\ntorch.Size([2])\n\n\n\n\nVectorization over number of examples\n\n\n\ninput\nshape in forward\nshape in vectorized forward\n\n\n\n\na\n[n0=3]\n[n=50, n0=3]\n\n\nw\n[n1=2, n0=3]\n[n1=2, n0=3]\n\n\nb\n[n1=2]\n[n1=2]\n\n\noutput\n[n1=2]\n[n=50, n1=2]\n\n\n\n\nv2_forward = torch.vmap(v1_forward, in_dims=(0, None, None), out_dims=0)\n\n\nfinal_out = v2_forward(a, layer.weight, layer.bias)\nfinal_out.shape\n\ntorch.Size([50, 2])\n\n\n\n\nComparing with torch model forward pass\n\nlayer_out = F.relu(layer(a))\ntorch.allclose(final_out, layer_out)\n\nTrue"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#xor-example",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#xor-example",
    "title": "Introduction to Neural Networks",
    "section": "XOR example",
    "text": "XOR example\n\nDefine inputs, outputs, weights and biases\n\nxor_x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\nprint(xor_x)\nprint(xor_x.shape)\n\ntensor([[0., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 1.]])\ntorch.Size([4, 2])\n\n\n\nxor_y = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\nprint(xor_y)\nprint(xor_y.shape)\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]])\ntorch.Size([4, 1])\n\n\n\nW_1 = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\nb_1 = torch.tensor([0.0, -1.0])\nprint(W_1)\nprint(b_1)\nprint(W_1.shape, b_1.shape)\n\ntensor([[1., 1.],\n        [1., 1.]])\ntensor([ 0., -1.])\ntorch.Size([2, 2]) torch.Size([2])\n\n\n\nW_2 = torch.tensor([[1.0, -2.0]])\nb_2 = torch.tensor([0.0])\nprint(W_2)\nprint(b_2)\nprint(W_2.shape, b_2.shape)\n\ntensor([[ 1., -2.]])\ntensor([0.])\ntorch.Size([1, 2]) torch.Size([1])\n\n\n\n\nForward pass\n\na1 = v2_forward(xor_x, W_1, b_1)\na1\n\ntensor([[0., 0.],\n        [1., 0.],\n        [1., 0.],\n        [2., 1.]])\n\n\n\na2 = v2_forward(a1, W_2, b_2)\na2\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]])\n\n\n\n\nForward pass with torch neural network\n\nmodel = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 1))\nprint(model[0])\nprint(model[1])\nprint(model[2])\n\nLinear(in_features=2, out_features=2, bias=True)\nReLU()\nLinear(in_features=2, out_features=1, bias=True)\n\n\n\nmodel[0].weight.data = W_1\nmodel[0].bias.data = b_1\nmodel[2].weight.data = W_2\nmodel[2].bias.data = b_2\n\n\nmodel_out = model(xor_x)\nmodel_out\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]], grad_fn=&lt;AddmmBackward0&gt;)"
  },
  {
    "objectID": "notebooks/basis.html",
    "href": "notebooks/basis.html",
    "title": "Basis Expansion in Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n# Interactive widget\nfrom ipywidgets import interact\n\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\n# Download CO2 data from NOAA\nurl = 'https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv'\n\nnames = 'year,month,decimal date,average,deseasonalized,ndays,sdev,unc'.split(',')\n\n# no index\ndf = pd.read_csv(url, skiprows=72, names=names, index_col=False)\n\n\ndf\n\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal date\naverage\ndeseasonalized\nndays\nsdev\nunc\n\n\n\n\n0\n1960\n10\n1960.7896\n313.83\n316.83\n-1\n-9.99\n-0.99\n\n\n1\n1960\n11\n1960.8743\n315.00\n316.88\n-1\n-9.99\n-0.99\n\n\n2\n1960\n12\n1960.9563\n316.19\n316.96\n-1\n-9.99\n-0.99\n\n\n3\n1961\n1\n1961.0411\n316.89\n316.86\n-1\n-9.99\n-0.99\n\n\n4\n1961\n2\n1961.1260\n317.70\n317.08\n-1\n-9.99\n-0.99\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n754\n2023\n8\n2023.6250\n419.68\n421.57\n21\n0.45\n0.19\n\n\n755\n2023\n9\n2023.7083\n418.51\n421.96\n18\n0.30\n0.14\n\n\n756\n2023\n10\n2023.7917\n418.82\n422.11\n27\n0.47\n0.17\n\n\n757\n2023\n11\n2023.8750\n420.46\n422.43\n21\n0.91\n0.38\n\n\n758\n2023\n12\n2023.9583\n421.86\n422.58\n20\n0.69\n0.29\n\n\n\n\n759 rows  8 columns\n\n\n\n\n\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\n/tmp/ipykernel_3219587/1556785423.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  df.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\n\n\ndf.average.plot(figsize=(6, 4), title='CO2 Levels at Mauna Loa Observatory')\nplt.xlabel('Year')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')\n\n\n\n\n\n\n\n\n\n\n# Create X and y\n\n# X = months since first measurement\nX = np.array(range(len(df)))\ny = df.average.values\n\n\nplt.plot(X, y)\nplt.xlabel('Months since first measurement')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')\n\n\n\n\n\n\n\n\n\n\n# Normalize X and y\n\ns1 = StandardScaler()\ns2 = StandardScaler()\n\nX_norm = s1.fit_transform(X.reshape(-1, 1))\ny_norm = s2.fit_transform(y.reshape(-1, 1))\n\n\nX_norm.mean(), X_norm.std()\n\n(0.0, 1.0)\n\n\n\ndf = pd.DataFrame({\"x\":X.flatten(), \"transformed\":X_norm.flatten()})\ndf\n\n\n\n\n\n\n\n\n\nx\ntransformed\n\n\n\n\n0\n0\n-1.729770\n\n\n1\n1\n-1.725206\n\n\n2\n2\n-1.720642\n\n\n3\n3\n-1.716078\n\n\n4\n4\n-1.711514\n\n\n...\n...\n...\n\n\n754\n754\n1.711514\n\n\n755\n755\n1.716078\n\n\n756\n756\n1.720642\n\n\n757\n757\n1.725206\n\n\n758\n758\n1.729770\n\n\n\n\n759 rows  2 columns\n\n\n\n\n\ndf[\"re-transformed\"] = s1.inverse_transform(df[\"transformed\"].values.reshape(-1, 1))\ndf\n\n\n\n\n\n\n\n\n\nx\ntransformed\nre-transformed\n\n\n\n\n0\n0\n-1.729770\n0.0\n\n\n1\n1\n-1.725206\n1.0\n\n\n2\n2\n-1.720642\n2.0\n\n\n3\n3\n-1.716078\n3.0\n\n\n4\n4\n-1.711514\n4.0\n\n\n...\n...\n...\n...\n\n\n754\n754\n1.711514\n754.0\n\n\n755\n755\n1.716078\n755.0\n\n\n756\n756\n1.720642\n756.0\n\n\n757\n757\n1.725206\n757.0\n\n\n758\n758\n1.729770\n758.0\n\n\n\n\n759 rows  3 columns\n\n\n\n\n\ndf.mean()\n\nx              379.0\ntransformed      0.0\ndtype: float64\n\n\n\ndf.std()\n\nx              219.248717\ntransformed      1.000659\ndtype: float64\n\n\n\nx_test = np.array([800])\ns1.transform(x_test.reshape(-1, 1))\n\narray([[1.92145988]])\n\n\n\ny_norm.mean(), y_norm.std()\n\n(1.647635329298783e-15, 1.0)\n\n\n\nplt.plot(X_norm, y_norm)\nplt.xlabel('(Normalized) Months since first measurement')\nplt.ylabel('(Normalized) CO2 Level')\n\nText(0, 0.5, '(Normalized) CO2 Level')\n\n\n\n\n\n\n\n\n\n\nTask 1: Interpolation\n\nnp.random.seed(42)\ntrain_idx = np.random.choice(range(len(X_norm)), size=int(len(X_norm) * 0.7), replace=False)\ntest_idx = list(set(range(len(X_norm))) - set(train_idx))\n\nX_train = X[train_idx]\ny_train = y[train_idx]\n\nX_test = X[test_idx]\ny_test = y[test_idx]\n\nX_norm_train = X_norm[train_idx]\ny_norm_train = y_norm[train_idx]\n\nX_norm_test = X_norm[test_idx]\ny_norm_test = y_norm[test_idx]\n\n\nplt.plot(X_norm_train, y_norm_train, 'o', label='train',markersize=1)\nplt.plot(X_norm_test, y_norm_test, 'o', label='test', ms=3)\nplt.xlabel('(Normalized) Months since first measurement')\nplt.ylabel('(Normalized) CO2 Level')\nplt.legend()\n\n\n\n\n\n\n\n\n\nerrors= {}\nX_lin_1d = np.linspace(X_norm.min(), X_norm.max(), 100).reshape(-1, 1)\n\n\n\nModel 1: Vanilla Linear Regression\n\ndef plot_fit_predict(model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n    model.fit(X_norm_train, y_norm_train)\n\n    y_hat_train = model.predict(X_norm_train).reshape(-1, 1)\n    y_hat_test = model.predict(X_norm_test).reshape(-1, 1)\n\n    # Transform back to original scale\n    y_hat_train = s2.inverse_transform(y_hat_train)\n    y_hat_test = s2.inverse_transform(y_hat_test)\n\n    y_hat_lin = s2.inverse_transform(model.predict(X_lin).reshape(-1, 1))\n\n    errors[title] = {\"train\": mean_squared_error(y_train, y_hat_train),\n                     \"test\": mean_squared_error(y_test, y_hat_test)}\n\n    if plot:\n        plt.plot(X_train, y_train, 'o', label='train', markersize=1)\n        plt.plot(X_test, y_test, 'o', label='test', ms=3)\n        plt.plot(s1.inverse_transform(X_lin_1d), y_hat_lin, label='model')\n        plt.xlabel('Months since first measurement')\n        plt.ylabel('CO2 Levels')\n        plt.legend()\n        plt.title('{}\\n Train MSE: {:.2f} | Test MSE: {:.2f}'.format(title, errors[title][\"train\"], errors[title][\"test\"]))\n\n    return errors[title]\n\n\nmodel = LinearRegression()\nplot_fit_predict(model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Linear Regression\")\n\n{'train': 20.261366262795633, 'test': 19.19972185975937}\n\n\n\n\n\n\n\n\n\n\n\nMLP\n\n# use sk-learn for MLP\nmlp_model = MLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter = 10000)\n\nplot_fit_predict(mlp_model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"MLP Regression\")\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1625: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n{'train': 4.752028323119785, 'test': 4.665698336426404}\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression of degree d\n\ndef create_poly_features(X, d):\n    \"\"\"\n    X is (N, 1) array\n    d is degree of polynomial\n    returns normalized polynomial features of X\n    \"\"\"\n    \n    X_poly = np.zeros((len(X), d))\n    X_poly[:, 0] = X[:, 0]\n    for i in range(1, d):\n        X_poly[:, i] = X[:, 0] ** (i + 1)\n    \n    # Normalize each column\n    X_poly = StandardScaler().fit_transform(X_poly)\n    return X_poly\n\n\nxs = np.linspace(-5, 5, 50).reshape(-1, 1)\npoly_f = create_poly_features(xs, 3)\nfor i in range(3):\n    plt.plot(xs, poly_f[:, i], label='x^{}'.format(i+1))\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef show_poly_features(degree):\n    X_poly = create_poly_features(X_norm, degree)\n    plt.plot(X_norm, X_poly)\n    plt.xlabel('X')\n    plt.ylabel('Polynomial Features')\n    plt.title('Degree {}'.format(degree))\n\nshow_poly_features(2)\n\n\n\n\n\n\n\n\n\ninteract(show_poly_features, degree=(1, 10, 1))\n\n\n\n\n&lt;function __main__.show_poly_features(degree)&gt;\n\n\n\nmodel2 = LinearRegression()\ndegree = 4\nXf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\nXf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\nX_lin_poly = create_poly_features(X_lin_1d, degree)\n\nplot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Polynomial Regression (d={degree})\")\n\n{'train': 4.956984807734834, 'test': 6.3743420617353515}\n\n\n\n\n\n\n\n\n\n\nmodel2.coef_, model2.intercept_\n\n(array([[0.97834069, 0.09963612, 0.01969948, 0.0283343 ]]),\n array([-0.01122412]))\n\n\n\nX_lin_poly.shape\n\n(100, 4)\n\n\n\nX_lin_poly.shape\n\n(100, 4)\n\n\n\nmodel2.coef_.shape\n\n(1, 4)\n\n\n\nX_lin_1d.shape\n\n(100, 1)\n\n\n\nplt.plot(X_lin_1d, model2.intercept_.repeat(len(X_lin_1d)), label='Degree 0')\nplt.plot(X_lin_1d, X_lin_poly[:, 0:1]@model2.coef_[:, 0], label='Degree 1')\nplt.plot(X_lin_1d, X_lin_poly[:, 1:2]@model2.coef_[:, 1], label='Degree 2')\nplt.plot(X_lin_1d, X_lin_poly[:, 2:3]@model2.coef_[:, 2], label='Degree 3')\nplt.plot(X_lin_1d, X_lin_poly[:, 3:4]@model2.coef_[:, 3], label='Degree 4')\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef show_additive(model, X_lin_1d, max_degree):\n    ys = model.intercept_.repeat(len(X_lin_1d))\n    #plt.fill_between(X_lin_1d.squeeze(), s2.inverse_transform(ys.reshape(-1, 1)).squeeze(), alpha=0.1)\n    print(ys.shape, X_lin_1d.shape)\n    label = '{:0.2f}x'.format(model.intercept_[0])\n    \n    for i in range(1, max_degree + 1):\n        yd = X_lin_poly[:, i-1:i]@model.coef_[:, i-1]\n        ys = ys + yd\n        label += ' + {:0.2f} x^{}'.format(model.coef_[:, i-1][0], i)\n    ys = s2.inverse_transform(ys.reshape(-1, 1))\n    plt.plot(X_lin_1d, ys, label = label)\n    plt.plot(X_norm_train, y_train, 'o', label='train', markersize=1)\n    plt.legend()\n\nshow_additive(model2, X_lin_1d, 3)\n\n(100,) (100, 1)\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact, fixed\nm = model2\ninteract(show_additive, model=fixed(m), X_lin_1d=fixed(X_lin_1d), max_degree=(1, len(m.coef_[0]), 1))\n\n\n\n\n&lt;function __main__.show_additive(model, X_lin_1d, max_degree)&gt;\n\n\n\nfor degree in range(1, 10):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Polynomial Regression (d={degree})\", plot=False)\n\n\nerrors_df = pd.DataFrame(errors).T\nerrors_df\n\n\n\n\n\n\n\n\n\ntrain\ntest\n\n\n\n\nLinear Regression\n20.261366\n19.199722\n\n\nMLP Regression\n4.752028\n4.665698\n\n\nPolynomial Regression (d=4)\n4.956985\n6.374342\n\n\nPolynomial Regression (d=2)\n5.075907\n6.678665\n\n\nPolynomial Regression (d=3)\n5.017752\n6.567621\n\n\nPolynomial Regression (d=6)\n4.805097\n6.390612\n\n\nPolynomial Regression (d=1)\n20.261366\n20.851623\n\n\nPolynomial Regression (d=5)\n4.816000\n6.407422\n\n\nPolynomial Regression (d=7)\n4.767340\n6.491631\n\n\nPolynomial Regression (d=8)\n4.750171\n6.508795\n\n\nPolynomial Regression (d=9)\n4.683092\n6.514263\n\n\n\n\n\n\n\n\n\nerrors_df.plot(kind='bar', figsize=(12, 6), title='MSE for Train and Test Sets')\n\n\n\n\n\n\n\n\n\n# bias variance tradeoff\n\nerrors_poly = {}\n\nfor degree in range(1, 20):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, degree, plot=False)\n\n    # geting errors for polynomial regression only for plotting\n    errors_poly[degree] = errors[degree]\n\n\nerrors_poly_df = pd.DataFrame(errors_poly).T\nbest_degree = np.argmin(errors_poly_df.test) + 1\nmin_error = errors_poly_df.test[best_degree - 1]    # index of df = degree - 1\nprint(f\"Best degree: {best_degree}, Min error: {min_error}\")\n\n# set figure size\nplt.figure(figsize=(12, 6))\nplt.plot(errors_poly_df.index.values, errors_poly_df.train.values, label='train')\nplt.plot(errors_poly_df.index.values, errors_poly_df.test.values, label='test')\nplt.axvline(best_degree, color='black', linestyle='--', label='best degree')\nplt.xticks(np.arange(min(errors_poly_df.index), max(errors_poly_df.index)+1, 1.0))\nplt.ylim(4.5, 7.5)      # set y limit - to show the difference between train and test clearly\nplt.xlabel('Degree')\nplt.ylabel('MSE')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.show()\n\nBest degree: 4, Min error: 6.567621493533483\n\n\n\n\n\n\n\n\n\n\n\nRidge Regression with polynomial basis\n\n# initiate ridge regression model\nmodel_ridge = Ridge(alpha=0.3)\n\nerrors_ridge = {}\n\nfor degree in range(1, 20):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model_ridge, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f'ridge_{degree}', plot=False)\n\n    # geting errors for polynomial regression only for plotting\n    errors_ridge[degree] = errors[f'ridge_{degree}']\n\n\nerrors_ridge_df = pd.DataFrame(errors_ridge).T\nbest_degree_ridge = np.argmin(errors_ridge_df.test) + 1\nmin_error = errors_ridge_df.test[best_degree_ridge - 1]    # index of df = degree - 1\nprint(f\"Best degree: {best_degree_ridge}, Min error: {min_error}\")\n\n# set figure size\nplt.figure(figsize=(12, 6))\nplt.plot(errors_ridge_df.index.values, errors_ridge_df.train.values, label='train')\nplt.plot(errors_ridge_df.index.values, errors_ridge_df.test.values, label='test')\nplt.axvline(best_degree_ridge, color='black', linestyle='--', label='best degree')\nplt.xticks(np.arange(min(errors_ridge_df.index), max(errors_ridge_df.index)+1, 1.0))\nplt.ylim(4.5, 7.5)      # set y limit - to show the difference between train and test clearly\nplt.xlabel('Degree')\nplt.ylabel('MSE')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.show()\n\nBest degree: 4, Min error: 6.535907094411071\n\n\n\n\n\n\n\n\n\n\nplot_fit_predict(model_ridge, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Ridge Regression with Polynomial Features (d={best_degree_ridge})\", plot=True)\n\n{'train': 4.767435271070607, 'test': 6.39886853568042}\n\n\n\n\n\n\n\n\n\n\n\nGausian Basis Function\n\ndef create_guassian_basis(X , d , std = 1):\n    \"\"\"\n    X is (N, 1) array\n    d is number of basis functions\n    \"\"\"\n    means = np.linspace(X.min(), X.max(), d)\n    X = np.repeat(X, d, axis=1)\n    means = np.repeat(means.reshape(-1, 1), len(X), axis=1).T\n\n    return np.exp(-(X - means) ** 2 / (2 * std ** 2))\n\n\ndef show_gaussian_basis(d, stdev):\n    xs = np.linspace(-5, 5, 100).reshape(-1, 1)\n    X_gauss = create_guassian_basis(xs, d, std=stdev)\n    plt.plot(xs, X_gauss)\n    plt.xlabel('X')\n    plt.ylabel('Gaussian Basis')\n    plt.title('Degree {} Stddev'.format(d, stdev))\n\nshow_gaussian_basis(3, 1)\n\n\n\n\n\n\n\n\n\ninteract(show_gaussian_basis, d=(1, 10, 1), stdev=(0.1, 10, 0.1))\n\n\n\n\n&lt;function __main__.show_gaussian_basis(d, stdev)&gt;\n\n\n\nmodel_gauss = LinearRegression()\ndegree = 4\nXf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), degree)\nXf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), degree)\n\nX_lin_poly = create_guassian_basis(X_lin_1d, degree)\n\n\nplot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d={degree})\")\n\n{'train': 5.27763506266326, 'test': 5.349399604396995}\n\n\n\n\n\n\n\n\n\n\nerrors.clear()\n\n\nfor degree in range(3, 7):\n    Xf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_guassian_basis(X_lin_1d, degree)\n\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d={degree})\", plot=False)\n\n\nerrors_df = pd.DataFrame(errors).T\n\n\nerrors_df.plot(kind='bar', figsize=(12, 6), title='MSE for Train and Test Sets')\n\n\n\n\n\n\n\n\n\nerrors.clear()\n\n\n# Bias Variance Tradeoff wrt Sigma\n\nfor std in [0.1, 0.5, 1, 2, 5, 10]:\n    Xf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), 3, std)\n    Xf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), 3, std)\n\n    X_lin_poly = create_guassian_basis(X_lin_1d, 3, std)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d=3, std={std})\", plot=False)\n\n\n# Plot the train and test errors for different values of sigma\n\nerrors_df = pd.DataFrame(errors).T\n\ntest_errors = errors_df.test.values\ntrain_errors = errors_df.train.values\n\nlog_test_errors = np.log(test_errors)\nlog_train_errors = np.log(train_errors)\n\nstds = [0.1, 0.5, 1, 2, 5, 10]\n\nplt.plot(stds , log_test_errors, label='Log Test Loss')\nplt.plot(stds , log_train_errors, label='Log Train Loss')\nplt.scatter(stds, log_test_errors)\nplt.scatter(stds, log_train_errors)\nplt.legend()\nplt.xlabel('Sigma')\nplt.ylabel('Log(MSE)')\nplt.title('Bias Variance Tradeoff wrt Sigma')\n\nText(0.5, 1.0, 'Bias Variance Tradeoff wrt Sigma')\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process\n\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\nfrom sklearn.gaussian_process.kernels import WhiteKernel\n\nlong_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n\nseasonal_kernel = (\n    2.0**2\n    * RBF(length_scale=100.0)\n    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n)\n\nirregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n\nnoise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n)\n\nco2_kernel = (\n    long_term_trend_kernel + seasonal_kernel + irregularities_kernel + noise_kernel\n)\nco2_kernel\n\n50**2 * RBF(length_scale=50) + 2**2 * RBF(length_scale=100) * ExpSineSquared(length_scale=1, periodicity=1) + 0.5**2 * RationalQuadratic(alpha=1, length_scale=1) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01)\n\n\n\n# Using GP for the interpolation problem\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\ndef plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n\n    gaussian_process = GaussianProcessRegressor(\n        kernel=co2_kernel,n_restarts_optimizer=9\n    )\n\n    gaussian_process.fit(X_norm_train, y_norm_train)\n\n    y_hat_train, std_prediction_train = gaussian_process.predict(X_norm_train, return_std=True)\n    y_hat_test , std_prediction_test = gaussian_process.predict(X_norm_test, return_std=True)\n\n    y_hat_train = y_hat_train.reshape(-1, 1)\n    y_hat_test = y_hat_test.reshape(-1, 1)\n\n    # Transform back to original scale\n    y_hat_train = s2.inverse_transform(y_hat_train)\n    y_hat_test = s2.inverse_transform(y_hat_test)\n\n    y_hat_lin , std_prediction_lin = gaussian_process.predict(X_lin , return_std=True)\n    y_hat_lin = y_hat_lin.reshape(-1, 1)\n    y_hat_lin = s2.inverse_transform(y_hat_lin)\n\n    errors[title] = {\"train\": mean_squared_error(y_train, y_hat_train),\n                     \"test\": mean_squared_error(y_test, y_hat_test)}\n\n    if plot:\n        plt.plot(X_train, y_train, 'o', label='train',markersize=1)\n        plt.plot(X_test, y_test, 'o', label='test', ms=3)\n        plt.plot(s1.inverse_transform(X_lin_1d), y_hat_lin, label='model')\n        plt.fill_between(s1.inverse_transform(X_lin_1d).reshape(-1), \n                         (y_hat_lin - 1.96*std_prediction_lin.reshape(-1,1)).reshape(-1), \n                         (y_hat_lin + 1.96*std_prediction_lin.reshape(-1,1)).reshape(-1), alpha=0.5 , label='95% Confidence interval')\n        plt.xlabel('Months since first measurement')\n        plt.ylabel('CO2 Levels')\n        plt.legend()\n\n        plt.title('{}\\n Train MSE: {:.2f} | Test MSE: {:.2f}'.format(title, errors[title][\"train\"], errors[title][\"test\"]))\n\n    return errors[title]\n\n\nplot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Regression\")\n\nKeyboardInterrupt: \n\n\n\n\nFourier Features\nReference: https://bmild.github.io/fourfeat/\n\nFourier Feature Mapping\nFor every input x, we map it to a higher dimensional space using the following function:\n\\[\\gamma(x) = [\\cos(2\\pi Bx), \\sin(2\\pi Bx)]^{T} \\]\nwhere \\(B\\) is a random Gaussian matrix, where each entry is drawn independently from a normal distribution N(0, \\(^{2}\\))\n\nnp.random.seed(42)\nsigma = 5\nNUM_features = 5\nfs = sigma*np.random.randn(NUM_features)\nprint(fs)\n\nfor i in range(NUM_features):\n    plt.plot(X_norm, np.sin(fs[i]*X_norm), label=f'feature {i}-sin')\n    plt.plot(X_norm, np.cos(fs[i]*X_norm), label=f'feature {i}-cos')\nplt.legend()\nplt.title('Fourier Featurization of X manually')\n\n[ 2.48357077 -0.69132151  3.23844269  7.61514928 -1.17076687]\n\n\nText(0.5, 1.0, 'Fourier Featurization of X manually')\n\n\n\n\n\n\n\n\n\n\n# Explicit implementation of RFF\n\ndef create_random_features(X, gamma, NUM_features):\n    \"\"\"\n    X is (N, 1) array\n    gamma is a scalar\n    NUM_features is a scalar\n    \"\"\"\n    \n    X_rff = np.zeros((len(X), 2*NUM_features + 1))\n    X_rff[:, 0] = X[:, 0]\n    for i in range(NUM_features):\n        b = np.random.randn()\n        X_rff[:, i+1] = np.sin(2*np.pi*gamma*b*X[:, 0])\n        X_rff[:, i + NUM_features+1] = np.cos(2*np.pi*gamma*b*X[:, 0])\n    \n    # Normalize each column\n    X_rff = StandardScaler().fit_transform(X_rff)\n    return X_rff\n\n\n\nSklearn Implementation\n\n# Sklearn's implementation of RFF\n\nfrom sklearn.kernel_approximation import RBFSampler\nRBFSampler?\n\nInit signature: RBFSampler(*, gamma=1.0, n_components=100, random_state=None)\nDocstring:     \nApproximate a RBF kernel feature map using random Fourier features.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`.\n\nParameters\n----------\ngamma : 'scale' or float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2).\n    If ``gamma='scale'`` is passed then it uses\n    1 / (n_features * X.var()) as value of gamma.\n\n    .. versionadded:: 1.2\n       The option `\"scale\"` was added in 1.2.\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\nNystroem : Approximate a kernel map using a subset of the training data.\nPolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\nSkewedChi2Sampler : Approximate feature map for\n    \"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1, 1]\n&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)\n&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)\n&gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3)\n&gt;&gt;&gt; clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n&gt;&gt;&gt; clf.score(X_features, y)\n1.0\nFile:           ~/miniforge3/lib/python3.9/site-packages/sklearn/kernel_approximation.py\nType:           type\nSubclasses:     \n\n\n\nr= RBFSampler(n_components=5)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=0.1)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=20)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\n# Implicit implementation of RFF using sklearn\n\ndef create_rff(X, gamma, NUM_features):\n    # Random Fourier Features\n    # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n    rbf_feature = RBFSampler(gamma=gamma, n_components=NUM_features, random_state=1)\n    X_features = rbf_feature.fit_transform(X)\n    return X_features\n\n\nmodel3 = LinearRegression()\ngamma = 2.0\nNUM_features = 4\n\nXf_norm_train = create_rff(X_norm_train.reshape(-1, 1), gamma, NUM_features)\nXf_norm_test = create_rff(X_norm_test.reshape(-1, 1), gamma, NUM_features)\n\nX_lin_rff = create_rff(X_lin_1d, gamma, NUM_features)\n\nplot_fit_predict(model3, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_rff, f\"Random Fourier Features (gamma={gamma}, NUM_features={NUM_features})\")\n\n{'train': 5.035023975072016, 'test': 4.782073267288203}\n\n\n\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff for Fourier Features\nw.r.t gamma\n\nmodel4 = LinearRegression()\n\n# NUM_features_values = [1, 2, 3, 4, 5, 10, 20, 50, 100]\ngamma_values = [0.01, 0.1, 1, 2, 5, 10]\n\nerrors_rff = {}\n\nfor gamma in gamma_values:\n    # gamma = 2.0\n    NUM_features_ = 100\n    Xf_norm_train = create_rff(X_norm_train.reshape(-1, 1), gamma, NUM_features_)\n    Xf_norm_test = create_rff(X_norm_test.reshape(-1, 1), gamma, NUM_features_)\n\n    X_lin_rff = create_rff(X_lin_1d, gamma, NUM_features_)\n\n    plot_fit_predict(model4, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_rff, gamma, plot=False)\n\n    errors_rff[gamma] = errors[gamma]\n\n\npd.DataFrame(errors_rff).T.plot(logy=True, logx=True)\n\n\n\n\n\n\n\n\n\n\n\nExtrapolation using Gaussian Process\n\nX_norm_train = X_norm[:int(len(X_norm)*0.7)]\nX_norm_test = X_norm[int(len(X_norm)*0.7):]\n\ny_norm_train = y_norm[:int(len(y_norm)*0.7)]\ny_norm_test = y_norm[int(len(y_norm)*0.7):]\n\nX_train = X[:int(len(X)*0.7)]\nX_test = X[int(len(X)*0.7):]\n\ny_train = y[:int(len(y)*0.7)]\ny_test = y[int(len(y)*0.7):]\n\n\nplt.plot(X_norm_train, y_norm_train, 'o', label='train',markersize=1)\nplt.plot(X_norm_test, y_norm_test, 'o', label='test', ms=3)\n\n\n\n\n\n\n\n\n\nplot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Extrapolation Regression\")\n\nNameError: name 'plot_fit_gp' is not defined\n\n\n\n\nBefore this\n\n\nDataset 1: Sine wave with noise\n\n# Generate some data\nrng = np.random.RandomState(1)\nx = 15 * rng.rand(200)\ny = np.sin(x) + 0.1 * rng.randn(200)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# plot the data\nplt.scatter(df.x, df.y)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train test split\ntrain = df.sample(frac=0.8, random_state=1)\ntest = df.drop(train.index)\n\nplt.scatter(train.x, train.y, color='blue', label='train')\nplt.scatter(test.x, test.y, color='orange', label='test')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plot_predictions(train, test, yhat_train, yhat_test):\n        \n    # add yhat_train to train and yhat_test to test\n    train['yhat'] = yhat_train\n    test['yhat'] = yhat_test\n\n    # sort train and test by x\n    train = train.sort_values(by='x')\n    test = test.sort_values(by='x')\n\n    # Train and test error\n    train_rmse = np.sqrt(np.mean((train.yhat - train.y)**2))\n    test_rmse = np.sqrt(np.mean((test.yhat - test.y)**2))\n\n    plt.scatter(train.x, train.y, color='blue', label='train')\n    plt.scatter(test.x, test.y, color='orange', label='test')\n    plt.plot(train.x, train.yhat, color='red', label='train prediction')\n    plt.plot(test.x, test.yhat, color='green', label='test prediction')\n    plt.title('Train RMSE: {:.3f}, Test RMSE: {:.3f}'.format(train_rmse, test_rmse))\n    plt.legend()\n    plt.show()\n\n    return train_rmse, test_rmse\n\n\n# Hyperparameter tuning using grid search and showing bias variance tradeoff\ndef hyperparameter_tuning(params, train, test, model):\n    train_rmse = []\n    test_rmse = []\n\n    for d in params:\n        yhat_train, yhat_test = model(d, train, test)\n        train_rmse.append(np.sqrt(np.mean((yhat_train - train.y)**2)))\n        test_rmse.append(np.sqrt(np.mean((yhat_test - test.y)**2)))\n\n    plt.plot(params, train_rmse, label='train')\n    plt.plot(params, test_rmse, label='test')\n    plt.xlabel('params')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.show()\n\n    optimal_param = params[np.argmin(test_rmse)]\n\n    return optimal_param\n\n\nrmse_dict = {}\n\n\nModel 1: MLP\n\n# use sk-learn for MLP\nmlp_model = MLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter = 10000)\nmlp_model.fit(np.array(train.x).reshape(-1, 1), train.y)\n\nMLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressorMLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter=10000)\n\n\n\nyhat_train = mlp_model.predict(np.array(train.x).reshape(-1, 1))\nyhat_test = mlp_model.predict(np.array(test.x).reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['MLP'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\n\nModel 2: Vanilla Linear Regression\n\nlr1 = LinearRegression()\nlr1.fit(np.array(train.x).reshape(-1, 1), train.y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nyhat_train = lr1.predict(np.array(train.x).reshape(-1, 1))\nyhat_test = lr1.predict(np.array(test.x).reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Vanilla LR'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\n\nModel 3: Polynomial regression with degree d\n\ndef poly_regression(d, train, test):\n    lr = LinearRegression()\n    pf = PolynomialFeatures(degree=d)\n    \n    X_train = pf.fit_transform(train.x.values.reshape(-1, 1))\n    X_test = pf.fit_transform(test.x.values.reshape(-1, 1))\n    \n    lr.fit(X_train, train.y)\n    \n    yhat_train = lr.predict(X_train)\n    yhat_test = lr.predict(X_test)\n\n    return yhat_train, yhat_test\n\n\n# Hyperparameter tuning using grid search and showing bias variance tradeoff\ndegrees = range(1, 20)\nbest_degree = hyperparameter_tuning(degrees, train, test, poly_regression)\nyhat_train, yhat_test = poly_regression(best_degree, train, test)\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Polynomial Regression'] = (train_rmse, test_rmse)\nprint(\"Best degree: \", best_degree)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest degree:  10\n\n\n\n\nModel 4: Linear regression with sine and cosine basis functions\n\ndef sine_basis_regression(num_basis, train, test):\n    lr = LinearRegression()\n    for i in range(1, num_basis+1):\n        train[f\"sine_{i}\"] = np.sin(i*train.x)\n        train[f\"cosine_{i}\"] = np.cos(i*train.x)\n        test[f\"sine_{i}\"] = np.sin(i*test.x)\n        test[f\"cosine_{i}\"] = np.cos(i*test.x)\n    \n    X_train = train.drop(['y'], axis=1)\n    X_test = test.drop(['y'], axis=1)\n    \n    lr.fit(X_train, train.y)\n\n    yhat_train = lr.predict(X_train)\n    yhat_test = lr.predict(X_test)\n\n    return yhat_train, yhat_test\n\n\nbasis = range(1, 20)\nbest_num_basis = hyperparameter_tuning(basis, train, test, sine_basis_regression)\nyhat_train, yhat_test = sine_basis_regression(best_num_basis, train, test)\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Sine Basis Regression'] = (train_rmse, test_rmse)\nprint(\"Best number of basis: \", best_num_basis)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest number of basis:  14\n\n\n\n\nModel 5: Linear regression with Gaussian basis functions\n\n# Source: https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Gaussian-basis-functions\n\nclass GaussianFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Uniformly spaced Gaussian features for one-dimensional input\n    \n    Constructor with N centers and width_factor as hyperparameters\n    N comes from the number of basis functions\n    width_factor is the width of each basis function\n    \"\"\"\n    \n    def __init__(self, N, width_factor=2.0):\n        self.N = N\n        self.width_factor = width_factor\n    \n    @staticmethod\n    def _gauss_basis(x, y, width, axis=None):\n        arg = (x - y) / width\n        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n        \n    def fit(self, X, y=None):\n        # create N centers spread along the data range\n        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n        return self\n        \n    def transform(self, X):\n        return self._gauss_basis(X[:, :, np.newaxis], self.centers_, self.width_, axis=1)\n\n\n# Hyperparameter tuning\nbasis = range(2, 20)\n\ntrain_rmse = []\ntest_rmse = []\nfor d in basis:\n    model = make_pipeline(GaussianFeatures(d), LinearRegression())\n    model.fit(np.array(train.x).reshape(-1, 1), train.y)\n    yhat_train = model.predict(np.array(train.x).reshape(-1, 1))\n    yhat_test = model.predict(np.array(test.x).reshape(-1, 1))\n    train_rmse.append(np.sqrt(np.mean((yhat_train - train.y)**2)))\n    test_rmse.append(np.sqrt(np.mean((yhat_test - test.y)**2)))\n\nbest_num_basis = basis[np.argmin(test_rmse)]\nprint(\"Best number of basis: \", best_num_basis)\nplt.plot(basis, train_rmse, label='train')\nplt.plot(basis, test_rmse, label='test')\nplt.xlabel('degree')\nplt.ylabel('RMSE')\nplt.legend()\nplt.show()\n\nBest number of basis:  13\n\n\n\n\n\n\n\n\n\n\ngauss_model = make_pipeline(GaussianFeatures(best_num_basis), LinearRegression())\ngauss_model.fit(np.array(train.x).reshape(-1, 1), train.y)\nyhat_train = gauss_model.predict(train.x.values.reshape(-1, 1))\nyhat_test = gauss_model.predict(test.x.values.reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Gaussian Basis Regression'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\nPlotting rmse using different variants of linear regression\n\n# create a bar plot of train and test RMSE\n\ntrain_rmse = [rmse_dict[key][0] for key in rmse_dict.keys()]\ntest_rmse = [rmse_dict[key][1] for key in rmse_dict.keys()]\nlabels = [key for key in rmse_dict.keys()]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 5))\nrects1 = ax.bar(x - width/2, train_rmse, width, label='Train RMSE')\nrects2 = ax.bar(x + width/2, test_rmse, width, label='Test RMSE')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('RMSE')\nax.set_title('RMSE by model')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nDataset 2: CO2 Dataset\n\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n6.255330\n-0.020070\n\n\n1\n10.804867\n-0.920032\n\n\n2\n0.001716\n0.024965\n\n\n3\n4.534989\n-0.916051\n\n\n4\n2.201338\n0.776696\n\n\n...\n...\n...\n\n\n195\n13.979581\n1.115462\n\n\n196\n0.209274\n0.163526\n\n\n197\n3.515431\n-0.332839\n\n\n198\n9.251675\n0.161240\n\n\n199\n14.235245\n0.996049\n\n\n\n\n200 rows  2 columns\n\n\n\n\n\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\nKeyError: \"None of [Index(['year', 'month'], dtype='object')] are in the [columns]\"\n\n\n\ndf\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n6.255330\n-0.020070\n\n\n1\n10.804867\n-0.920032\n\n\n2\n0.001716\n0.024965\n\n\n3\n4.534989\n-0.916051\n\n\n4\n2.201338\n0.776696\n\n\n...\n...\n...\n\n\n195\n13.979581\n1.115462\n\n\n196\n0.209274\n0.163526\n\n\n197\n3.515431\n-0.332839\n\n\n198\n9.251675\n0.161240\n\n\n199\n14.235245\n0.996049\n\n\n\n\n200 rows  2 columns\n\n\n\n\n\ndf.average.plot()\n\nAttributeError: 'DataFrame' object has no attribute 'average'\n\n\n\ntrain_cutoff = 2000\ntrain = df[df.year &lt; train_cutoff]\ntest = df[df.year &gt;= train_cutoff]\ndf.average.plot()\n\ntrain.average.plot(color='blue')\ntest.average.plot(color='orange')\n\nlen(train), len(test)\n\n\nmonths_from_start = range(len(df))\nmonths_from_start = np.array(months_from_start).reshape(-1, 1)\n\n\n# use sk-learn for MLP\n\nmlp_model = MLPRegressor(hidden_layer_sizes=[512, 512, 512, 512, 512], max_iter = 5000)\nmlp_model.fit(months_from_start[:len(train)], train.average.values)\n\n\nyhat_train = mlp_model.predict(months_from_start[:len(train)])\nyhat_test = mlp_model.predict(months_from_start[len(train):])\n\nyhat_train = pd.Series(yhat_train, index=train.index)\nyhat_test = pd.Series(yhat_test, index=test.index)\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\n# normalize data\n\ntrain_scaled = (train - train.mean()) / train.std()\ntest_scaled = (test - test.mean()) / test.std()\nmonths_from_start_scaled = (months_from_start - months_from_start.mean()) / months_from_start.std()\n\n# train_scaled = (train - train.mean()) / train.std()\n# test_scaled = (test - test.mean()) / test.std()\n# months_from_start_scaled = (months_from_start - months_from_start.mean()) / months_from_start.std()\n\nmlp_model = MLPRegressor(hidden_layer_sizes=512, max_iter = 1000)\nmlp_model.fit(months_from_start_scaled[:len(train)], train_scaled.average.values)\n\nyhat_train = mlp_model.predict(months_from_start_scaled[:len(train)])\nyhat_test = mlp_model.predict(months_from_start_scaled[len(train):])\n\nyhat_train_scaled = pd.Series(yhat_train, index=train.index)\nyhat_test_scaled = pd.Series(yhat_test, index=test.index)\n\nyhat_train = yhat_train_scaled * train.std() + train.mean()\n# yhat_test = yhat_test_scaled * test.std() + test.mean()\n\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\nModel 2: Vanilla Linear Regression\n\nlr1 = LinearRegression()\nlr1.fit(months_from_start[:len(train)], train.average.values)\nyhat1_test = lr1.predict(months_from_start[len(train):])\nyhat1_train = lr1.predict(months_from_start[:len(train)])\n\nyhat_train = pd.Series(yhat1_train, index=train.index)\nyhat_test = pd.Series(yhat1_test, index=test.index)\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\n\nModel 3: Polynomial regression with degree d\n\ndef poly_regression(d, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    pf = PolynomialFeatures(degree=d)\n    X_train = pf.fit_transform(months_from_start[:len(train)])\n    X_test = pf.fit_transform(months_from_start[len(train):])\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    df.average.plot()\n    yhat_train.plot()\n    yhat_test.plot()\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_regression(2, train, test)\n\n\n\nModel 4: Linear Regression with sine and cosine basis functions\n\n### Adding sine and cosine terms\ndef sine_cosine_features(X, n):\n    \"\"\"\n    X: array of shape (n_samples, 1)\n    n: number of sine and cosine features to add\n    \"\"\"\n    for i in range(1, n+1):\n        X = np.hstack([X, np.sin(i*X), np.cos(i*X)])\n    return X\n\n\nX = np.linspace(-1, 1, 100).reshape(-1, 1)\n\n\n_ = plt.plot(X, sine_cosine_features(X, 0))\n\n\n\nModel 5: Gaussian basis functions\n\n\nModel 6: Linear Regression with polynomial and sine/cosine basis functions\n\ndef poly_sine_cosine_regression(n, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    \n    X_train = sine_cosine_features(months_from_start[:len(train)], n)\n    \n    X_test = sine_cosine_features(months_from_start[len(train):], n)\n    print(X_train.shape, X_test.shape)\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    \n    yhat_train.plot(alpha=0.2, lw=4)\n    yhat_test.plot(alpha=0.2, lw=4)\n    df.average.plot(color='k', lw=1)\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_sine_cosine_regression(6, train, test)\n\n\n\nModel 7: Random Fourier Features\n\ndef rff_featurise(X, n_components=100):\n    # Random Fourier Features\n    # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n    from sklearn.kernel_approximation import RBFSampler\n    rbf_feature = RBFSampler(gamma=1, n_components=n_components, random_state=1)\n    X_features = rbf_feature.fit_transform(X)\n    return X_features\n\n\ndef poly_rff_regression(n, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    \n    X_train = rff_featurise(months_from_start[:len(train)], n)\n    \n    X_test = rff_featurise(months_from_start[len(train):], n)\n    print(X_train.shape, X_test.shape)\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    \n    yhat_train.plot(alpha=0.2, lw=4)\n    yhat_test.plot(alpha=0.2, lw=4)\n    df.average.plot(color='k', lw=1)\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_rff_regression(440, train, test)"
  },
  {
    "objectID": "notebooks/hyperparameter-optimisation.html",
    "href": "notebooks/hyperparameter-optimisation.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\n\nMakeMoons Dataset\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\nlatexify(fig_width=5, fig_height=4)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label='Train') \nformat_axes(plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\n\n#Define the hyperparameters' possible values\nmax_depth_values = [1,2,3,4,5,6,7,8,9,10]\nmin_samples_split_values = [2,3,4,5,6,7,8]\ncriteria_values = ['gini', 'entropy']\n\nNested For Loops\n\nbest_accuracy = 0\nbest_params = {}\n\nfor max_depth in max_depth_values:\n    for min_samples_split in min_samples_split_values:\n        for criterion in criteria_values:\n            # Define the Decision Tree Classifier\n            dt_classifier = DecisionTreeClassifier(\n                max_depth=max_depth,\n                min_samples_split=min_samples_split,\n                criterion=criterion,\n                random_state=42\n            )\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            \n            # Check if this combination gives a better accuracy\n            if val_accuracy &gt; best_accuracy:\n                best_accuracy = val_accuracy\n                best_params = {\n                    'max_depth': max_depth,\n                    'min_samples_split': min_samples_split,\n                    'criterion': criterion\n                }\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_accuracy)\n\n# Train the model with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n\n# Evaluate on the test set\ntest_accuracy = best_dt_classifier.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'entropy'}\nBest Validation Accuracy: 0.925\nTest Accuracy: 0.8950\n\n\nUsing Itertools\n\nfrom itertools import product\n\nbest_accuracy = 0\nbest_params = {}\n\n# Use itertools.product for a more succinct code\nfor max_depth, min_samples_split, criterion in product(max_depth_values, min_samples_split_values, criteria_values):\n    # Define the Decision Tree Classifier\n    dt_classifier = DecisionTreeClassifier(\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        criterion=criterion,\n        random_state=42\n    )\n    dt_classifier.fit(X_train, y_train)\n    \n    # Evaluate on the validation set\n    val_accuracy = dt_classifier.score(X_val, y_val)\n    \n    # Check if this combination gives a better accuracy\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_params = {\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'criterion': criterion\n        }\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_accuracy)\n\n# Train the model with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n\n# Evaluate on the test set\ntest_accuracy = best_dt_classifier.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'entropy'}\nBest Validation Accuracy: 0.925\nTest Accuracy: 0.8950\n\n\nUsing Sklearn Grid Search (5 fold Cross-Validation)\n\nfrom sklearn.model_selection import GridSearchCV\n# Define the Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'max_depth': max_depth_values,\n    'min_samples_split': min_samples_split_values,\n    'criterion': criteria_values\n}\n\nX_train_val = np.concatenate([X_train, X_val], axis=0)\ny_train_val = np.concatenate([y_train, y_val], axis=0)\n\n# Use GridSearchCV for hyperparameter tuning\nnum_inner_folds = 5\ngrid_search = GridSearchCV(dt_classifier, param_grid, scoring='accuracy', cv=num_inner_folds)\ngrid_search.fit(X_train_val, y_train_val)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Evaluate on the test set\ntest_accuracy = grid_search.best_estimator_.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 8}\nTest Accuracy: 0.9000"
  },
  {
    "objectID": "notebooks/decision-tree-real-output.html",
    "href": "notebooks/decision-tree-real-output.html",
    "title": "Decision Trees Real Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-real-output.csv\", index_col=[0])\n\n\ndf\n\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWind\nMinutes Played\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\n20\n\n\nD2\nSunny\nHot\nHigh\nStrong\n24\n\n\nD3\nOvercast\nHot\nHigh\nWeak\n40\n\n\nD4\nRain\nMild\nHigh\nWeak\n50\n\n\nD5\nRain\nCool\nNormal\nWeak\n60\n\n\nD6\nRain\nCool\nNormal\nStrong\n10\n\n\nD7\nOvercast\nCool\nNormal\nStrong\n4\n\n\nD8\nSunny\nMild\nHigh\nWeak\n10\n\n\nD9\nSunny\nCool\nNormal\nWeak\n60\n\n\nD10\nRain\nMild\nNormal\nWeak\n40\n\n\nD11\nSunny\nMild\nHigh\nStrong\n45\n\n\nD12\nOvercast\nMild\nHigh\nStrong\n40\n\n\nD13\nOvercast\nHot\nNormal\nWeak\n35\n\n\nD14\nRain\nMild\nHigh\nStrong\n20\n\n\n\n\n\n\n\n\n\nmean_mins = df[\"Minutes Played\"].mean()\nprint(mean_mins)\n\n32.714285714285715\n\n\n\ninitial_mse = ((df[\"Minutes Played\"] - mean_mins) ** 2).mean()\nprint(initial_mse)\n\n311.3469387755102\n\n\n\n# Explore MSE for different splits based on the \"Outlook\" attribute\nweighted_total_mse = 0.0\nfor category in df[\"Wind\"].unique():\n    subset = df[df[\"Wind\"] == category]\n    \n    # Calculate MSE for the subset\n    mse_subset = ((subset[\"Minutes Played\"] - subset[\"Minutes Played\"].mean()) ** 2).mean()\n    \n    # Calculate the weighted MSE\n    weighted_mse = (len(subset) / len(df)) * mse_subset\n    weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    print(subset[\"Minutes Played\"].values)\n    print(f\"Wind: {category}\")\n    print(\"Subset MSE:\", mse_subset)\n    print(f\"Weighted MSE = {len(subset)}/{len(df)} * {mse_subset:0.4} = {weighted_mse:0.4}\")\n    print(\"\\n\")\n\nprint(\"Weighted total MSE:\", weighted_total_mse)\n\n[20 40 50 60 10 60 40 35]\nWind: Weak\nSubset MSE: 277.734375\nWeighted MSE = 8/14 * 277.7 = 158.7\n\n\n[24 10  4 45 40 20]\nWind: Strong\nSubset MSE: 218.13888888888889\nWeighted MSE = 6/14 * 218.1 = 93.49\n\n\nWeighted total MSE: 252.19345238095235\n\n\n\nreduction_mse_wind = initial_mse - weighted_total_mse\nprint(reduction_mse_wind)\n\n59.15348639455783\n\n\n\ndef reduction_mse(df_dataset, input_attribute, target_attribute):\n    # Calculate the initial MSE\n    mean_target = df_dataset[target_attribute].mean()\n    initial_mse = ((df_dataset[target_attribute] - mean_target) ** 2).mean()\n    weighted_total_mse = 0.0\n\n    for category in df_dataset[input_attribute].unique():\n        subset = df_dataset[df_dataset[input_attribute] == category]\n        mse_subset = ((subset[target_attribute] - subset[target_attribute].mean()) ** 2).mean()\n        \n        weighted_mse = (len(subset) / len(df_dataset)) * mse_subset\n        weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    return initial_mse - weighted_total_mse\n\n    \n\n\nreduction = {}\nfor attribute in [\"Outlook\", \"Temp\", \"Humidity\", \"Wind\"]:\n    reduction[attribute] = reduction_mse(df, attribute, \"Minutes Played\")\n    \nreduction_ser = pd.Series(reduction)\n\n\nlatexify()\n\n\nbars = reduction_ser.plot(kind='bar', rot=0, color='k')\nformat_axes(plt.gca())\n\n# Add values on top of the bars\nfor bar in bars.patches:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\nplt.xlabel(\"Attribute\")\nplt.ylabel(\"Reduction in MSE\")\nplt.savefig(\"../figures/decision-trees/discrete-input-real-output-level-1.pdf\")"
  },
  {
    "objectID": "notebooks/geometric-linear-regression.html",
    "href": "notebooks/geometric-linear-regression.html",
    "title": "Geometric interpretation of Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\n\n\nfrom latexify import latexify, format_axes\nlatexify(columns=2)\n\n\n\n\n# Define points\nA = (1, 3)\nB = (2, 1)\nOrigin = (0, 0)\n\n# Plot vectors\nplt.quiver(*Origin, *A, angles='xy', scale_units='xy', scale=1, color='b', label='$v_1$')\nplt.quiver(*Origin, *B, angles='xy', scale_units='xy', scale=1, color='r', label='$v_2$')\n\n# Set axis limits\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\n\n# Add legend\nplt.legend()\n\n# Show plot\nplt.grid(alpha=0.1)\n\nax = plt.gca()\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-1.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Now, create v3 = v1 + v2 and v4 = v1 - v2 and plot\nC = (A[0] + B[0], A[1] + B[1])\nD = (A[0] - B[0], A[1] - B[1])\n\n# Set axis limits\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\n\nplt.quiver(*Origin, *A, angles='xy', scale_units='xy', scale=1, color='b', label='$v_1$')\nplt.quiver(*Origin, *B, angles='xy', scale_units='xy', scale=1, color='r', label='$v_2$')\nplt.quiver(*Origin, *C, angles='xy', scale_units='xy', scale=1, color='g', label='$v_3$')\nplt.quiver(*Origin, *D, angles='xy', scale_units='xy', scale=1, color='y', label='$v_4$')\n\nax = plt.gca()\nformat_axes(ax)\nplt.legend()\n\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-2.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nA_arr = np.array(A)\nB_arr = np.array(B)\n\nAB_matrix = np.zeros((2, 2))\n# First column is A, second column is B\nAB_matrix[:, 0] = A_arr\nAB_matrix[:, 1] = B_arr\n\nprint(AB_matrix)\n\n[[1. 2.]\n [3. 1.]]\n\n\n\ndef new_vector(AB_matrix, alpha):\n    return AB_matrix @ alpha\n\nprint(new_vector(AB_matrix, np.array([1, -1])))\n\n[-1.  2.]\n\n\n\n# Generate a bunch of alphas\nalphas = np.random.uniform(-3, 3, size=(30000, 2))\nnew_vecs = []\nfor i, alpha in enumerate(alphas):\n    new_vecs.append(new_vector(AB_matrix, alpha))\n\nnew_vecs = np.array(new_vecs)\n\n\nt = new_vecs\nplt.scatter(t[:, 0], t[:, 1], alpha=0.2)\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nax = plt.gca()\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-3.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nA = np.array([1, 2])\nB = np.array([2, 4])\n\nAB_matrix = np.zeros((2, 2))\n# First column is A, second column is B\nAB_matrix[:, 0] = A\nAB_matrix[:, 1] = B\n\nprint(AB_matrix)\n\n[[1. 2.]\n [2. 4.]]\n\n\n\n# Generate a bunch of alphas\nalphas = np.random.uniform(-3, 3, size=(10000, 2))\nnew_vecs = []\nfor i, alpha in enumerate(alphas):\n    new_vecs.append(new_vector(AB_matrix, alpha))\n\nnew_vecs = np.array(new_vecs)\n\nplt.scatter(new_vecs[:, 0], new_vecs[:, 1], alpha=0.2, s=2)\nax = plt.gca()\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-4.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a figure and 3D axis\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define the surface plot\nX = np.linspace(0, 2.2, 100)\nY = np.linspace(-2.2, 1, 100)\nX, Y = np.meshgrid(X, Y)\nZ = X\n\n# Plot the surface\nsurf = ax.plot_surface(X, Y, Z,  alpha=0.3, rstride=100, cstride=100)\n\n# Define points\nA = np.array([1, 1, 1])\nD = np.array([0, 0, 0])\nB = np.array([2, -2, 2])\n\n# Mark the origin\nax.scatter(*D, color='black', label='Origin')\n\n# Plot vectors with labels including the vector\nax.quiver(D[0], D[1], D[2], A[0], A[1], A[2], color='b', label=f'$X_1 = {A.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], B[0], B[1], B[2], color='r', label=f'$X_2 = {B.tolist()}$', arrow_length_ratio=0.1)\n\n# Set axis labels\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_zlabel('$z$', fontsize=12)\n\n# Set legend\nax.legend()\n\n# Adjust view angle\nax.view_init(elev=15, azim=-35)\n\n# Customize grid lines\nax.grid(linestyle='dashed', color='white', alpha=0.2)  # Adjust color here\n\n\nplt.savefig(\"../figures/linear-regression/geometric-1.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\n\n# Create a figure and 3D axis\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define the surface plot\nX = np.linspace(0, 10.2, 300)\nY = np.linspace(-5, 5, 300)\nX, Y = np.meshgrid(X, Y)\nZ = X\n\n# Plot the surface\nsurf = ax.plot_surface(X, Y, Z, alpha=0.3, rstride=100, cstride=100,)\n\n# Define points\nA = np.array([1, 1, 1])\nD = np.array([0, 0, 0])\nB = np.array([2, -2, 2])\ny_vec = np.array([8.8957, 0.6130, 1.7761])\n\n# Mark the origin\nax.scatter(*D, color='black', label='Origin')\n\n# Plot vectors with labels including the vector\nax.quiver(D[0], D[1], D[2], A[0], A[1], A[2], color='b', label=f'$X_1 = {A.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], B[0], B[1], B[2], color='r', label=f'$X_2 = {B.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], y_vec[0], y_vec[1], y_vec[2], color='g', label=f'$y = {y_vec.tolist()}$', arrow_length_ratio=0.1)\n\n# Set axis labels\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_zlabel('$z$', fontsize=12)\n\n# Set legend\nax.legend()\n\n# Adjust view angle\nax.view_init(elev=15, azim=-35)\n\n# Customize grid lines\nax.grid(linestyle='dashed', color='white', alpha=0.2)  # Adjust color here\n\n\nplt.savefig(\"../figures/linear-regression/geometric-2.pdf\", bbox_inches=\"tight\")\n\n\nX_matrix = np.zeros((3, 2))\nX_matrix[:, 0] = A\nX_matrix[:, 1] = B\n\nprint(X_matrix)\n\ntheta_hat = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y_vec\n\nprint(theta_hat)\n\ny_hat = X_matrix @ theta_hat\nprint(y_hat)\n\n\n# Plot y_hat vector\nax.quiver(D[0], D[1], D[2], y_hat[0], y_hat[1], y_hat[2], color='y', label=f'$\\hat y = {list(map(lambda x: round(x, 4), y_hat))}$', arrow_length_ratio=0.1)\nplt.legend()\nplt.savefig(\"../figures/linear-regression/geometric-3.pdf\", bbox_inches=\"tight\")\n\n\n# perpendiculat vector\nperp_vec = y_vec - y_hat\n# Plot perp vector with y_hat as origin\nax.quiver(y_hat[0], y_hat[1], y_hat[2], perp_vec[0], perp_vec[1], perp_vec[2], color='m', label=f'$y - \\hat y = {list(map(lambda x: round(x, 4), perp_vec))}$', arrow_length_ratio=0.1)\nplt.legend()\nplt.savefig(\"../figures/linear-regression/geometric-4.pdf\", bbox_inches=\"tight\")\n\n[[ 1.  2.]\n [ 1. -2.]\n [ 1.  2.]]\n[2.97445  1.180725]\n[5.3359 0.613  5.3359]\n\n\n\n\n\n\n\n\n\n\nperp_vec\n\narray([ 3.5598,  0.    , -3.5598])\n\n\n\nX_matrix[:, 0]\n\narray([1., 1., 1.])\n\n\n\n\nperp_vec@X_matrix[:, 0]\n\n-1.3322676295501878e-15\n\n\n\nperp_vec@X_matrix[:, 1]\n\n-2.6645352591003757e-15\n\n\n\nX_matrix.T @ perp_vec\n\narray([-1.33226763e-15, -2.66453526e-15])"
  },
  {
    "objectID": "notebooks/logits-usage.html",
    "href": "notebooks/logits-usage.html",
    "title": "Why use logits",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nimport torch\nimport torch.nn.functional as F\nimport ipywidgets as widgets\nfrom ipywidgets import interactive\nfrom IPython.display import display\n\n# Example ground truth probabilities\nground_truth_probs = torch.tensor([0.3, 0.7])\n\n# Example model predictions (logits)\nmodel_logits = torch.tensor([-2.0, 2.0])\n\n# Applying softmax to logits to get probabilities\nmodel_probs = F.sigmoid(model_logits)\n\n# Cross-entropy loss using probabilities\nloss_probs = F.binary_cross_entropy(model_probs, ground_truth_probs)\n\n# Cross-entropy loss using logits\nloss_logits = F.binary_cross_entropy_with_logits(model_logits, ground_truth_probs)\n\nprint(\"Loss using probabilities:\", loss_probs.item())\nprint(\"Loss using logits:\", loss_logits.item())\n\nLoss using probabilities: 0.7269279956817627\nLoss using logits: 0.7269280552864075\n\n\n\nl1 = widgets.FloatSlider(value=0.3, min=-300, max=300, step=0.01, description='Logits ex 1')\nl2 = widgets.FloatSlider(value=0.7, min=-300, max=300, step=0.01, description='Logits ex 2')\n\nbox = widgets.VBox([l1, l2])\n\n\ndef print_loss_using_both_methods(l1, l2):\n    logits = torch.tensor([l1, l2])\n    probs = F.sigmoid(logits)\n    loss_probs = F.binary_cross_entropy(probs, ground_truth_probs)\n    loss_logits = F.binary_cross_entropy_with_logits(logits, ground_truth_probs)\n    print(\"Loss using probabilities:\", loss_probs.item())\n    print(\"Loss using logits:\", loss_logits.item())\n\n\n# add interactivity\ninteractive(print_loss_using_both_methods, l1=l1, l2=l2)\n\n\n\n\n\nprint_loss_using_both_methods(l1.value, l2.value)\n\nLoss using probabilities: 8.003093719482422\nLoss using logits: 8.003093719482422\n\n\n\ndef our_sigmoid(z):\n    return 1/(1+torch.exp(-z))\n\n\nour_sigmoid(torch.tensor(-90.))\n\ntensor(0.)\n\n\n\nF.binary_cross_entropy_with_logits?\n\nSignature:\nF.binary_cross_entropy_with_logits(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    weight: Optional[torch.Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = 'mean',\n    pos_weight: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor\nDocstring:\nCalculate Binary Cross Entropy between target and input logits.\n\nSee :class:`~torch.nn.BCEWithLogitsLoss` for details.\n\nArgs:\n    input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\n    target: Tensor of the same shape as input with values between 0 and 1\n    weight (Tensor, optional): a manual rescaling weight\n        if provided it's repeated to match input tensor shape\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when reduce is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n    pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n        Must be a tensor with equal size along the class dimension to the number of classes.\n        Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n        operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n        size [B, C, H, W] will apply different pos_weights to each element of the batch or\n        [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n        along all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n        Default: ``None``\n\nExamples::\n\n     &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n     &gt;&gt;&gt; target = torch.empty(3).random_(2)\n     &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)\n     &gt;&gt;&gt; loss.backward()\nFile:      ~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/nn/functional.py\nType:      function"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-discrete-output.html",
    "href": "notebooks/decision-tree-real-input-discrete-output.html",
    "title": "Decision Trees Real Input and Discrete Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\nHeavily borrowed and inspired from https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n\nlatexify(columns=2)\n\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.2)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='rainbow')\nformat_axes(plt.gca())\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\n\nText(0, 0.5, '$x_2$')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef visualize_tree(depth, X, y, ax=None, cmap='rainbow'):\n    model = DecisionTreeClassifier(max_depth=depth)\n    ax = ax or plt.gca()\n    print(model, depth)\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    format_axes(plt.gca())\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.2,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    plt.tight_layout()\n    plt.savefig(f\"../figures/decision-trees/dt-{depth}.pdf\", bbox_inches=\"tight\")\n    plt.clf()\n\n\nfor depth in range(1, 11):\n    visualize_tree(depth, X, y)\n\nDecisionTreeClassifier(max_depth=1) 1\nDecisionTreeClassifier(max_depth=2) 2\nDecisionTreeClassifier(max_depth=3) 3\nDecisionTreeClassifier(max_depth=4) 4\nDecisionTreeClassifier(max_depth=5) 5\nDecisionTreeClassifier(max_depth=6) 6\nDecisionTreeClassifier(max_depth=7) 7\nDecisionTreeClassifier(max_depth=8) 8\nDecisionTreeClassifier(max_depth=9) 9\nDecisionTreeClassifier(max_depth=10) 10\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\n&lt;Figure size 400x246.914 with 0 Axes&gt;"
  },
  {
    "objectID": "notebooks/Maths for ML-2.html",
    "href": "notebooks/Maths for ML-2.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2 + Y**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Filled Contours Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-1.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-2.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(np.abs(X) + np.abs(Y))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-3.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-2.0, 8.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(Y*(X**2))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-4.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X*Y)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-5.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\n# Contour Plot\nX, Y = np.mgrid[-1:1:100j, -1:1:100j]\nZ = X**2 + Y**2 \ncp = plt.contour(X, Y, Z)\n# cb = plt.colorbar(cp)\n\n# Vector Field\nY, X = np.mgrid[-1:1:30j, -1:1:30j]\nU = 2*X\nV = 2*Y\nspeed = np.sqrt(U**2 + V**2)\nUN = U/speed\nVN = V/speed\nquiv = plt.quiver(X, Y, UN, VN,  # assign to var\n           color='Teal', \n           headlength=7)\nplt.savefig(\"gradient-field.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncp = ax.contour(X, Y, Z,levels=[1,2,3,4,5,6,7,8,9])\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(0.11,4.5,600)\nplt.plot(d,0.5/d)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-6.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i in np.linspace(0,2,100):\n    print (i,0.5 - i**2 + 1/i**2)\n\n0.0 inf\n0.020202020202020204 2450.7495918783793\n0.04040404040404041 613.0608675135189\n0.06060606060606061 272.74632690541773\n0.08080808080808081 153.63409505407608\n0.10101010101010102 98.4997969594939\n0.12121212121212122 68.54780762167124\n0.14141414141414144 50.4851040814244\n0.16161616161616163 38.75903646630445\n0.18181818181818182 30.71694214876033\n0.20202020202020204 24.96168783797571\n0.22222222222222224 20.700617283950617\n0.24242424242424243 17.45685548668503\n0.26262626262626265 14.929548156238129\n0.2828282828282829 12.92128367263648\n0.30303030303030304 11.298172635445361\n0.32323232323232326 9.966809927717833\n0.3434343434343435 8.860426554171964\n0.36363636363636365 7.930268595041323\n0.38383838383838387 7.1400642169759925\n0.4040404040404041 6.462376351902866\n0.42424242424242425 5.876140814452502\n0.4444444444444445 5.364969135802469\n0.4646464646464647 4.9159562148764175\n0.48484848484848486 4.518828196740127\n0.5050505050505051 4.165323987348229\n0.5252525252525253 3.848739962230637\n0.5454545454545455 3.5635904499540856\n0.5656565656565657 3.305351527280638\n0.5858585858585859 3.0702655556635308\n0.6060606060606061 2.8551905417814507\n0.6262626262626263 2.65748294812874\n0.6464646464646465 2.474905726496339\n0.6666666666666667 2.305555555555555\n0.686868686868687 2.1478048326048214\n0.7070707070707072 2.0002550968351827\n0.7272727272727273 1.8616993801652892\n0.7474747474747475 1.7310915822381832\n0.7676767676767677 1.6075214108402638\n0.787878787878788 1.4901937611727818\n0.8080808080808082 1.3784116576114678\n0.8282828282828284 1.27156207154134\n0.8484848484848485 1.1691040741365415\n0.8686868686868687 1.0705588948578604\n0.888888888888889 0.9755015432098765\n0.9090909090909092 0.8835537190082641\n0.9292929292929294 0.7943777895623855\n0.9494949494949496 0.7076716541475031\n0.9696969696969697 0.6231643494605139\n0.98989898989899 0.5406122763442311\n1.0101010101010102 0.4597959493929188\n1.0303030303030305 0.3805171882397417\n1.0505050505050506 0.302596683242079\n1.0707070707070707 0.22587187959584054\n1.090909090909091 0.15019513314967814\n1.1111111111111112 0.07543209876543189\n1.1313131313131315 0.0014603183062319447\n1.1515151515151516 -0.07183201951522311\n1.1717171717171717 -0.14454717092494984\n1.191919191919192 -0.2167788004559923\n1.2121212121212122 -0.2886128328741967\n1.2323232323232325 -0.36012820815496915\n1.2525252525252526 -0.4313975519179223\n1.272727272727273 -0.5024877719682923\n1.292929292929293 -0.5734605901083917\n1.3131313131313131 -0.6443730171236\n1.3333333333333335 -0.7152777777777782\n1.3535353535353536 -0.7862236917418948\n1.373737373737374 -0.8572560156014735\n1.393939393939394 -0.9284167504222499\n1.4141414141414144 -0.9997449187817161\n1.4343434343434345 -1.0712768146824043\n1.4545454545454546 -1.143046229338843\n1.474747474747475 -1.2150846554637709\n1.494949494949495 -1.2874214723620951\n1.5151515151515154 -1.3600841138659328\n1.5353535353535355 -1.4330982209048717\n1.5555555555555556 -1.5064877802973042\n1.575757575757576 -1.58027525116686\n1.595959595959596 -1.6544816802290594\n1.6161616161616164 -1.729126807054128\n1.6363636363636365 -1.8042291602897669\n1.6565656565656568 -1.8798061457204203\n1.676767676767677 -1.9558741269450486\n1.696969696969697 -2.032448499372201\n1.7171717171717173 -2.1095437581575784\n1.7373737373737375 -2.187173560644274\n1.7575757575757578 -2.2653507838082487\n1.777777777777778 -2.344087577160494\n1.7979797979797982 -2.423395411511965\n1.8181818181818183 -2.503285123966943\n1.8383838383838385 -2.583766959474586\n1.8585858585858588 -2.664850609236279\n1.878787878787879 -2.7465452462378015\n1.8989898989898992 -2.828859558149688\n1.9191919191919193 -2.9118017778162164\n1.9393939393939394 -2.9953797115329435\n1.9595959595959598 -3.079600765294187\n1.97979797979798 -3.1644719691753447\n2.0 -3.25\n\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n  \n\n\n\nxlist = np.linspace(-2.0, 2.0, 100)\nylist = np.linspace(-2.0, 2.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncontours = ax.contour(X, Y, Z,levels=[0.1,0.2,.3,.4,.5,.6,.7,.8,.9,1])#,levels=[.5**(.5),2,3,4,5,6,7,8,9])\nplt.clabel(contours, inline=True, fontsize=8)\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(-4,5,600)\nplt.plot(d,1-d)\nplt.xlim(-1,1)\nplt.ylim(-1,1)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-7.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n(2*0.5**2)**(.5)\n\n0.7071067811865476"
  },
  {
    "objectID": "notebooks/anscombe.html",
    "href": "notebooks/anscombe.html",
    "title": "Anscombes Quartet",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/hyperparams-experiments.html",
    "href": "notebooks/hyperparams-experiments.html",
    "title": "Hyperparams Tuning Strategies Experimentation",
    "section": "",
    "text": "import numpy as np \nnp.random.seed(20)\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\n\nMakeMoons Dataset\n1.1 Fixed Train-Test (70:30) split ; No Tuning\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n# Split the data into training, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n\nlatexify(fig_width=5, fig_height=4)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label='Train') \nformat_axes(plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\n\n#hyperparameters take their default values\ndt_classifier = DecisionTreeClassifier(random_state=42)\ndt_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ntest_accuracy = dt_classifier.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTest set accuracy: 0.8833\n\n\n1.2 Multiple Random Train-Test splits\n\n# Initialize an empty list to store the accuracy metrics\naccuracy_metrics = []\nall_test_sets = []\nall_predictions = []\n\n# Perform 20 random train-test splits and repeat the fit\nfor _ in range(20):\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=np.random.randint(100))\n    \n    # Create and fit the decision tree classifier\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(X_train, y_train)\n\n    current_predictions = dt_classifier.predict(X_test)\n    all_predictions.append(current_predictions)\n    current_accuracy = np.mean(current_predictions == y_test)\n    all_test_sets.append(y_test)\n    \n    # Calculate the accuracy on the test set\n    test_accuracy = dt_classifier.score(X_test, y_test)\n    \n    # Append the accuracy to the list\n    accuracy_metrics.append(test_accuracy)\n\n# Calculate the mean and standard deviation of the accuracy metrics\nmean_accuracy = np.mean(accuracy_metrics)\nstd_accuracy = np.std(accuracy_metrics)\n\n# Print the mean and standard deviation\nprint(\"Mean accuracy: {:.4f}\".format(mean_accuracy))\nprint(\"Standard deviation: {:.4f}\".format(std_accuracy))\n\n# Print minimum and maximum accuracies\nprint(\"Minimum accuracy: {:.4f}\".format(min(accuracy_metrics)))\nprint(\"Maximum accuracy: {:.4f}\".format(max(accuracy_metrics)))\n\nMean accuracy: 0.8932\nStandard deviation: 0.0165\nMinimum accuracy: 0.8633\nMaximum accuracy: 0.9133\n\n\n1.3 K-Fold Cross Validation\n\nimport numpy as np\n# Define the number of folds (k)\nk = 5\n\n# Initialize lists to store predictions and accuracies\npredictions = {}\naccuracies = []\n\n# Calculate the size of each fold\nfold_size = len(X) // k\n\n# Perform k-fold cross-validation\nfor i in range(k):\n    # Split the data into training and test sets\n    test_start = i * fold_size\n    test_end = (i + 1) * fold_size\n    test_set = X[test_start:test_end]\n    test_labels = y[test_start:test_end]\n    \n    training_set = np.concatenate((X[:test_start], X[test_end:]), axis=0)\n    training_labels = np.concatenate((y[:test_start], y[test_end:]), axis=0)\n    \n    # Train the model\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(training_set, training_labels)\n    \n    # Make predictions on the validation set\n    fold_predictions = dt_classifier.predict(test_set)\n    \n    # Calculate the accuracy of the fold\n    fold_accuracy = np.mean(fold_predictions == test_labels)\n    \n    # Store the predictions and accuracy of the fold\n    predictions[i] = fold_predictions\n    accuracies.append(fold_accuracy)\n\n# Print the predictions and accuracies of each fold\nfor i in range(k):\n    print(\"Fold {}: Accuracy: {:.4f}\".format(i+1, accuracies[i]))\n\nFold 1: Accuracy: 0.8700\nFold 2: Accuracy: 0.8850\nFold 3: Accuracy: 0.9300\nFold 4: Accuracy: 0.8650\nFold 5: Accuracy: 0.8850\n\n\n\nfrom sklearn.model_selection import KFold\n\n# Define the number of folds (k)\nk = 5\n\n# Initialize lists to store predictions and accuracies\npredictions = {}\naccuracies = []\n\n# Create a KFold instance\nkf = KFold(n_splits=k, shuffle=False)\n\n# Perform k-fold cross-validation\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    # Split the data into training and test sets\n    training_set, test_set = X[train_index], X[test_index]\n    training_labels, test_labels = y[train_index], y[test_index]\n    \n    # Train the model\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(training_set, training_labels)\n    \n    # Make predictions on the validation set\n    fold_predictions = dt_classifier.predict(test_set)\n    \n    # Calculate the accuracy of the fold\n    fold_accuracy = np.mean(fold_predictions == test_labels)\n    \n    # Store the predictions and accuracy of the fold\n    predictions[i] = fold_predictions\n    accuracies.append(fold_accuracy)\n\n    # Print the predictions and accuracy of each fold\n    print(\"Fold {}: Accuracy: {:.4f}\".format(i+1, fold_accuracy))\n\nFold 1: Accuracy: 0.8700\nFold 2: Accuracy: 0.8850\nFold 3: Accuracy: 0.9300\nFold 4: Accuracy: 0.8650\nFold 5: Accuracy: 0.8850\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Method 1 for computing accuracy\naccuracy_1 = accuracy_score(y, np.concatenate(list(predictions.values())))\n\n# Calculate macro-averaged accuracy\naccuracy_2 = np.mean(accuracies)\n\n# Print the micro and macro averaged accuracy\nprint(\"Method 1 accuracy: {:.4f}\".format(accuracy_1))\nprint(\"Method2 accuracy: {:.4f}\".format(accuracy_2))\n\nMethod 1 accuracy: 0.8870\nMethod2 accuracy: 0.8870\n\n\n2.1 Fixed Train-Test Split (hyperparameters tuned on Validation set)\n2.1.1 Validation Set as fixed Subset of Training Set\n\n# Step 1: Split the data into training, validation, and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=42)\n\n\nprint(\"Number of training examples: {}\".format(len(X_train)))\nprint(\"Number of validation examples: {}\".format(len(X_val)))\nprint(\"Number of testing examples: {}\".format(len(X_test)))\n\nNumber of training examples: 500\nNumber of validation examples: 200\nNumber of testing examples: 300\n\n\n\nhyperparameters = {}\nhyperparameters['max_depth'] = [1,2,3,4,5,6,7,8,9,10]\nhyperparameters['min_samples_split'] = [2,3,4,5,6,7,8]\nhyperparameters['criteria_values'] = ['gini', 'entropy']\n\nbest_accuracy = 0\nbest_hyperparameters = {}\n\nout = {}\ncount = 0\nfor max_depth in hyperparameters['max_depth']:\n    for min_samples_split in hyperparameters['min_samples_split']:\n        for criterion in hyperparameters['criteria_values']:\n            # Create and fit the decision tree classifier with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate the performance on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            out[count] = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'criterion': criterion, 'val_accuracy': val_accuracy}\n            count += 1\n\n\nhparam_df = pd.DataFrame(out).T\nhparam_df\n\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n1\n2\ngini\n0.785\n\n\n1\n1\n2\nentropy\n0.785\n\n\n2\n1\n3\ngini\n0.785\n\n\n3\n1\n3\nentropy\n0.785\n\n\n4\n1\n4\ngini\n0.785\n\n\n...\n...\n...\n...\n...\n\n\n135\n10\n6\nentropy\n0.895\n\n\n136\n10\n7\ngini\n0.89\n\n\n137\n10\n7\nentropy\n0.895\n\n\n138\n10\n8\ngini\n0.885\n\n\n139\n10\n8\nentropy\n0.895\n\n\n\n\n140 rows  4 columns\n\n\n\n\n\nhparam_df.sort_values(by='val_accuracy', ascending=False).head(10)\n\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n76\n6\n5\ngini\n0.925\n\n\n77\n6\n5\nentropy\n0.925\n\n\n78\n6\n6\ngini\n0.925\n\n\n79\n6\n6\nentropy\n0.925\n\n\n80\n6\n7\ngini\n0.925\n\n\n81\n6\n7\nentropy\n0.925\n\n\n83\n6\n8\nentropy\n0.925\n\n\n70\n6\n2\ngini\n0.92\n\n\n82\n6\n8\ngini\n0.92\n\n\n90\n7\n5\ngini\n0.915\n\n\n\n\n\n\n\n\n\nbest_hyperparameters_row = hparam_df.iloc[hparam_df['val_accuracy'].idxmax()]\nbest_accuracy = best_hyperparameters_row['val_accuracy']\nbest_hyperparameters = best_hyperparameters_row[['max_depth', 'min_samples_split', 'criterion']].to_dict()\n\n\n# Evaluate the performance of the selected hyperparameter combination on the test set\ndt_classifier = DecisionTreeClassifier(max_depth=best_hyperparameters['max_depth'], \n                                       min_samples_split=best_hyperparameters['min_samples_split'], \n                                       criterion=best_hyperparameters['criterion'], \n                                       random_state=42)\ndt_classifier.fit(X_train_val, y_train_val)\ntest_accuracy = dt_classifier.score(X_test, y_test)\n\nprint(\"Best Hyperparameters:\", best_hyperparameters)\nprint(\"Validation Set accuracy: {:.4f}\".format(best_accuracy))\nprint(\"Test Set accuracy: {:.4f}\".format(test_accuracy))\n\nBest Hyperparameters: {'max_depth': 6, 'min_samples_split': 5, 'criterion': 'gini'}\nValidation Set accuracy: 0.9250\nTest Set accuracy: 0.9067\n\n\nAvoiding nested loops by using itertools.product\nfor max_depth in hyperparameters['max_depth']:\n    for min_samples_split in hyperparameters['min_samples_split']:\n        for criterion in hyperparameters['criteria_values']:\n            # Create and fit the decision tree classifier with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate the performance on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            out[count] = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'criterion': criterion, 'val_accuracy': val_accuracy}\n            count += 1\n\nfrom itertools import product\n\nfor max_depth, min_samples_split, criterion in product(hyperparameters['max_depth'], hyperparameters['min_samples_split'], hyperparameters['criteria_values']):\n    # Define the Decision Tree Classifier\n    dt_classifier = DecisionTreeClassifier(\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        criterion=criterion,\n        random_state=42\n    )\n    dt_classifier.fit(X_train, y_train)\n\n2.1.2 Multiple random subsets of Training Set used as Validation Set\n\n# Initialize a list to store the optimal hyperparameters for each validation set\noptimal_hyperparameters = {}\ntest_accuracies = []\n\n# Set the number of subsets and iterations\nnum_subsets = 5\n\n# Make a pandas dataframe with columns as the hyperparameters, subset number, and validation accuracy\nhyperparameters_df = pd.DataFrame(columns=['max_depth', 'min_samples_split', 'criterion', 'subset', 'validation accuracy'])\n\n# Iterate over the subsets\nfor i in range(num_subsets):\n    # Split the data into training and validation sets\n    X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=i)\n    \n    # Initialize variables to store the best hyperparameters and accuracy for the current subset\n    best_accuracy = 0\n    best_hyperparameters = {}\n    \n    # Iterate over the hyperparameter values\n\n    for max_depth in hyperparameters['max_depth']:\n        for min_samples_split in hyperparameters['min_samples_split']:\n            for criterion in hyperparameters['criteria_values']:\n                # Initialize and train the model with the current hyperparameters\n                dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n                dt_classifier.fit(X_train_subset, y_train_subset)\n                \n                # Evaluate the model on the validation set\n                val_accuracy = dt_classifier.score(X_val_subset, y_val_subset)\n                hyperparameters_df.loc[len(hyperparameters_df)] = [max_depth, min_samples_split, criterion, i+1, val_accuracy]\n                \n                # Update the best accuracy and hyperparameters\n                if val_accuracy &gt; best_accuracy:\n                    best_accuracy = val_accuracy\n                    best_hyperparameters = {\n                        'max_depth': max_depth,\n                        'min_samples_split': min_samples_split,\n                        'criterion': criterion\n                    }\n    \n    optimal_hyperparameters[i] = best_hyperparameters\n\n    # Evaluate the model with the best hyperparameters on the test set\n    dt_classifier = DecisionTreeClassifier(max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], criterion=best_hyperparameters['criterion'], random_state=42)\n    dt_classifier.fit(X_train_val, y_train_val)\n    test_accuracy = dt_classifier.score(X_test, y_test)\n    test_accuracies.append(test_accuracy)\n\n\n\nprint(\"Optimal hyperparameters for {} inner folds/validation sets\".format(num_subsets))\nprint()\n# Print the optimal hyperparameters for each validation set\nfor i in range(num_subsets):\n    print(\"Optimal hyperparameters for validation set {}: {}\".format(i+1, optimal_hyperparameters[i]))\n    print(\"Test Accuracy for validation set {}: {:.4f}\".format(i+1, test_accuracies[i]))\n\nOptimal hyperparameters for 5 inner folds/validation sets\n\nOptimal hyperparameters for validation set 1: {'max_depth': 7, 'min_samples_split': 6, 'criterion': 'entropy'}\nTest Accuracy for validation set 1: 0.9000\nOptimal hyperparameters for validation set 2: {'max_depth': 5, 'min_samples_split': 7, 'criterion': 'gini'}\nTest Accuracy for validation set 2: 0.9033\nOptimal hyperparameters for validation set 3: {'max_depth': 6, 'min_samples_split': 2, 'criterion': 'entropy'}\nTest Accuracy for validation set 3: 0.9233\nOptimal hyperparameters for validation set 4: {'max_depth': 7, 'min_samples_split': 4, 'criterion': 'entropy'}\nTest Accuracy for validation set 4: 0.9000\nOptimal hyperparameters for validation set 5: {'max_depth': 6, 'min_samples_split': 2, 'criterion': 'entropy'}\nTest Accuracy for validation set 5: 0.9233\n\n\n\nhyperparameters_df\n\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nsubset\nvalidation accuracy\n\n\n\n\n0\n1\n2\ngini\n1\n0.790\n\n\n1\n1\n2\nentropy\n1\n0.790\n\n\n2\n1\n3\ngini\n1\n0.790\n\n\n3\n1\n3\nentropy\n1\n0.790\n\n\n4\n1\n4\ngini\n1\n0.790\n\n\n...\n...\n...\n...\n...\n...\n\n\n695\n10\n6\nentropy\n5\n0.900\n\n\n696\n10\n7\ngini\n5\n0.905\n\n\n697\n10\n7\nentropy\n5\n0.900\n\n\n698\n10\n8\ngini\n5\n0.905\n\n\n699\n10\n8\nentropy\n5\n0.900\n\n\n\n\n700 rows  5 columns\n\n\n\n\n\ngrouped_df = hyperparameters_df.groupby(['max_depth', 'min_samples_split', 'criterion']).mean()['validation accuracy']\ngrouped_df\n\nmax_depth  min_samples_split  criterion\n1          2                  entropy      0.769\n                              gini         0.771\n           3                  entropy      0.769\n                              gini         0.771\n           4                  entropy      0.769\n                                           ...  \n10         6                  gini         0.889\n           7                  entropy      0.902\n                              gini         0.894\n           8                  entropy      0.904\n                              gini         0.893\nName: validation accuracy, Length: 140, dtype: float64\n\n\n\ngrouped_df.sort_values(ascending=False).head(10)\n\nmax_depth  min_samples_split  criterion\n6          7                  entropy      0.914\n           8                  entropy      0.914\n7          7                  entropy      0.912\n6          6                  entropy      0.912\n7          8                  entropy      0.912\n           6                  entropy      0.910\n6          4                  entropy      0.910\n           5                  entropy      0.910\n7          4                  entropy      0.909\n           5                  entropy      0.909\nName: validation accuracy, dtype: float64\n\n\n\noptimal_hyperparams = grouped_df.idxmax()\noptimal_hyperparams\n\n(6, 7, 'entropy')\n\n\n\ndf_classifier = DecisionTreeClassifier(max_depth=optimal_hyperparams[0], min_samples_split=optimal_hyperparams[1], criterion=optimal_hyperparams[2], random_state=42)\ndf_classifier.fit(X_train_val, y_train_val)\ntest_accuracy = df_classifier.score(X_test, y_test)\nprint(\"Test accuracy: {:.4f}\".format(test_accuracy))\n\nTest accuracy: 0.9233\n\n\n2.2 Nested Cross-Validation\n\nhyperparameters['max_depth'] = [1,2,3,4,5,6,7,8,9,10]\nhyperparameters['min_samples_split'] = [2,3,4,5,6,7,8]\nhyperparameters['criteria_values'] = ['gini', 'entropy']\n\n\nnum_outer_folds = 5\nnum_inner_folds = 5\n\nkf_outer = KFold(n_splits=num_outer_folds, shuffle=False)\nkf_inner = KFold(n_splits=num_inner_folds, shuffle=False)\n\n# Initialize lists to store the accuracies for the outer and inner loops\nouter_loop_accuracies = []\ninner_loop_accuracies = []\n\nresults= {}\nouter_count = 0\noverall_count = 0\n# Iterate over the outer folds\nfor outer_train_index, outer_test_index in kf_outer.split(X):\n    # Split the data into outer training and test sets\n    X_outer_train, X_outer_test = X[outer_train_index], X[outer_test_index]\n    y_outer_train, y_outer_test = y[outer_train_index], y[outer_test_index]\n    \n    \n    inner_count = 0\n    \n    for innner_train_index, inner_test_index in kf_inner.split(X_outer_train):\n        print(\"Outer Fold {}, Inner Fold {}\".format(outer_count+1, inner_count+1))\n        # Split the data into inner training and test sets\n        X_inner_train, X_inner_test = X_outer_train[innner_train_index], X_outer_train[inner_test_index]\n        y_inner_train, y_inner_test = y_outer_train[innner_train_index], y_outer_train[inner_test_index]\n        \n        for max_depth, min_samples_split, criterion in product(hyperparameters['max_depth'],\n                                                               hyperparameters['min_samples_split'],\n                                                               hyperparameters['criteria_values']):\n            \n            #print(max_depth, min_samples_split, criterion)\n            # Initialize and train the model with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, \n                                                   min_samples_split=min_samples_split, \n                                                   criterion=criterion, random_state=42)\n            dt_classifier.fit(X_inner_train, y_inner_train)\n            \n            # Evaluate the model on the inner test set\n            val_accuracy = dt_classifier.score(X_inner_test, y_inner_test)\n            \n            results[overall_count] = {'outer_fold': outer_count, \n                                      'inner_fold': inner_count, \n                                      'max_depth': max_depth, \n                                      'min_samples_split': min_samples_split, \n                                      'criterion': criterion, \n                                      'val_accuracy': val_accuracy}\n            overall_count += 1\n\n        inner_count += 1\n    outer_count += 1\n    \n            \n            \n\nOuter Fold 1, Inner Fold 1\nOuter Fold 1, Inner Fold 2\nOuter Fold 1, Inner Fold 3\nOuter Fold 1, Inner Fold 4\nOuter Fold 1, Inner Fold 5\nOuter Fold 2, Inner Fold 1\nOuter Fold 2, Inner Fold 2\nOuter Fold 2, Inner Fold 3\nOuter Fold 2, Inner Fold 4\nOuter Fold 2, Inner Fold 5\nOuter Fold 3, Inner Fold 1\nOuter Fold 3, Inner Fold 2\nOuter Fold 3, Inner Fold 3\nOuter Fold 3, Inner Fold 4\nOuter Fold 3, Inner Fold 5\nOuter Fold 4, Inner Fold 1\nOuter Fold 4, Inner Fold 2\nOuter Fold 4, Inner Fold 3\nOuter Fold 4, Inner Fold 4\nOuter Fold 4, Inner Fold 5\nOuter Fold 5, Inner Fold 1\nOuter Fold 5, Inner Fold 2\nOuter Fold 5, Inner Fold 3\nOuter Fold 5, Inner Fold 4\nOuter Fold 5, Inner Fold 5\n\n\n\noverall_results = pd.DataFrame(results).T\n\n\noverall_results\n\n\n\n\n\n\n\n\n\nouter_fold\ninner_fold\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n0\n0\n1\n2\ngini\n0.7625\n\n\n1\n0\n0\n1\n2\nentropy\n0.7625\n\n\n2\n0\n0\n1\n3\ngini\n0.7625\n\n\n3\n0\n0\n1\n3\nentropy\n0.7625\n\n\n4\n0\n0\n1\n4\ngini\n0.7625\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3495\n4\n4\n10\n6\nentropy\n0.9\n\n\n3496\n4\n4\n10\n7\ngini\n0.91875\n\n\n3497\n4\n4\n10\n7\nentropy\n0.9\n\n\n3498\n4\n4\n10\n8\ngini\n0.925\n\n\n3499\n4\n4\n10\n8\nentropy\n0.9\n\n\n\n\n3500 rows  6 columns\n\n\n\n\nFind the best hyperparameters for each outer fold\n\nouter_fold = 0\nouter_fold_df = overall_results.query('outer_fold == @outer_fold')\nouter_fold_df\n\n\n\n\n\n\n\n\n\nouter_fold\ninner_fold\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n0\n0\n1\n2\ngini\n0.7625\n\n\n1\n0\n0\n1\n2\nentropy\n0.7625\n\n\n2\n0\n0\n1\n3\ngini\n0.7625\n\n\n3\n0\n0\n1\n3\nentropy\n0.7625\n\n\n4\n0\n0\n1\n4\ngini\n0.7625\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n695\n0\n4\n10\n6\nentropy\n0.85\n\n\n696\n0\n4\n10\n7\ngini\n0.86875\n\n\n697\n0\n4\n10\n7\nentropy\n0.85625\n\n\n698\n0\n4\n10\n8\ngini\n0.86875\n\n\n699\n0\n4\n10\n8\nentropy\n0.85625\n\n\n\n\n700 rows  6 columns\n\n\n\n\nAggregate the validation accuracies for each hyperparameter combination across all inner folds\n\nouter_fold_df.groupby(['max_depth', 'min_samples_split', 'criterion']).mean()['val_accuracy'].sort_values(ascending=False).head(10)\n\nmax_depth  min_samples_split  criterion\n6          7                  gini          0.9175\n           8                  gini          0.9175\n           6                  gini          0.9175\n           4                  gini         0.91625\n           3                  gini         0.91625\n           2                  gini         0.91625\n           5                  gini         0.91625\n7          6                  gini         0.91625\n           7                  gini         0.91625\n           8                  gini           0.915\nName: val_accuracy, dtype: object"
  },
  {
    "objectID": "notebooks/contour.html",
    "href": "notebooks/contour.html",
    "title": "Contour and Surface Plots",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2, fig_height=4.5)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport torch\n\ndef create_XYZ(f):\n    # Generate data\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    \n    # Convert to PyTorch tensors\n    X_torch = torch.from_numpy(X)\n    Y_torch = torch.from_numpy(Y)\n\n    # Evaluate the function\n    Z = f(X_torch, Y_torch)\n    return X, Y, Z, X_torch, Y_torch\n\ndef create_contour(X, Y, Z, ax2, alpha, scatter_pts=None, filled=True, levels=10, mark_levels=False):\n    if filled:\n        scatter_color='white'\n        contour = ax2.contourf(X, Y, Z.detach().numpy(), levels=levels, cmap='magma', alpha=alpha)\n    else:\n        scatter_color='black'\n        contour = ax2.contour(X, Y, Z.detach().numpy(), levels=levels, cmap='magma', alpha=alpha)\n    if scatter_pts is not None:\n        ax2.scatter(scatter_pts[0], scatter_pts[1], s=10, c=scatter_color)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Contour Plot')\n    \n\n    # Add a colorbar in between the subplots\n    divider = make_axes_locatable(ax2)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    cbar = plt.colorbar(contour, cax=cax)\n    return ax2, contour\n\ndef plot_surface_and_contour(f, function_name, uv = None, stride=4, alpha=1, scatter_pts=None, filled=True, levels=10):\n    X, Y, Z, X_torch, Y_torch = create_XYZ(f)\n\n    # Create the single figure with two subplots\n    fig = plt.figure()\n\n    # Plot the 3D surface on the first subplot\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot_surface(X, Y, Z.detach().numpy(), cmap='magma', edgecolor='none', alpha=alpha)  # Remove grid lines\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('z')\n    ax1.grid(False)\n    ax1.xaxis.pane.fill = False\n    ax1.yaxis.pane.fill = False\n    ax1.zaxis.pane.fill = False\n    ax1.view_init(elev=30, azim=30)\n    ax1.set_title('Surface Plot')\n    if scatter_pts is not None:\n        ax1.scatter(scatter_pts[0], scatter_pts[1], f(scatter_pts[0], scatter_pts[1]), s=100, c='black')\n    \n\n    # Plot the contour plot on the second subplot\n    ax2 = fig.add_subplot(122, aspect='equal')  # Set 1:1 aspect ratio\n    \n    ax2, contour = create_contour(X, Y, Z, ax2, alpha, scatter_pts, filled, levels)\n    file_name = f\"../figures/mml/contour-{function_name}.pdf\"\n    if uv is not None:\n        u = uv[0](X_torch, Y_torch)\n        v = uv[1](X_torch, Y_torch)\n        # Quiver plot for gradient\n        ax2.quiver(X[::stride, ::stride], Y[::stride, ::stride], u[::stride, ::stride].detach().numpy(),\n                   v[::stride, ::stride].detach().numpy(), scale=140)\n        # for c in contour, set alpha\n        for c in contour.collections:\n            c.set_alpha(0.5)\n        \n        \n        file_name = f\"../figures/mml/contour-{function_name}-with-gradient.pdf\"\n        \n\n    \n    # Save the figure\n    plt.tight_layout(pad=1.0, w_pad=1.0)\n    fig.savefig(file_name, bbox_inches=\"tight\")\n\n# Example usage:\n# Define your function f(x, y) and its gradient g(x, y)\n#f = lambda x, y: x**2 + y**2\n#g = lambda x, y: (2*x, 2*y)\n#plot_surface_and_contour(f, \"x_squared_plus_y_squared\", uv=(lambda x, y: 2*x, lambda x, y: 2*y))\n\n\nplot_surface_and_contour(lambda x, y: x**2 + y**2, \"x_squared_plus_y_squared_quiver\", \n                         uv=(lambda x, y: 2*x, lambda x, y: 2*y)\n                         ,stride=5)\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x**2 + y**2, \"x_squared_plus_y_squared\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x**2, \"x_squared\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: torch.abs(x) + torch.abs(y), \"mod_x_plus_mod_y\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: (x**2) * y, \"x_square_times_y\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x * y, \"x_times_y\")\n\n\n\n\n\n\n\n\n\ndef f(x, y):\n    return (14 + 3*x**2 +14*y**2 -12*x - 28*y + 12*x*y)/3\n\nx0, y0 = torch.tensor(4.0), torch.tensor(0.0)\n\n\n\ndel_x, del_y = torch.func.grad(f, argnums=(0, 1))(x0, y0)\nprint(del_x, del_y)\n\ntensor(4.) tensor(6.6667)\n\n\n\nlatexify(columns=2, fig_width=6.5, fig_height=3)\nX, Y, Z, X_torch, Y_torch = create_XYZ(f)\nlevels = [0, 10, 25, 50, 100, 200, 300, 400]\n\nxi = x0\nyi = y0\nalpha = 0.1\nfor i in range(50):\n    fig, ax = plt.subplots(ncols=2)\n    _, _ = create_contour(X, Y, Z, ax[0], alpha=0.8, scatter_pts=(xi, yi), filled=True, levels=levels)\n    # Mark the minima with horizontal and vertical lines\n    ax[0].axhline(y=yi, color='red', linestyle='--', alpha=0.7)\n    ax[0].axvline(x=xi, color='red', linestyle='--', alpha=0.7)\n    ax[0].scatter([0], [1], s=10, c='pink', marker='x', label='Minima')\n    del_x, del_y = torch.func.grad(f, argnums=(0, 1))(xi, yi)\n    xi = xi - alpha * del_x\n    yi = yi - alpha * del_y\n    print(xi, yi)\n    \n    # plot the line fit \n    x_line = np.linspace(0, 5, 100)\n    y_line = xi + yi*x_line\n    ax[1].plot(x_line, y_line, label='Line fit')\n    ax[1].set_ylim(-1, 7)\n    ax[1].plot(x_line, x_line, label='Actual line')\n    ax[1].legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"../figures/mml/gradient-descent-{i}.pdf\", bbox_inches=\"tight\")\n\n\n\n\ntensor(3.6000) tensor(-0.6667)\ntensor(3.5467) tensor(-0.5511)\ntensor(3.4578) tensor(-0.5221)\ntensor(3.3751) tensor(-0.4846)\ntensor(3.2939) tensor(-0.4490)\ntensor(3.2147) tensor(-0.4141)\ntensor(3.1374) tensor(-0.3802)\ntensor(3.0620) tensor(-0.3470)\ntensor(2.9884) tensor(-0.3146)\ntensor(2.9165) tensor(-0.2830)\ntensor(2.8464) tensor(-0.2522)\ntensor(2.7780) tensor(-0.2221)\ntensor(2.7112) tensor(-0.1927)\ntensor(2.6461) tensor(-0.1640)\ntensor(2.5824) tensor(-0.1360)\ntensor(2.5204) tensor(-0.1087)\ntensor(2.4598) tensor(-0.0821)\ntensor(2.4006) tensor(-0.0560)\ntensor(2.3429) tensor(-0.0307)\ntensor(2.2866) tensor(-0.0059)\ntensor(2.2316) tensor(0.0183)\ntensor(2.1780) tensor(0.0419)\ntensor(2.1256) tensor(0.0649)\ntensor(2.0745) tensor(0.0874)\ntensor(2.0247) tensor(0.1093)\ntensor(1.9760) tensor(0.1308)\ntensor(1.9285) tensor(0.1517)\ntensor(1.8821) tensor(0.1720)\ntensor(1.8369) tensor(0.1919)\ntensor(1.7927) tensor(0.2114)\ntensor(1.7496) tensor(0.2303)\ntensor(1.7076) tensor(0.2488)\ntensor(1.6665) tensor(0.2669)\ntensor(1.6265) tensor(0.2845)\ntensor(1.5874) tensor(0.3017)\ntensor(1.5492) tensor(0.3185)\ntensor(1.5120) tensor(0.3349)\ntensor(1.4756) tensor(0.3509)\ntensor(1.4401) tensor(0.3665)\ntensor(1.4055) tensor(0.3817)\ntensor(1.3717) tensor(0.3966)\ntensor(1.3388) tensor(0.4111)\ntensor(1.3066) tensor(0.4252)\ntensor(1.2752) tensor(0.4391)\ntensor(1.2445) tensor(0.4525)\ntensor(1.2146) tensor(0.4657)\ntensor(1.1854) tensor(0.4785)\ntensor(1.1569) tensor(0.4911)\ntensor(1.1291) tensor(0.5033)\ntensor(1.1020) tensor(0.5153)\n\n\n/tmp/ipykernel_4080399/2912499836.py:9: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n  fig, ax = plt.subplots(ncols=2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlevels = 10\ndef f1(x, y):\n    return (2-x-2*y)**2\n\ndef f2(x, y):\n    return (3-x-3*y)**2\n\ndef f3(x, y):\n    return (1-x-y)**2\n\nx0, y0 = torch.tensor(4.0), torch.tensor(0.0)\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(8, 3))\nX, Y, Z, X_torch, Y_torch = create_XYZ(f1)\n_, _ = create_contour(X, Y, Z, ax[0], alpha=0.8, scatter_pts=(x0, y0), filled=True, levels=levels)\n\nX, Y, Z, X_torch, Y_torch = create_XYZ(f2)\n_, _ = create_contour(X, Y, Z, ax[1], alpha=0.8, scatter_pts=(x0, y0), filled=True, levels=levels)\n\nX, Y, Z, X_torch, Y_torch = create_XYZ(f3)\n_, _ = create_contour(X, Y, Z, ax[2], alpha=0.8, scatter_pts=(x0, y0), filled=True, levels=levels)\n\nfig.tight_layout()\nplt.savefig(f\"../figures/mml/gradient-descent-3-functions.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nX, Y, Z, X_torch, Y_torch = create_XYZ(f1)\nalpha = 0.1\n\n\ndel_x, del_y = torch.func.grad(f1, argnums=(0, 1))(x0, y0)\nx1 = x0 - alpha * del_x\ny1 = y0 - alpha * del_y\n\nprint(x1, y1)\n\ntensor(3.6000) tensor(-0.8000)\n\n\n\ndel_x, del_y = torch.func.grad(f2, argnums=(0, 1))(x1, y1)\nx2 = x1 - alpha * del_x\ny2 = y1 - alpha * del_y\n\nprint(x2, y2)\n\ntensor(3.9600) tensor(0.2800)\n\n\n\ndel_x, del_y = torch.func.grad(f3, argnums=(0, 1))(x2, y2)\nx3 = x2 - alpha * del_x\ny3 = y2 - alpha * del_y\n\nprint(x3, y3)\n\ntensor(3.3120) tensor(-0.3680)"
  },
  {
    "objectID": "notebooks/Ridge.html",
    "href": "notebooks/Ridge.html",
    "title": "Question",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\n/tmp/ipykernel_2413694/3589217045.py:3: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify()\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nformat_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\nNameError: name 'format_axes' is not defined\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfor i, deg in enumerate([1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 20, 25]):\n    poly = PolynomialFeatures(degree=deg, include_bias=False)\n    X_ = poly.fit_transform(x.reshape(-1, 1))\n    from sklearn.linear_model import LinearRegression\n    clf = LinearRegression()\n    clf.fit(X_, data['y'])\n    print(X_.shape)\n    y_pred = clf.predict(X_)\n    plt.figure()\n    plt.plot(data['x'], y_pred, label='Degree: {}'.format(deg))\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.scatter(x, y)\n    plt.legend()\n    train_rmse = np.sqrt(np.mean((y_pred - y)**2))\n    highest_coef = np.max([np.max(clf.coef_), clf.intercept_])\n    sqrt_sum_squares_coef = np.sqrt(np.sum(clf.coef_**2) + clf.intercept_**2)\n    plt.title('RMSE: {:.2f}\\n Highest Coef: {:.2f}\\n Sqrt Sum of Squares of Coef: {:.2f}'.format(train_rmse, highest_coef, sqrt_sum_squares_coef))\n    #format_axes(plt.gca())\n    #plt.savefig('lin_{}.pdf'.format(i+2), transparent=True, bbox_inches=\"tight\")\n\n(60, 1)\n(60, 2)\n(60, 3)\n(60, 4)\n(60, 5)\n(60, 6)\n(60, 7)\n(60, 8)\n(60, 10)\n(60, 12)\n(60, 15)\n(60, 20)\n(60, 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\ncoef_list = []\nfor i,deg in enumerate([1,3,6,11]):\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=False, fit_intercept=False)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    coef_list.append(abs(max(regressor.coef_, key=abs)))\n\n    plt.scatter(data['x'],data['y'], label='Train')\n    plt.plot(data['x'], y_pred,'k', label='Prediction')\n    plt.plot(data['x'], y_true,'g.', label='True Function')\n    format_axes(plt.gca())\n    plt.legend() \n    plt.title(f\"Degree: {deg} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n    plt.savefig('lin_plot_{}.pdf'.format(deg), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\nplt.semilogy([1,3,6,11],coef_list,'o-k')\nplt.xticks([1,3,6,11])\nplt.xlabel('Degree')\nplt.ylabel('Max Coef')\nformat_axes(plt.gca())\nplt.savefig('lin_plot_coef.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\nfrom matplotlib.patches import Circle\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.4)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Circle((0, 0), 3, color='g', label=r'$\\theta_0^2+\\theta_1^2=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig('ridge_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes(projection='3d')\n\nax = plt.axes(projection='3d')\nax.plot_surface(x_grid, y_grid, cost_matrix,cmap='viridis', edgecolor='none')\nax.set_title('Least squares objective function');\nax.set_xlabel(r\"$\\theta_0$\")\nax.set_ylabel(r\"$\\theta_1$\")\nax.set_xlim([-4,15])\nax.set_ylim([-4,15])\n\nu = np.linspace(0, np.pi, 30)\nv = np.linspace(0, 2 * np.pi, 30)\n\n# x = np.outer(500*np.sin(u), np.sin(v))\n# y = np.outer(500*np.sin(u), np.cos(v))\n# z = np.outer(500*np.cos(u), np.ones_like(v))\n# ax.plot_wireframe(x, y, z)\n\nax.view_init(45, 120)\nplt.savefig('ridge_base_surface.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nlatexify(fig_width=5, fig_height=2.5)\nfrom sklearn.linear_model import Ridge\n\nfor alpha in [1, 10, 1000]:\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Ridge(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    format_axes(ax[0])\n    format_axes(ax[1])\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    p1 = Circle((0, 0), np.sqrt(regressor.coef_.T@regressor.coef_), alpha=0.6, color='g', label=r'$\\theta_0^2+\\theta_1^2={:.2f}$'.format(np.sqrt(regressor.coef_.T@regressor.coef_)))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.3)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Ridge Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n    ax[1].set_aspect('equal')\n    plt.savefig('ridge_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\nlatexify()\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        format_axes(plt.gca())\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('ridge_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.62699e-21): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\n# from sklearn.linear_model import Ridge\n\n# for i,deg in enumerate([2,4,8,16]):\n#   predictors = ['x']\n#   if deg &gt;= 2:\n#     predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n#   fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20, 4))\n\n#   for i,alpha in enumerate([1e-15,1e-4,1,20]):\n#     regressor = Ridge(alpha=alpha,normalize=True)\n#     regressor.fit(data[predictors],data['y'])\n#     y_pred = regressor.predict(data[predictors])\n#     ax[i].scatter(data['x'],data['y'], label='Train')\n#     ax[i].plot(data['x'], y_pred,'k', label='Prediction')\n#     ax[i].plot(data['x'], y_true,'g.', label='True Function')\n#     ax[i].legend() \n#     ax[i].set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"data.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-2,10,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Ridge(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nformat_axes(plt.gca())\n\nlim = True\nif lim:\n    plt.ylim((10e-20, 100))\n    plt.savefig('rid_reg-without-lim.pdf', transparent=True, bbox_inches=\"tight\")\nelse:\n    plt.savefig('rid_reg-with-lim.pdf', transparent=True, bbox_inches=\"tight\")\n\n# plt.set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\n\n\n\n\n\n\n\nplt.style.use('seaborn-whitegrid')\n\nx = [1,2,3,4]\ny = [1,2,3,0]\ny_1 = [(2-i/5) for i in x]\ny_2 = [(0.5+0.4*i) for i in x]\nplt.ylim(-0.2,3.3)\nplt.plot(x,y,'.')\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\n#plt.plot(x,y_1, label=\"unreg\")\n#plt.plot(x,y_2, label=\"reg\")\n#plt.legend()\n#format_axes(plt.gca())\nplt.savefig('temp.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nx_new = np.vstack([np.ones_like(x), x]).T\nregressor = LinearRegression(normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_unreg.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nregressor = Ridge(alpha=4, normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred_r = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\ndef ridge_func(x):\n    return 0.56 + 0.26*np.array(x)\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.plot(x,ridge_func(x),'-b', label='Regularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_reg.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nregressor.coef_\n\narray([0.37209302, 0.30232558])\n\n\n\nRetrying with a better example\n\nnp.linalg.inv([[4, 10], [10, 34]])@np.array([6, 14])\n\narray([ 1.77777778, -0.11111111])\n\n\n\nnp.linalg.inv([[8, 10], [10, 34]])@np.array([6, 14])\n\narray([0.37209302, 0.30232558])\n\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,600,10)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,5,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n#format_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([17]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1e-3, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        #format_axes(plt.gca())\n        #print(regressor.coef_)\n        plt.ylim([0,60])\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha}\")\n        plt.savefig('ridge_new_{}_{}.pdf'.format(i, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.11479e-28): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;"
  },
  {
    "objectID": "notebooks/transcript.html",
    "href": "notebooks/transcript.html",
    "title": "YouTube video to transcript using openAI whisper and summary using OLLama",
    "section": "",
    "text": "try:\n    from pydub import AudioSegment\nexcept ImportError:\n    %pip install pydub\n    %pip install pydub[extras]\n    from pydub import AudioSegment\n    from pydub.playback import play\n\n\nfrom IPython.display import Audio\naudio_path = '../datasets/audio/Prime-minister.m4a'\naudio = AudioSegment.from_file(audio_path, format=\"m4a\")\naudio\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\ntry:    \n    import whisper\nexcept ImportError:\n    %pip install openai-whisper\n    import whisper\n\n\nwhisper_model = whisper.load_model(\"base.en\")\n\n\ntranscription = whisper_model.transcribe(audio_path, fp16=True, verbose=False)\n\n100%|| 347/347 [00:00&lt;00:00, 367.83frames/s]\n\n\n\ntranscription\n\n{'text': ' Who is the Prime Minister of India?',\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 3.0,\n   'text': ' Who is the Prime Minister of India?',\n   'tokens': [50363, 5338, 318, 262, 5537, 4139, 286, 3794, 30, 50513],\n   'temperature': 0.0,\n   'avg_logprob': -0.34697675704956055,\n   'compression_ratio': 0.813953488372093,\n   'no_speech_prob': 0.005249415524303913}],\n 'language': 'en'}\n\n\n\nfrom IPython.display import Audio\n\n\ntry:\n    from gtts import gTTS\nexcept ImportError:\n    %pip install gtts\n    from gtts import gTTS\n\n\ndef speak(text, file):\n    tts = gTTS(text, lang='en')\n    with open(file, 'wb') as f:\n        tts.write_to_fp(f)\n    return Audio(file)\n\n\nspeak(transcription['text'], '../datasets/audio/pm-2.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n\n\nllm = Ollama(model=\"llama2\", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n\n\ndef answers(llm, prompt_qs, prompts, text):\n    outputs = []\n    for prompt, prompt_qs in zip(prompts, prompt_qs):\n        print(prompt_qs, end=\"\\n\")\n        output = llm(prompt, temperature=0.5)\n        #print(output, end=\"\\n\\n\")\n        print(\"\\n\" + \"==\"*50, end=\"\\n\\n\")\n    outputs.append(output) \n    return outputs\n\n\nprompt_qs = [\"Please be concise.\"] \nprompts = [q + \":\"+ transcription[\"text\"] for q in prompt_qs]\n\noutputs = answers(llm, prompt_qs, prompts, transcription[\"text\"])\n\nPlease be concise.\n\nThe Prime Minister of India is Narendra Modi.\n====================================================================================================\n\n\n\n\nspeak(outputs[0].replace(\"\\n\", \"\"), '../datasets/audio/pm-answer.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nReferences\n\nWhisper\nLangchain and LLama\nEnglish to Hindi using Transformers\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('https://www.youtube.com/watch?v=CuBzyh4Xmvk', width=500, height=300)\n\n\n        \n        \n\n\n\ntry:\n    import yt_dlp\nexcept ImportError:\n    %pip install yt_dlp\n    import yt_dlp\n\n\ndef download(video_id: str, save_path: str) -&gt; str:\n    video_url = f'https://www.youtube.com/watch?v={video_id}'\n    ydl_opts = {\n        'format': 'm4a/bestaudio/best',\n        'paths': {'home': save_path},\n        'outtmpl': {'default': \"lecture.m4a\"},\n        'postprocessors': [{\n            'key': 'FFmpegExtractAudio',\n            'preferredcodec': 'm4a',\n        }]\n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        error_code = ydl.download([video_url])\n        if error_code != 0:\n            raise Exception('Failed to download video')\n\n    return save_path\n\n\ndownload('CuBzyh4Xmvk', '../datasets/audio/')\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=CuBzyh4Xmvk\n[youtube] CuBzyh4Xmvk: Downloading webpage\n[youtube] CuBzyh4Xmvk: Downloading ios player API JSON\n[youtube] CuBzyh4Xmvk: Downloading android player API JSON\n[youtube] CuBzyh4Xmvk: Downloading m3u8 information\n[info] CuBzyh4Xmvk: Downloading 1 format(s): 140\n[download] ../datasets/audio/lecture.m4a has already been downloaded\n[download] 100% of   72.26MiB\n[ExtractAudio] Not converting audio ../datasets/audio/lecture.m4a; file is already in target format m4a\n\n\n'../datasets/audio/'\n\n\n\naudio_path = '../datasets/audio/lecture.m4a'\naudio = AudioSegment.from_file(audio_path, format=\"m4a\")\n\n\naudio[:13000]\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\ntranscription = whisper_model.transcribe(\"../datasets/audio/lecture.m4a\", fp16=True, verbose=False)\n\n 99%|| 465481/468481 [02:07&lt;00:00, 3643.86frames/s]\n\n\n\nprint(transcription[\"text\"][:500].replace(\". \", \"\\n\"))\n\n Please look at the code mentioned above and please sign up on the Google Cloud\nWe've already started making some announcements\nYou will likely end up missing the announcements and you'll have no one else to play with\nThe second quick logistical announcement is that we'll have an extra lecture on Saturday, 11th Jan at 11am in 1.101\nSo a lot of ones over there\nAnd I think one or two people still have conflict, but in the larger, in the larger phone we'll have almost everyone available, so we\n\n\n\ntranscription.keys()\n\ndict_keys(['text', 'segments', 'language'])\n\n\n\ndef create_srt_from_transcription(transcription_objects, srt_file_path):\n    with open(srt_file_path, 'w') as srt_file:\n        index = 1  # SRT format starts with index 1\n\n        for entry in transcription_objects['segments']:\n            start_time = entry['start']\n            end_time = entry['end']\n            text = entry['text']\n\n            # Convert time to SRT format\n            start_time_str = format_time(start_time)\n            end_time_str = format_time(end_time)\n\n            # Write entry to SRT file\n            srt_file.write(f\"{index}\\n\")\n            srt_file.write(f\"{start_time_str} --&gt; {end_time_str}\\n\")\n            srt_file.write(f\"{text}\\n\\n\")\n\n            index += 1\n\ndef format_time(time_seconds):\n    minutes, seconds = divmod(time_seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},000\"\n\n\ncreate_srt_from_transcription(transcription, \"../datasets/audio/lecture.srt\")\n\n\n!head ../datasets/audio/lecture.srt\n\n1\n00:00:00,000 --&gt; 00:00:05,000\n Please look at the code mentioned above and please sign up on the Google Cloud.\n\n2\n00:00:05,000 --&gt; 00:00:08,000\n We've already started making some announcements.\n\n3\n00:00:08,000 --&gt; 00:00:14,000\n\n\n\nspeak(transcription['text'][:1300], '../datasets/audio/hello.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ntry:\n    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nexcept:\n    %pip install transformers -U -q\n    %pip install sentencepiece\n    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n    \n\n\n\n# download and save model\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n\n# import tokenizer\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n\n\ntext_to_translate = transcription[\"text\"][:500].split(\". \")\ntext_to_translate\n\n[' Please look at the code mentioned above and please sign up on the Google Cloud',\n \"We've already started making some announcements\",\n \"You will likely end up missing the announcements and you'll have no one else to play with\",\n \"The second quick logistical announcement is that we'll have an extra lecture on Saturday, 11th Jan at 11am in 1.101\",\n 'So a lot of ones over there',\n \"And I think one or two people still have conflict, but in the larger, in the larger phone we'll have almost everyone available, so we\"]\n\n\n\nmodel_inputs = tokenizer(text_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n\n\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n)\n\ntranslation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\n\n\ntranslation\n\n['       Google     ',\n '          ',\n '                  ',\n '           Saturday, 11th Jan 11am  1.101  ',\n '    ',\n '            ,  ,         ,  ']\n\n\n\nllm = Ollama(model=\"mistral\", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\nprompt_qs = [\"Please provide a bullet-point summary for the given text:\",\n             \"Highlight the important topics and subtopics in the given lecture:\",\n             \"Give us some question for a quiz based on the following text:\",\n             \"Summarize the following text in Hindi in 10 lines or less:\",\n            ]\n\nprompts = [q + \"\\n\\n\" + transcription[\"text\"] for q in prompt_qs]\n\noutputs = answers(llm, prompt_qs, prompts, transcription[\"text\"])\n\nPlease provide a bullet-point summary for the given text:\n * The text discusses a machine learning course and announces several logistical matters, including signing up for Google Cloud, an extra lecture on Saturday, and providing access to Google Docs for FAQ and project questions.\n* The definition of machine learning is discussed, with the ability to learn without explicit programming being highlighted.\n* A task to recognize digits from a dataset is introduced as an example, and rules are suggested for recognizing the digit \"4\".\n* It is explained that traditional programming involves explicitly programming rules, while machine learning involves using data and experience to learn patterns and make predictions.\n* An example of predicting tomato quality based on visual features is given, with the goal being to scale up this process in a business setting.\n* The concept of precision and recall in machine learning evaluation metrics is touched upon, as well as the idea of a decision tree algorithm for classification tasks.\n* The text encourages students to come up with simple rules for recognizing patterns and using decision trees to make predictions based on those rules.\n* The greedy algorithm for finding the best attribute for splitting data in a decision tree is mentioned, along with the concept of entropy as a measure of disorder or uncertainty in a dataset.\n====================================================================================================\n\nHighlight the important topics and subtopics in the given lecture:\n The given lecture covers several important topics related to machine learning, including:\n\n1. Machine Learning Definition and Concepts\n* Explicit programming vs. machine learning\n* Linear programming vs. machine learning\n* Learning into a computer program\n2. Recognizing Digits using Machine Learning\n* Writing rules to recognize digits from dataset\n3. Machine Learning Algorithms and Techniques\n* Decision Trees for Classification Problems\n4. Performance Measures in Machine Learning\n* Accuracy, Precision, Recall, F-score, and Matthew's Correlation Coefficient\n5. Optimal Decision Tree and Greedy Algorithm\n6. Data Preprocessing and Feature Selection\n7. Entropy, Information Gain, and Attribute Selection\n8. Decision Tree Implementation and Details\n9. Limitations and Future Work in Machine Learning\n\nThe lecture also includes discussions on the importance of data preprocessing, feature selection, and understanding performance measures for evaluating machine learning models effectively. It is important to note that this list might not be exhaustive, but it covers the main topics mentioned in the given lecture.\n====================================================================================================\n\nGive us some question for a quiz based on the following text:\n 1. What is machine learning and when was it first introduced?\n2. What is the difference between explicit programming and machine learning?\n3. In the context of machine learning, what is a training set and a test set?\n4. What are some rules for recognizing the digit \"4\" in an image dataset?\n5. What is precision and recall in machine learning?\n6. What is the difference between precision and Matthew's correlation coefficient?\n7. In the given example, what is the precision, recall, F score, and Matthew's correlation coefficient for predicting cancerous or not based on a dataset with 91 entries, of which 90 are not cancerous and 1 is cancerous?\n8. What is the main difference between decision trees and other machine learning algorithms?\n9. How does a decision tree algorithm work to classify data based on attributes?\n10. What is entropy in information theory and how is it related to decision trees?\n====================================================================================================\n\nSummarize the following text in Hindi in 10 lines or less:\n    10 :\n\n1.     Google Cloud  IGN UP      ,               ,          \n2. , 11-01-2023  11:00               ,      \n3. FAQ    Google Docs    ,          ,         \n4.     ,  Google Docs      \n5.             ,  \n6. Arthur Sandler  1959      \" \" (Machine Learning)     \n7.             \n8.    ,      \n9.           ?            \n10.    (0-9)  ,           recognize \n===================================================================================================="
  },
  {
    "objectID": "notebooks/ensemble-feature-importance.html",
    "href": "notebooks/ensemble-feature-importance.html",
    "title": "Random Forest Feature Importance",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import Image\n\n# To plot trees in forest via graphviz\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ntry:\n    from latexify import latexify, format_axes\n    latexify(columns=2)\nexcept:\n    pass\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load IRIS dataset from Seaborn\niris = sns.load_dataset('iris')\niris\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows  5 columns\n\n\n\n\n\n# classes\niris.species.unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nplt.hist(iris.sepal_width, bins=20)\n\n(array([ 1.,  3.,  4.,  3.,  8., 14., 14., 10., 26., 11., 19., 12.,  6.,\n         4.,  9.,  2.,  1.,  1.,  1.,  1.]),\n array([2.  , 2.12, 2.24, 2.36, 2.48, 2.6 , 2.72, 2.84, 2.96, 3.08, 3.2 ,\n        3.32, 3.44, 3.56, 3.68, 3.8 , 3.92, 4.04, 4.16, 4.28, 4.4 ]),\n &lt;BarContainer object of 20 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=iris, x=\"sepal_length\")\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.\n\n\n\n\n\n\n\n\n\n\nsns.displot(iris.sepal_length.values, kind='kde')\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.\n\n\n\n\n\n\n\n\n\n\nsns.displot(data=iris, x=\"sepal_length\", kind='kde')\n\nValueError: Multi-dimensional indexing (e.g. `obj[:, None]`) is no longer supported. Convert to a numpy array before indexing instead.\n\n\n\n\n\n\n\n\n\n\niris.groupby(\"species\")[\"petal_length\"].mean()\n\nspecies\nsetosa        1.462\nversicolor    4.260\nvirginica     5.552\nName: petal_length, dtype: float64\n\n\n\n# Pairplot\nsns.pairplot(iris, hue=\"species\")\n\n\n\n\n\n\n\n\n\n# Divide dataset into X and y\nX, y = iris.iloc[:, :-1], iris.iloc[:, -1]\nrf = RandomForestClassifier(n_estimators=10,random_state=0, criterion='entropy', bootstrap=True)\nrf.fit(X, y)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)\n\n\n\n# Visualize each tree in the Random Forest\nfor i, tree in enumerate(rf.estimators_):\n    # Create DOT data for the i-th tree\n    dot_data = export_graphviz(tree, out_file=None, \n                               feature_names=iris.columns[:-1],  \n                               class_names=iris.species.unique(),\n                               filled=True, rounded=True,\n                               special_characters=True,\n                               impurity=True,\n                               node_ids=True)\n    \n    # Use Graphviz to render the DOT data into a graph\n    graph = graphviz.Source(dot_data)\n    \n    # Save or display the graph (change the format as needed)\n    graph.render(filename=f'../figures/ensemble/feature-imp-{i}', format='pdf', cleanup=True)\n    graph.render(filename=f'../figures/ensemble/feature-imp-{i}', format='png', cleanup=True)\n\n\n# Visualize the tree\nImage(filename='../figures/ensemble/feature-imp-0.png')\n\n\n\n\n\n\n\n\n\n\\(t\\) = node\n\\(N_t\\) = number of observations at node \\(t\\)\n\\(N_{t_L}\\) = number of observations in the left child node of node \\(t\\)\n\\(N_{t_R}\\) = number of observations in the right child node of node \\(t\\)\n\\(p(t)=N_t/N\\) = proportion of observations in node \\(t\\)\n\\(X_j\\) = feature \\(j\\)\n\\(j_t\\) = feature used at node \\(t\\) for splitting\n\\(i(t)\\) = impurity at node \\(t\\) (impurity = entropy in this case)\n\\(M\\) = number of trees in the forest\n\n\nFor a particular node:\n\nInformation gain at node \\(t\\) = Impurity reduction at node \\(t\\) = entropy(parent) - weighted entropy(children)\n\n\\(\\Delta i(t) = i(t) - \\frac{N_{t_L}}{N_t} i(t_L) - \\frac{N_{t_r}}{N_t} i(t_R)\\)\n\n\nFor a tree:\nImportance of feature \\(X_j\\) is given by:\n\\(\\text{Imp}(X_j) = \\sum_{t \\in \\varphi_{m}} 1(j_t = j) \\Big[ p(t) \\Delta i(t) \\Big]\\)\n\n\nFor a forest:\nImportance of feature \\(X_j\\) for an ensemble of \\(M\\) trees \\(\\varphi_{m}\\) is:\n\\[\\begin{equation*}\n  \\text{Imp}(X_j) = \\frac{1}{M} \\sum_{m=1}^M \\sum_{t \\in \\varphi_{m}} 1(j_t = j) \\Big[ p(t) \\Delta i(t) \\Big]\n\\end{equation*}\\]\n\n1-1/np.e\n\n0.6321205588285577\n\n\n\nN = 150\n(1-1/np.e)*N\n\n94.81808382428365\n\n\n\nrf.feature_importances_\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\ns = []\nfor tree in rf.estimators_:\n    s.append(tree.feature_importances_)\n\n\nnp.array(s).mean(axis=0)\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\ntree_0 = rf.estimators_[0]\ntree_0.feature_importances_\n\narray([0.00397339, 0.01375245, 0.35802357, 0.6242506 ])\n\n\n\n# take one tree \ntree = rf.estimators_[0].tree_\n\n\ntree.feature\n\narray([ 3, -2,  2,  3, -2,  1, -2, -2,  0, -2,  2, -2, -2], dtype=int64)\n\n\n\n\n\n# Creating a mapping of feature names to the feature indices\nmapping = {-2: 'Leaf', 0: 'sepal_length', 1: 'sepal_width', 2: 'petal_length', 3: 'petal_width'}\n\n# print the node number along with the corresponding feature name\nfor node in range(tree.node_count):\n    print(f'Node {node}: {mapping[tree.feature[node]]}')\n\nNode 0: petal_width\nNode 1: Leaf\nNode 2: petal_length\nNode 3: petal_width\nNode 4: Leaf\nNode 5: sepal_width\nNode 6: Leaf\nNode 7: Leaf\nNode 8: sepal_length\nNode 9: Leaf\nNode 10: petal_length\nNode 11: Leaf\nNode 12: Leaf\n\n\n\nid = 2\ntree.children_left[id], tree.children_right[id]\n\n(3, 8)\n\n\n\ndef print_child_id(tree, node):\n    '''\n    Prints the child node ids of a given node.\n    tree: tree object\n    node: int\n    '''\n\n    # check if leaf\n    l, r = tree.children_left[node], tree.children_right[node]\n    if l == -1 and r == -1:\n        return None, None\n    return tree.children_left[node], tree.children_right[node]\n\nprint_child_id(tree, 0)\n\n(1, 2)\n\n\n\ntree.impurity\n\narray([1.57310798, 0.        , 0.98464683, 0.34781691, 0.        ,\n       0.81127812, 0.        , 0.        , 0.12741851, 0.        ,\n       0.2108423 , 0.        , 0.        ])\n\n\n\ndef all_data(tree, node):\n    '''\n    Returns all the data required to calculate the information gain.\n    '''\n\n    # get the child nodes\n    left, right = print_child_id(tree, node)\n\n    # check if leaf, then return None\n    if left is None:\n        return None\n    \n    # get the data\n    entropy_node = tree.impurity[node]\n    entropy_left = tree.impurity[left]\n    entropy_right = tree.impurity[right]\n\n    # N = total number of samples considered during bagging, therefore, it is equal to the number of samples at the root node\n    N = tree.n_node_samples[0]\n\n    # n_l = number of samples at the left child node\n    n_l = tree.n_node_samples[left]\n\n    # n_r = number of samples at the right child node\n    n_r = tree.n_node_samples[right]\n\n    # n_t = total number of samples at the node\n    n_t = n_l + n_r\n\n    feature = mapping[tree.feature[node]]\n    \n    # calculate the information gain\n    info_gain_t = entropy_node - (n_l/n_t * entropy_left + n_r/n_t * entropy_right)\n    \n    return info_gain_t, N, n_l, n_r, n_t, feature\n\n\n# Calculate the importance of each features using the information gain for a tree\n\nscores = {}\nfor node in range(tree.node_count):\n    # Add the information gain of the node to the dictionary if it is not a leaf node\n    try:\n        ig, N, n_l, n_r, n_t, feature = all_data(tree, node)\n        p_t = n_t / N\n        scores[feature] = scores.get(feature, 0) + p_t * ig\n\n    # Skip if it is a leaf node\n    except:\n        continue\n\nser = pd.Series(scores) \ninfo_gain_tree = ser/ser.sum()\ninfo_gain_tree.sort_values(ascending=False)\n\npetal_width     0.639307\npetal_length    0.340335\nsepal_width     0.016459\nsepal_length    0.003899\ndtype: float64\n\n\n\n# Feature importance using sklearn for a tree\nsklearn_imp = tree.compute_feature_importances()\npd.Series(sklearn_imp, index=iris.columns[:-1]).sort_values(ascending=False)\n\npetal_width     0.624251\npetal_length    0.358024\nsepal_width     0.013752\nsepal_length    0.003973\ndtype: float64\n\n\n\n# Feature importance using sklearn for the forest\nsklearn_imp_forest = np.array([x.tree_.compute_feature_importances() for x in rf.estimators_]).mean(axis=0)\npd.Series(sklearn_imp_forest, index=iris.columns[:-1]).sort_values(ascending=False)\n\npetal_width     0.544270\npetal_length    0.323122\nsepal_length    0.098647\nsepal_width     0.033960\ndtype: float64\n\n\n\nser = pd.Series(sklearn_imp_forest, index=iris.columns[:-1])\nser.plot(kind='bar', rot=0)\nformat_axes(plt.gca())\nplt.savefig('../figures/ensemble/feature-imp-forest.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\nAside:\ntree.tree_.feature returns the feature used at each node to divide the node into two child nodes with the below given mapping. The sequence of the features is the same as the column sequence of the input data.\n\n-2: leaf node\n0: sepal_length\n1: sepal_width\n2: petal_length\n3: petal_width\n\ntree.tree_.children_left[node] returns the node number of the left child of the node\ntree.tree_.children_right[node] returns the node number of the right child of the node\nif there is no left or right child, it returns -1\n\n\nBootstrap code:\nin the random_forest.fit() function\n\n\n\nbootstrap_code"
  },
  {
    "objectID": "notebooks/logistic-circular.html",
    "href": "notebooks/logistic-circular.html",
    "title": "Logistic Regression - Basis",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\n\n# Choose some points between\n\n\nnp.random.seed(0)\nx1 = np.random.randn(1, 100)\nx2 = np.random.randn(1, 100)\n\n\ny = x1**2 + x2**2\n\n\nx1\n\narray([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n        -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n         0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n         0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n        -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n        -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n         0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n         0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n        -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n        -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n        -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n         0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n        -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n        -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n         0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n        -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n        -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n         1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n        -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n         0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936]])\n\n\n\ny[y&gt;1] = 1\ny[y&lt;1] = 0\n\nc = 0\nfor i in range(100):\n    if y[0, i] == 1:\n        y[0, i] = 0\n        c += 1\n    if c == 10:\n        break\n\n\nlatexify()\nplt.scatter(x1, x2, c=y,s=5)\n\n\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-data.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nnp.vstack((x1, x2)).shape\n\n(2, 100)\n\n\n\nclf_1 = LogisticRegression(penalty='none',solver='newton-cg')\nclf_1.fit(np.vstack((x1, x2)).T, y.T)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nX = np.vstack((x1, x2)).T\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf_1.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"../figures/logistic-regression/logisitic-linear-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nnew_x = np.zeros((4, 100))\n\n\nnew_x[0] = x1\nnew_x[1] = x2\nnew_x[2] = x1**2\nnew_x[3] = x2**2\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(new_x.T, y.T)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nclf.coef_\n\narray([[-0.50464855, -0.30337009,  1.08937351,  0.73697949]])\n\n\n\nnew_x.T[:, 0]\n\narray([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n        0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n        0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n       -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n       -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n        0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n        0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n       -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n       -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n       -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n        0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n       -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n       -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n        0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n       -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n       -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n        1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n       -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n        0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936])\n\n\n\nX = np.vstack((x1, x2)).T\nX.shape\n\n(100, 2)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nZ.shape\n\n(261, 272)\n\n\n\nnp.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())]\n\narray([[-2.85298982, -2.52340315,  8.13955089,  6.36756347],\n       [-2.83298982, -2.52340315,  8.0258313 ,  6.36756347],\n       [-2.81298982, -2.52340315,  7.9129117 ,  6.36756347],\n       ...,\n       [ 2.52701018,  2.67659685,  6.38578047,  7.16417069],\n       [ 2.54701018,  2.67659685,  6.48726088,  7.16417069],\n       [ 2.56701018,  2.67659685,  6.58954129,  7.16417069]])\n\n\n\nxx.ravel()\n\narray([-2.85298982, -2.83298982, -2.81298982, ...,  2.52701018,\n        2.54701018,  2.56701018])\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - h, X[:, 0].max() + h\ny_min, y_max = X[:, 1].min() - h, X[:, 1].max() + h\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n# Put the result into a color plot\nZ = Z[:, 0].reshape(xx.shape)\nlatexify()\nplt.contourf(xx, yy, Z,levels=np.linspace(0, 1.1, num=10),cmap='Blues')\nplt.gca().set_aspect('equal')\n#plt.scatter(x1, x2, c=y)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.colorbar(label='P(Tomatoes)')\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-probability.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nxx.shape\n\n(233, 244)\n\n\n\nZ.size\n\n56852\n\n\n\nnp.linspace(0, 1.1, num=50)\n\narray([0.        , 0.02244898, 0.04489796, 0.06734694, 0.08979592,\n       0.1122449 , 0.13469388, 0.15714286, 0.17959184, 0.20204082,\n       0.2244898 , 0.24693878, 0.26938776, 0.29183673, 0.31428571,\n       0.33673469, 0.35918367, 0.38163265, 0.40408163, 0.42653061,\n       0.44897959, 0.47142857, 0.49387755, 0.51632653, 0.53877551,\n       0.56122449, 0.58367347, 0.60612245, 0.62857143, 0.65102041,\n       0.67346939, 0.69591837, 0.71836735, 0.74081633, 0.76326531,\n       0.78571429, 0.80816327, 0.83061224, 0.85306122, 0.8755102 ,\n       0.89795918, 0.92040816, 0.94285714, 0.96530612, 0.9877551 ,\n       1.01020408, 1.03265306, 1.05510204, 1.07755102, 1.1       ])"
  },
  {
    "objectID": "notebooks/numpy-pandas-basics.html",
    "href": "notebooks/numpy-pandas-basics.html",
    "title": "Numpy Pandas Basics",
    "section": "",
    "text": "# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nLists\n\n# Creating lists\nlist_a = [1, 2, 3, 4, 5]\nlist_b = [6, 7, 8, 9, 10]\n\n\n# Operations on lists\n# Adding lists\nlist_sum = [a + b for a, b in zip(list_a, list_b)]\nprint(\"List Sum:\", list_sum)\n\n# Vector product using lists    \nvector_product = [a * b for a, b in zip(list_a, list_b)]\nprint(\"Vector Product:\", vector_product)\n\nList Sum: [7, 9, 11, 13, 15]\nVector Product: [6, 14, 24, 36, 50]\n\n\n\n\nNumpy Array\n\n# Creating numpy arrays\nnumpy_array_a = np.array(list_a)\nnumpy_array_b = np.array(list_b)\n\n\n# Operations on numpy arrays\n# Adding numpy arrays\nnumpy_sum = numpy_array_a + numpy_array_b\nprint(\"Numpy Sum:\", numpy_sum)\n\n# Vector product using numpy arrays\nnumpy_vector_product = np.multiply(numpy_array_a, numpy_array_b)\nprint(\"Numpy Vector Product:\", numpy_vector_product)\n\nNumpy Sum: [ 7  9 11 13 15]\nNumpy Vector Product: [ 6 14 24 36 50]\n\n\n\nnp.allclose(list_sum, numpy_sum), np.allclose(vector_product, numpy_vector_product)\n\n(True, True)\n\n\n\n\nTime comparison between list and numpy array\n\n# Creating large arrays and lists for time comparison\nnumpy_array_a = np.random.randint(0, 100, size=10000)\nnumpy_array_b = np.random.randint(0, 100, size=10000)\n\nlist_a = list(numpy_array_a)\nlist_b = list(numpy_array_b)\n\n\n# Time for list addition\nstart_time = time.time()\nfor _ in range(1000):\n    list_sum = [a + b for a, b in zip(list_a, list_b)]\nend_time = time.time()\nprint(\"Time taken for lists addition:\", end_time - start_time)\n\n# Time for numpy addition\nstart_time = time.time()\nfor _ in range(1000):\n    numpy_sum = numpy_array_a + numpy_array_b\nend_time = time.time()\nprint(\"Time taken for numpy addition:\", end_time - start_time)\n\nTime taken for lists addition: 0.5500102043151855\nTime taken for numpy addition: 0.0038487911224365234\n\n\n\n# Time for list vector product\nstart_time = time.time()\nfor _ in range(10000):\n    list_product = [a * b for a, b in zip(list_a, list_b)]\n\nend_time = time.time()\nprint(\"Time taken for list vector product:\", end_time - start_time)\n\n# Time for numpy vector product \nstart_time = time.time()\nfor _ in range(10000):\n    numpy_product = np.multiply(numpy_array_a, numpy_array_b)\n\nend_time = time.time()\nprint(\"Time taken for numpy vector product:\", end_time - start_time)\n\nTime taken for list vector product: 5.371699571609497\nTime taken for numpy vector product: 0.047417640686035156\n\n\n\nnp.allclose(list_sum, numpy_sum), np.allclose(vector_product, numpy_vector_product)\n\n(True, True)\n\n\n\ntimeit_add_list = %timeit -o [a + b for a, b in zip(list_a, list_b)]\n\n542 s  593 ns per loop (mean  std. dev. of 7 runs, 1,000 loops each)\n\n\n\ntimeit_add_numpy = %timeit -o numpy_array_a + numpy_array_b\n\n3.5 s  6.1 ns per loop (mean  std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode clarity\n\n# Numpy code is often more concise and readable than list comprehensions\n# Example: Calculate the element-wise product of two lists\nlist_product = [a * b for a, b in zip(list_a, list_b)]\nnumpy_product = np.multiply(numpy_array_a, numpy_array_b)\n\n\nnumpy_product\n\narray([5950, 1995,  264, ..., 2436,  928,  665])\n\n\n\nnumpy_array_a@numpy_array_b\n\n24470992\n\n\n\n\nReading CSV file using Numpy\n\n!head ../datasets/tennis-discrete-output.csv\n\nDay,Outlook,Temp,Humidity,Windy,Play\nD1,Sunny,Hot,High,Weak,No\nD2,Sunny,Hot,High,Strong,No\nD3,Overcast,Hot,High,Weak,Yes\nD4,Rain,Mild,High,Weak,Yes\nD5,Rain,Cool,Normal,Weak,Yes\nD6,Rain,Cool,Normal,Strong,No\nD7,Overcast,Cool,Normal,Strong,Yes\nD8,Sunny,Mild,High,Weak,No\nD9,Sunny,Cool,Normal,Weak,Yes\n\n\n\nnp.genfromtxt?\n\nSignature:\nnp.genfromtxt(\n    fname,\n    dtype=&lt;class 'float'&gt;,\n    comments='#',\n    delimiter=None,\n    skip_header=0,\n    skip_footer=0,\n    converters=None,\n    missing_values=None,\n    filling_values=None,\n    usecols=None,\n    names=None,\n    excludelist=None,\n    deletechars=\" !#$%&'()*+,-./:;&lt;=&gt;?@[\\\\]^{|}~\",\n    replace_space='_',\n    autostrip=False,\n    case_sensitive=True,\n    defaultfmt='f%i',\n    unpack=None,\n    usemask=False,\n    loose=True,\n    invalid_raise=True,\n    max_rows=None,\n    encoding='bytes',\n    *,\n    ndmin=0,\n    like=None,\n)\nDocstring:\nLoad data from a text file, with missing values handled as specified.\n\nEach line past the first `skip_header` lines is split at the `delimiter`\ncharacter, and characters following the `comments` character are discarded.\n\nParameters\n----------\nfname : file, str, pathlib.Path, list of str, generator\n    File, filename, list, or generator to read.  If the filename\n    extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n    that generators must return bytes or strings. The strings\n    in a list or produced by a generator are treated as lines.\ndtype : dtype, optional\n    Data type of the resulting array.\n    If None, the dtypes will be determined by the contents of each\n    column, individually.\ncomments : str, optional\n    The character used to indicate the start of a comment.\n    All the characters occurring on a line after a comment are discarded.\ndelimiter : str, int, or sequence, optional\n    The string used to separate values.  By default, any consecutive\n    whitespaces act as delimiter.  An integer or sequence of integers\n    can also be provided as width(s) of each field.\nskiprows : int, optional\n    `skiprows` was removed in numpy 1.10. Please use `skip_header` instead.\nskip_header : int, optional\n    The number of lines to skip at the beginning of the file.\nskip_footer : int, optional\n    The number of lines to skip at the end of the file.\nconverters : variable, optional\n    The set of functions that convert the data of a column to a value.\n    The converters can also be used to provide a default value\n    for missing data: ``converters = {3: lambda s: float(s or 0)}``.\nmissing : variable, optional\n    `missing` was removed in numpy 1.10. Please use `missing_values`\n    instead.\nmissing_values : variable, optional\n    The set of strings corresponding to missing data.\nfilling_values : variable, optional\n    The set of values to be used as default when the data are missing.\nusecols : sequence, optional\n    Which columns to read, with 0 being the first.  For example,\n    ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.\nnames : {None, True, str, sequence}, optional\n    If `names` is True, the field names are read from the first line after\n    the first `skip_header` lines. This line can optionally be preceded\n    by a comment delimiter. If `names` is a sequence or a single-string of\n    comma-separated names, the names will be used to define the field names\n    in a structured dtype. If `names` is None, the names of the dtype\n    fields will be used, if any.\nexcludelist : sequence, optional\n    A list of names to exclude. This list is appended to the default list\n    ['return','file','print']. Excluded names are appended with an\n    underscore: for example, `file` would become `file_`.\ndeletechars : str, optional\n    A string combining invalid characters that must be deleted from the\n    names.\ndefaultfmt : str, optional\n    A format used to define default field names, such as \"f%i\" or \"f_%02i\".\nautostrip : bool, optional\n    Whether to automatically strip white spaces from the variables.\nreplace_space : char, optional\n    Character(s) used in replacement of white spaces in the variable\n    names. By default, use a '_'.\ncase_sensitive : {True, False, 'upper', 'lower'}, optional\n    If True, field names are case sensitive.\n    If False or 'upper', field names are converted to upper case.\n    If 'lower', field names are converted to lower case.\nunpack : bool, optional\n    If True, the returned array is transposed, so that arguments may be\n    unpacked using ``x, y, z = genfromtxt(...)``.  When used with a\n    structured data-type, arrays are returned for each field.\n    Default is False.\nusemask : bool, optional\n    If True, return a masked array.\n    If False, return a regular array.\nloose : bool, optional\n    If True, do not raise errors for invalid values.\ninvalid_raise : bool, optional\n    If True, an exception is raised if an inconsistency is detected in the\n    number of columns.\n    If False, a warning is emitted and the offending lines are skipped.\nmax_rows : int,  optional\n    The maximum number of rows to read. Must not be used with skip_footer\n    at the same time.  If given, the value must be at least 1. Default is\n    to read the entire file.\n\n    .. versionadded:: 1.10.0\nencoding : str, optional\n    Encoding used to decode the inputfile. Does not apply when `fname` is\n    a file object.  The special value 'bytes' enables backward compatibility\n    workarounds that ensure that you receive byte arrays when possible\n    and passes latin1 encoded strings to converters. Override this value to\n    receive unicode arrays and pass strings as input to converters.  If set\n    to None the system default is used. The default value is 'bytes'.\n\n    .. versionadded:: 1.14.0\nndmin : int, optional\n    Same parameter as `loadtxt`\n\n    .. versionadded:: 1.23.0\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Data read from the text file. If `usemask` is True, this is a\n    masked array.\n\nSee Also\n--------\nnumpy.loadtxt : equivalent function when no data is missing.\n\nNotes\n-----\n* When spaces are used as delimiters, or when no delimiter has been given\n  as input, there should not be any missing data between two fields.\n* When the variables are named (either by a flexible dtype or with `names`),\n  there must not be any header in the file (else a ValueError\n  exception is raised).\n* Individual values are not stripped of spaces by default.\n  When using a custom converter, make sure the function does remove spaces.\n\nReferences\n----------\n.. [1] NumPy User Guide, section `I/O with NumPy\n       &lt;https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html&gt;`_.\n\nExamples\n--------\n&gt;&gt;&gt; from io import StringIO\n&gt;&gt;&gt; import numpy as np\n\nComma delimited file with mixed dtype\n\n&gt;&gt;&gt; s = StringIO(u\"1,1.3,abcde\")\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n... ('mystring','S5')], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nUsing dtype = None\n\n&gt;&gt;&gt; _ = s.seek(0) # needed for StringIO example only\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=None,\n... names = ['myint','myfloat','mystring'], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nSpecifying dtype and names\n\n&gt;&gt;&gt; _ = s.seek(0)\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n... names=['myint','myfloat','mystring'], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nAn example with fixed-width columns\n\n&gt;&gt;&gt; s = StringIO(u\"11.3abcde\")\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n...     delimiter=[1,3,5])\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('intvar', '&lt;i8'), ('fltvar', '&lt;f8'), ('strvar', 'S5')])\n\nAn example to show comments\n\n&gt;&gt;&gt; f = StringIO('''\n... text,# of chars\n... hello world,11\n... numpy,5''')\n&gt;&gt;&gt; np.genfromtxt(f, dtype='S12,S12', delimiter=',')\narray([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n  dtype=[('f0', 'S12'), ('f1', 'S12')])\nFile:      ~/miniforge3/lib/python3.9/site-packages/numpy/lib/npyio.py\nType:      function\n\n\n\ndata = np.genfromtxt('../datasets/tennis-discrete-output.csv', delimiter=',')\ndata\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan]])\n\n\nWait! What happened?\n\ndata = np.genfromtxt('../datasets/tennis-discrete-output.csv', delimiter=',', dtype=str)\ndata\n\narray([['Day', 'Outlook', 'Temp', 'Humidity', 'Windy', 'Play'],\n       ['D1', 'Sunny', 'Hot', 'High', 'Weak', 'No'],\n       ['D2', 'Sunny', 'Hot', 'High', 'Strong', 'No'],\n       ['D3', 'Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n       ['D4', 'Rain', 'Mild', 'High', 'Weak', 'Yes'],\n       ['D5', 'Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n       ['D6', 'Rain', 'Cool', 'Normal', 'Strong', 'No'],\n       ['D7', 'Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n       ['D8', 'Sunny', 'Mild', 'High', 'Weak', 'No'],\n       ['D9', 'Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n       ['D10', 'Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n       ['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n       ['D12', 'Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n       ['D13', 'Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n       ['D14', 'Rain', 'Mild', 'High', 'Strong', 'No']], dtype='&lt;U8')\n\n\n\ndata.shape\n\n(15, 6)\n\n\nQuestion: Find the outlook on D11\n\nidx = np.argwhere(data[:, 0] == 'D11')[0, 0]\nidx\n\n11\n\n\n\ndata[idx]\n\narray(['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'], dtype='&lt;U8')\n\n\n\ndata[idx][1]\n\n'Sunny'\n\n\n\n\nReading CSV file using Pandas\n\ndf = pd.read_csv('../datasets/tennis-discrete-output.csv')\n\n\ndf\n\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\n\n\n0\nD1\nSunny\nHot\nHigh\nWeak\nNo\n\n\n1\nD2\nSunny\nHot\nHigh\nStrong\nNo\n\n\n2\nD3\nOvercast\nHot\nHigh\nWeak\nYes\n\n\n3\nD4\nRain\nMild\nHigh\nWeak\nYes\n\n\n4\nD5\nRain\nCool\nNormal\nWeak\nYes\n\n\n5\nD6\nRain\nCool\nNormal\nStrong\nNo\n\n\n6\nD7\nOvercast\nCool\nNormal\nStrong\nYes\n\n\n7\nD8\nSunny\nMild\nHigh\nWeak\nNo\n\n\n8\nD9\nSunny\nCool\nNormal\nWeak\nYes\n\n\n9\nD10\nRain\nMild\nNormal\nWeak\nYes\n\n\n10\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\n11\nD12\nOvercast\nMild\nHigh\nStrong\nYes\n\n\n12\nD13\nOvercast\nHot\nNormal\nWeak\nYes\n\n\n13\nD14\nRain\nMild\nHigh\nStrong\nNo\n\n\n\n\n\n\n\n\n\ndf['Day'] == 'D11'\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10     True\n11    False\n12    False\n13    False\nName: Day, dtype: bool\n\n\n\ndf[df['Day'] == 'D11']\n\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\n\n\n10\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\n\n\n\n\n\n\n\ndf[df['Day'] == 'D11']['Outlook']\n\n10    Sunny\nName: Outlook, dtype: object\n\n\n\ndf.query('Day == \"D11\"')['Outlook']\n\n10    Sunny\nName: Outlook, dtype: object\n\n\n\ndf.shape\n\n(14, 6)\n\n\nQuestion. How many times do we play v/s not play tennis\n\nser = df['Play']\nser\n\n0      No\n1      No\n2     Yes\n3     Yes\n4     Yes\n5      No\n6     Yes\n7      No\n8     Yes\n9     Yes\n10    Yes\n11    Yes\n12    Yes\n13     No\nName: Play, dtype: object\n\n\n\nunique_play_options = df['Play'].unique()\nunique_play_options\n\narray(['No', 'Yes'], dtype=object)\n\n\n\nfor option in unique_play_options:\n    print(option, (df['Play'] == option).sum())\n\nNo 5\nYes 9\n\n\n\ndf['Play'].value_counts()\n\nPlay\nYes    9\nNo     5\nName: count, dtype: int64\n\n\n\ndf.groupby('Play').size()\n\nPlay\nNo     5\nYes    9\ndtype: int64\n\n\n\ngby = df.groupby('Play')\n\n\n{k: len(v) for k, v in gby.groups.items()}\n\n{'No': 5, 'Yes': 9}\n\n\n\npd.crosstab(index=df['Play'], columns='count')\n\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nPlay\n\n\n\n\n\nNo\n5\n\n\nYes\n9\n\n\n\n\n\n\n\n\nWhat is the distribution of any given attribute?\n\ndef distribution(df, attribute):\n    return df[attribute].value_counts()\n\n\nser = distribution(df, 'Outlook')\n\n\nser\n\nOutlook\nSunny       5\nRain        5\nOvercast    4\nName: count, dtype: int64\n\n\n\ntype(ser)\n\npandas.core.series.Series\n\n\n\nser.values\n\narray([5, 5, 4])\n\n\n\nser.index\n\nIndex(['Sunny', 'Rain', 'Overcast'], dtype='object', name='Outlook')\n\n\n\ndistribution(df, 'Temp')\n\nTemp\nMild    6\nHot     4\nCool    4\nName: count, dtype: int64\n\n\nFinding entropy for target variable\n\ntarget_attribute = 'Play'\ndist_target = distribution(df, target_attribute)\n\n\ndist_target\n\nPlay\nYes    9\nNo     5\nName: count, dtype: int64\n\n\nNormalize distribution\n\ndist_target/dist_target.sum()\n\nPlay\nYes    0.642857\nNo     0.357143\nName: count, dtype: float64\n\n\n\ndf['Play'].value_counts(normalize=True)\n\nPlay\nYes    0.642857\nNo     0.357143\nName: proportion, dtype: float64\n\n\n\nnormalized_dist_target = dist_target/dist_target.sum()\n\nFor loop way of calculating entropy\n\ne = 0.0\nfor value, p in normalized_dist_target.items():\n    e = e - p * np.log2(p + 1e-6) # 1e-6 is added to avoid log(0)\nprint(e)\n\n0.9402830732836911\n\n\n\nnormalized_dist_target.apply(lambda x: -x * np.log2(x + 1e-6))\n\nPlay\nYes    0.409775\nNo     0.530508\nName: count, dtype: float64\n\n\n\nnormalized_dist_target.apply(lambda x: -x * np.log2(x + 1e-6)).sum()\n\n0.9402830732836911\n\n\nMore on crosstab\n\npd.crosstab(index=df['Outlook'], columns=df['Play'])\n\n\n\n\n\n\n\n\nPlay\nNo\nYes\n\n\nOutlook\n\n\n\n\n\n\nOvercast\n0\n4\n\n\nRain\n2\n3\n\n\nSunny\n3\n2\n\n\n\n\n\n\n\n\n\npd.crosstab(index=df['Outlook'], columns=df['Play']).T\n\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0\n2\n3\n\n\nYes\n4\n3\n2\n\n\n\n\n\n\n\n\n\ndf_attr = pd.crosstab(index=df['Play'], columns=df['Outlook'], normalize='columns')\ndf_attr\n\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.0\n0.4\n0.6\n\n\nYes\n1.0\n0.6\n0.4\n\n\n\n\n\n\n\n\nUsing groupby\n\ndf.groupby(['Play', 'Outlook']).size()\n\nPlay  Outlook \nNo    Rain        2\n      Sunny       3\nYes   Overcast    4\n      Rain        3\n      Sunny       2\ndtype: int64\n\n\n\ndf.groupby(['Play', 'Outlook']).size().index\n\nMultiIndex([( 'No',     'Rain'),\n            ( 'No',    'Sunny'),\n            ('Yes', 'Overcast'),\n            ('Yes',     'Rain'),\n            ('Yes',    'Sunny')],\n           names=['Play', 'Outlook'])\n\n\n\ndf.groupby(['Play', 'Outlook']).size().unstack('Outlook')\n\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\nNaN\n2.0\n3.0\n\n\nYes\n4.0\n3.0\n2.0\n\n\n\n\n\n\n\n\n\ndf_attr_groupby = df.groupby(['Play', 'Outlook']).size().unstack('Outlook').fillna(0)\ndf_attr_groupby\n\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.0\n2.0\n3.0\n\n\nYes\n4.0\n3.0\n2.0\n\n\n\n\n\n\n\n\nApply\n\nneg_plogp = df_attr.apply(lambda x: -x * np.log2(x + 1e-6), axis=0)\nneg_plogp\n\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.000000\n0.528770\n0.442178\n\n\nYes\n-0.000001\n0.442178\n0.528770\n\n\n\n\n\n\n\n\n\nneg_plogp.sum(axis=0).sort_index()\n\nOutlook\nOvercast   -0.000001\nRain        0.970948\nSunny       0.970948\ndtype: float64\n\n\n\ndf_attr_dist = distribution(df, 'Outlook')\nnorm_attr_dist = df_attr_dist/df_attr_dist.sum()\nnorm_attr_dist\n\nOutlook\nSunny       0.357143\nRain        0.357143\nOvercast    0.285714\nName: count, dtype: float64\n\n\n\n(norm_attr_dist*neg_plogp.sum(axis=0).sort_index()).sum()\n\n0.6935336657070463"
  },
  {
    "objectID": "notebooks/dummy-variables-multi-colinearity.html",
    "href": "notebooks/dummy-variables-multi-colinearity.html",
    "title": "Dummy Variables and Multi-collinearity",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\n\n\nx1 = np.array([1, 2, 3])\nx2 = 2*x1\n\ny = np.array([4, 6, 8])\n\n\nall_ones = np.ones(x1.shape[0])\nX = np.array([all_ones, x1, x2]).T\n\n\nX.shape\n\n(3, 3)\n\n\n\nX\n\narray([[1., 1., 2.],\n       [1., 2., 4.],\n       [1., 3., 6.]])\n\n\n\ndef solve_normal_equation(X, y):\n    try:\n        theta = np.linalg.inv(X.T @ X) @ X.T @ y\n        return theta\n    except np.linalg.LinAlgError:\n        print('The matrix is singular')\n        print(\"X.T @ X = \\n\", X.T @ X)\n        return None\n    \n### Assignment question: Use np.linalg.solve instead of inv. Why is this better?\n\n\nsolve_normal_equation(X, y)\n\nThe matrix is singular\nX.T @ X = \n [[ 3.  6. 12.]\n [ 6. 14. 28.]\n [12. 28. 56.]]\n\n\n\nnp.linalg.matrix_rank(X), np.linalg.matrix_rank(X.T @ X)\n\n(2, 2)\n\n\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\ndata = np.array([x1, x2]).T\n\nlr.fit(data, y)\nlr.coef_, lr.intercept_\n\n\n# Assignment question: figure why sklearn is able to solve the problem\n\n(array([0.4, 0.8]), 2.0)\n\n\n\n# Regularization\n\neps = 1e-5\nX = np.array([all_ones, x1, x2]).T\nX = np.eye(3)*eps + X\nX\n\narray([[1.00001, 1.     , 2.     ],\n       [1.     , 2.00001, 4.     ],\n       [1.     , 3.     , 6.00001]])\n\n\n\nnp.linalg.matrix_rank(X)\n\n3\n\n\n\nsolve_normal_equation(X, y)\n\narray([2.00023248, 1.19987743, 0.40001887])\n\n\n\n# Drop variables\nX = np.array([all_ones, x1]).T\nprint(X)\n\n[[1. 1.]\n [1. 2.]\n [1. 3.]]\n\n\n\nsolve_normal_equation(X, y)\n\narray([2., 2.])\n\n\n\n# Dummy variables\n\n## dataset\nnum_records = 12\nwindspeed = np.random.randint(0, 10, num_records)\nvehicles = np.random.randint(100, 500, num_records)\ndirection = np.random.choice(['N', 'S', 'E', 'W'], num_records)\npollution = np.random.randint(0, 100, num_records)\n\ndf = pd.DataFrame({'windspeed': windspeed, 'vehicles': vehicles, 'direction': direction, 'pollution': pollution})\ndf\n\n\n\n\n\n\n\n\n\nwindspeed\nvehicles\ndirection\npollution\n\n\n\n\n0\n0\n355\nW\n30\n\n\n1\n2\n367\nS\n30\n\n\n2\n2\n447\nS\n78\n\n\n3\n1\n223\nE\n32\n\n\n4\n1\n272\nS\n16\n\n\n5\n9\n394\nS\n36\n\n\n6\n0\n333\nN\n45\n\n\n7\n3\n308\nW\n52\n\n\n8\n7\n480\nN\n24\n\n\n9\n9\n360\nN\n74\n\n\n10\n0\n125\nS\n36\n\n\n11\n9\n401\nS\n62\n\n\n\n\n\n\n\n\n\ndef fit_data(df, X, y):\n    try:\n        lr = LinearRegression()\n        lr.fit(X, y)\n        rep = f\"y = {lr.intercept_:0.2f}\"\n        for i, coef in enumerate(lr.coef_):\n            rep += f\" + {coef:0.2f}*{df.columns[i]}\"\n        return rep\n    except Exception as e:\n        print(e)\n        return None\n        \n\n\nfit_data(df, df[df.columns[:-1]], df['pollution'])\n\ncould not convert string to float: 'W'\n\n\n\n# Ordinal encoding\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nenc = OrdinalEncoder()\n\n\ndf2 = df.copy()\ndf2['direction'] = enc.fit_transform(df[['direction']]).flatten()\ndf2\n\n\n\n\n\n\n\n\n\nwindspeed\nvehicles\ndirection\npollution\n\n\n\n\n0\n0\n355\n3.0\n30\n\n\n1\n2\n367\n2.0\n30\n\n\n2\n2\n447\n2.0\n78\n\n\n3\n1\n223\n0.0\n32\n\n\n4\n1\n272\n2.0\n16\n\n\n5\n9\n394\n2.0\n36\n\n\n6\n0\n333\n1.0\n45\n\n\n7\n3\n308\n3.0\n52\n\n\n8\n7\n480\n1.0\n24\n\n\n9\n9\n360\n1.0\n74\n\n\n10\n0\n125\n2.0\n36\n\n\n11\n9\n401\n2.0\n62\n\n\n\n\n\n\n\n\n\nfit_data(df2, df2[df2.columns[:-1]], df2['pollution'])\n\n'y = 26.49 + 1.49*windspeed + 0.03*vehicles + 1.02*direction'\n\n\n\npd.Series({x: i for i, x in enumerate(enc.categories_[0])})\n\nE    0\nN    1\nS    2\nW    3\ndtype: int64\n\n\n\n# One-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\n\n\ndirection_ohe = ohe.fit_transform(df[['direction']])\ndirection_ohe\n\narray([[0., 0., 0., 1.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.]])\n\n\n\ncol_names_ohe = [f\"Is it {x}?\" for x in enc.categories_[0]]\n\n\ndirection_ohe_df = pd.DataFrame(direction_ohe, columns=col_names_ohe)\ndirection_ohe_df\n\n\n\n\n\n\n\n\n\nIs it E?\nIs it N?\nIs it S?\nIs it W?\n\n\n\n\n0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n1.0\n0.0\n\n\n5\n0.0\n0.0\n1.0\n0.0\n\n\n6\n0.0\n1.0\n0.0\n0.0\n\n\n7\n0.0\n0.0\n0.0\n1.0\n\n\n8\n0.0\n1.0\n0.0\n0.0\n\n\n9\n0.0\n1.0\n0.0\n0.0\n\n\n10\n0.0\n0.0\n1.0\n0.0\n\n\n11\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\n# Confirm that we can write Is it W? as a linear combination of the other columns\n1-direction_ohe_df[[\"Is it N?\", \"Is it S?\", \"Is it E?\"]].sum(axis=1) - direction_ohe_df[\"Is it W?\"]\n\n0     0.0\n1     0.0\n2     0.0\n3     0.0\n4     0.0\n5     0.0\n6     0.0\n7     0.0\n8     0.0\n9     0.0\n10    0.0\n11    0.0\ndtype: float64\n\n\n\nX = np.hstack([df[['windspeed', 'vehicles']].values, direction_ohe])\n\n\nX\n\narray([[  0., 355.,   0.,   0.,   0.,   1.],\n       [  2., 367.,   0.,   0.,   1.,   0.],\n       [  2., 447.,   0.,   0.,   1.,   0.],\n       [  1., 223.,   1.,   0.,   0.,   0.],\n       [  1., 272.,   0.,   0.,   1.,   0.],\n       [  9., 394.,   0.,   0.,   1.,   0.],\n       [  0., 333.,   0.,   1.,   0.,   0.],\n       [  3., 308.,   0.,   0.,   0.,   1.],\n       [  7., 480.,   0.,   1.,   0.,   0.],\n       [  9., 360.,   0.,   1.,   0.,   0.],\n       [  0., 125.,   0.,   0.,   1.,   0.],\n       [  9., 401.,   0.,   0.,   1.,   0.]])\n\n\n\nX_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n\n\nX_aug\n\narray([[  1.,   0., 355.,   0.,   0.,   0.,   1.],\n       [  1.,   2., 367.,   0.,   0.,   1.,   0.],\n       [  1.,   2., 447.,   0.,   0.,   1.,   0.],\n       [  1.,   1., 223.,   1.,   0.,   0.,   0.],\n       [  1.,   1., 272.,   0.,   0.,   1.,   0.],\n       [  1.,   9., 394.,   0.,   0.,   1.,   0.],\n       [  1.,   0., 333.,   0.,   1.,   0.,   0.],\n       [  1.,   3., 308.,   0.,   0.,   0.,   1.],\n       [  1.,   7., 480.,   0.,   1.,   0.,   0.],\n       [  1.,   9., 360.,   0.,   1.,   0.,   0.],\n       [  1.,   0., 125.,   0.,   0.,   1.,   0.],\n       [  1.,   9., 401.,   0.,   0.,   1.,   0.]])\n\n\n\nX_aug.shape\n\n(12, 7)\n\n\n\nnp.linalg.matrix_rank(X_aug), np.linalg.matrix_rank(X_aug.T @ X_aug), (X_aug.T @ X_aug).shape\n\n(6, 6, (7, 7))\n\n\n\npd.DataFrame(X_aug.T @ X_aug)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n12.0\n43.0\n4065.0\n1.0\n3.0\n6.0\n2.0\n\n\n1\n43.0\n311.0\n16802.0\n1.0\n16.0\n23.0\n3.0\n\n\n2\n4065.0\n16802.0\n1481651.0\n223.0\n1173.0\n2006.0\n663.0\n\n\n3\n1.0\n1.0\n223.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n3.0\n16.0\n1173.0\n0.0\n3.0\n0.0\n0.0\n\n\n5\n6.0\n23.0\n2006.0\n0.0\n0.0\n6.0\n0.0\n\n\n6\n2.0\n3.0\n663.0\n0.0\n0.0\n0.0\n2.0\n\n\n\n\n\n\n\n\n\nohe = OneHotEncoder(sparse_output=False, drop='first')\nohe.fit_transform(df[['direction']])\n\narray([[0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\n\ndirection_ohe_n_1 = ohe.fit_transform(df[['direction']])\ncol_names_ohe_n_1 = [f\"Is it {x}?\" for x in enc.categories_[0][1:]]\ndf_ohe_n_1 = pd.DataFrame(direction_ohe_n_1, columns=col_names_ohe_n_1)\ndf_ohe_n_1\n\n\n\n\n\n\n\n\n\nIs it N?\nIs it S?\nIs it W?\n\n\n\n\n0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n1.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n5\n0.0\n1.0\n0.0\n\n\n6\n1.0\n0.0\n0.0\n\n\n7\n0.0\n0.0\n1.0\n\n\n8\n1.0\n0.0\n0.0\n\n\n9\n1.0\n0.0\n0.0\n\n\n10\n0.0\n1.0\n0.0\n\n\n11\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n\nX = np.hstack([df[['windspeed', 'vehicles']].values, df_ohe_n_1.values])\nX_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n\nX_aug\n\narray([[  1.,   0., 355.,   0.,   0.,   1.],\n       [  1.,   2., 367.,   0.,   1.,   0.],\n       [  1.,   2., 447.,   0.,   1.,   0.],\n       [  1.,   1., 223.,   0.,   0.,   0.],\n       [  1.,   1., 272.,   0.,   1.,   0.],\n       [  1.,   9., 394.,   0.,   1.,   0.],\n       [  1.,   0., 333.,   1.,   0.,   0.],\n       [  1.,   3., 308.,   0.,   0.,   1.],\n       [  1.,   7., 480.,   1.,   0.,   0.],\n       [  1.,   9., 360.,   1.,   0.,   0.],\n       [  1.,   0., 125.,   0.,   1.,   0.],\n       [  1.,   9., 401.,   0.,   1.,   0.]])\n\n\n\nnp.linalg.matrix_rank(X_aug), np.linalg.matrix_rank(X_aug.T @ X_aug), (X_aug.T @ X_aug).shape\n\n(6, 6, (6, 6))\n\n\n\n# Interepeting dummy variables\n\n## dataset\n\nX = np.array(['F', 'F', 'F', 'M', 'M'])\ny = np.array([5, 5.2, 5.4, 5.8, 6])\n\n\nfrom sklearn.preprocessing import LabelBinarizer\nl = LabelBinarizer()\nl.fit_transform(X)\n\narray([[0],\n       [0],\n       [0],\n       [1],\n       [1]])\n\n\n\nX_binary = 1 - l.fit_transform(X)\n\n\nX_binary    \n\narray([[1],\n       [1],\n       [1],\n       [0],\n       [0]])\n\n\n\nlr = LinearRegression()\nlr.fit(X_binary, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlr.coef_, lr.intercept_\n\n(array([-0.7]), 5.8999999999999995)\n\n\n\ny[(X_binary==0).flatten()].mean()\n\n5.9\n\n\n\ny[(X_binary==1).flatten()].mean()\n\n5.2"
  },
  {
    "objectID": "notebooks/confusion-mnist.html",
    "href": "notebooks/confusion-mnist.html",
    "title": "Notion of Confusion in ML",
    "section": "",
    "text": "import numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.utils.multiclass import unique_labels\nimport torchvision\nimport torchvision.transforms as transforms\nfrom latexify import latexify\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n# Define transformations\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\n# Download and load MNIST dataset using torchvision\ntrain_dataset = torchvision.datasets.MNIST(root='../datasets', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.MNIST(root='../datasets', train=False, download=True, transform=transform)\n\n\n# Flatten the images for sklearn MLP\nX_train = train_dataset.data.numpy().reshape((len(train_dataset), -1))\ny_train = train_dataset.targets.numpy()\nX_test = test_dataset.data.numpy().reshape((len(test_dataset), -1))\ny_test = test_dataset.targets.numpy()\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Plot few images\nlatexify()\nfig, axs = plt.subplots(1, 7, figsize=(8, 10))\nfor i in range(7):\n    axs[i].imshow(X_train[i].reshape((28, 28)), cmap='gray')\n    axs[i].set_title(y_train[i])\n    axs[i].axis('off')\n\n\n\n\n\n\n\n\n\n# Create and train the MLP model\nmlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\nmlp_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the test set\ny_probabilities = mlp_model.predict_proba(X_test_scaled)\n\n# Predict on the test set\ny_pred = mlp_model.predict(X_test_scaled)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9735\n\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\n\nlatexify(fig_width=6)\ncm = confusion_matrix(y_test, y_pred)\ncm_display = ConfusionMatrixDisplay(cm).plot(values_format='d', cmap='gray', ax=plt.gca())\n\n# Save the figure with a higher resolution and without lossy compression\nplt.savefig(\"../figures/mnist-cm.png\", bbox_inches=\"tight\", dpi=400, transparent=True)\n\n# Show the plot\n\n\n\n\n\n\n\n\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.98       980\n           1       0.99      0.99      0.99      1135\n           2       0.97      0.96      0.97      1032\n           3       0.96      0.98      0.97      1010\n           4       0.98      0.97      0.98       982\n           5       0.98      0.97      0.97       892\n           6       0.97      0.97      0.97       958\n           7       0.97      0.98      0.97      1028\n           8       0.96      0.96      0.96       974\n           9       0.98      0.96      0.97      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000\n\n\n\n\n# Display the first k wrong classified images with highest probabilities\n\n# Find indices of wrongly classified samples\nwrong_indices = np.where(y_pred != y_test)[0]\n\n# Sort wrong predictions by highest class probability\nsorted_indices = np.argsort(np.max(y_probabilities[wrong_indices], axis=1))[::-1]\n\nk = 9\nlatexify(fig_width=8)\nfor i, idx in enumerate(sorted_indices[:k]):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(test_dataset[wrong_indices[idx]][0].numpy().squeeze(), cmap='gray')\n    plt.title(f'True: {y_test[wrong_indices[idx]]}, Pred: {y_pred[wrong_indices[idx]]}\\nProb: {np.max(y_probabilities[wrong_indices[idx]]):.1f}')\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/meshgrid.html",
    "href": "notebooks/meshgrid.html",
    "title": "Meshgrid",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nimport ipywidgets as widgets\nfrom ipywidgets import interactive\n\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples=1000, noise=0.1, random_state=0)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral);\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\n\nrf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\n# Decision surface\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.viridis)\nax = plt.gca()\nxlim = X[:, 0].min()-0.5, X[:, 0].max() + 0.5\nylim = X[:, 1].min()-0.5, X[:, 1].max() + 0.5\n\n# Create grid to evaluate model\nx_lin = np.linspace(xlim[0], xlim[1], 30)\ny_lin = np.linspace(ylim[0], ylim[1], 30)\n\nXX, YY = np.meshgrid(x_lin, y_lin)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\nZ = rf.predict_proba(xy)[:, 1].reshape(XX.shape)\n\n# Plot decision boundary\nax.contourf(XX, YY, Z, cmap=plt.cm.viridis, alpha=0.2);\n\nplt.colorbar();\n\n\n\n\n\n\n\n\n\nX_arr = np.array([1, 2, 3, 4])\nY_arr = np.array([5, 6, 7])\n\nXX, YY = np.meshgrid(X_arr, Y_arr)\n\n\nXX\n\narray([[1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4]])\n\n\n\nYY\n\narray([[5, 5, 5, 5],\n       [6, 6, 6, 6],\n       [7, 7, 7, 7]])\n\n\n\nXX.shape, YY.shape\n\n((3, 4), (3, 4))\n\n\n\nout = {}\ncount = 0\nfor i in range(XX.shape[0]):\n    for j in range(XX.shape[1]):\n        count = count + 1\n        out[count] = {\"i\": i, \"j\": j, \"XX\": XX[i, j], \"YY\": YY[i, j]}\n\n\npd.DataFrame(out).T\n\n\n\n\n\n\n\n\n\ni\nj\nXX\nYY\n\n\n\n\n1\n0\n0\n1\n5\n\n\n2\n0\n1\n2\n5\n\n\n3\n0\n2\n3\n5\n\n\n4\n0\n3\n4\n5\n\n\n5\n1\n0\n1\n6\n\n\n6\n1\n1\n2\n6\n\n\n7\n1\n2\n3\n6\n\n\n8\n1\n3\n4\n6\n\n\n9\n2\n0\n1\n7\n\n\n10\n2\n1\n2\n7\n\n\n11\n2\n2\n3\n7\n\n\n12\n2\n3\n4\n7\n\n\n\n\n\n\n\n\n\nXX[0], YY[0]\n\n(array([1, 2, 3, 4]), array([5, 5, 5, 5]))\n\n\n\nfor i in range(XX.shape[0]):\n    plt.plot(XX[i], YY[i], 'o', label=i)\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.plot(XX, YY, 'o');\n\n\n\n\n\n\n\n\n\nxlim = X[:, 0].min()-0.5, X[:, 0].max() + 0.5\nylim = X[:, 1].min()-0.5, X[:, 1].max() + 0.5\n\n# Create grid to evaluate model\nx_lin = np.linspace(xlim[0], xlim[1], 30)\ny_lin = np.linspace(ylim[0], ylim[1], 30)\n\n\nx_lin\n\narray([-1.67150293, -1.52070662, -1.36991031, -1.219114  , -1.06831769,\n       -0.91752137, -0.76672506, -0.61592875, -0.46513244, -0.31433613,\n       -0.16353982, -0.01274351,  0.1380528 ,  0.28884911,  0.43964542,\n        0.59044173,  0.74123804,  0.89203435,  1.04283066,  1.19362697,\n        1.34442328,  1.49521959,  1.6460159 ,  1.79681221,  1.94760852,\n        2.09840483,  2.24920114,  2.39999745,  2.55079376,  2.70159007])\n\n\n\nXX, YY = np.meshgrid(x_lin, y_lin)\n\n\ndef update_plot(i=0, j=2):\n    x_point = XX[i, j]\n    y_point = YY[i, j]\n\n\n    plt.plot(XX, YY, 'o', alpha=0.1, color='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.viridis)\n\n    pred = rf.predict_proba([[x_point, y_point]])[:, 1]\n\n    plt.scatter(x_point, y_point, s=100, c='r')\n    plt.title(f\"Prediction P(Class 1): {pred[0]:.2f}\")\n    plt.show()\n\nupdate_plot(0, 0)\n\n\n\n\n\n\n\n\n\nwidget = interactive(update_plot, i=(0, XX.shape[0]-1), j=(0, XX.shape[1]-1))\n\n# Display the widget\ndisplay(widget)\n\n\n\n\n\nXX[0], YY[:, 0]\n\n(array([-1.67150293, -1.52070662, -1.36991031, -1.219114  , -1.06831769,\n        -0.91752137, -0.76672506, -0.61592875, -0.46513244, -0.31433613,\n        -0.16353982, -0.01274351,  0.1380528 ,  0.28884911,  0.43964542,\n         0.59044173,  0.74123804,  0.89203435,  1.04283066,  1.19362697,\n         1.34442328,  1.49521959,  1.6460159 ,  1.79681221,  1.94760852,\n         2.09840483,  2.24920114,  2.39999745,  2.55079376,  2.70159007]),\n array([-1.23673767, -1.13313159, -1.02952551, -0.92591943, -0.82231335,\n        -0.71870727, -0.61510119, -0.51149511, -0.40788903, -0.30428295,\n        -0.20067687, -0.09707079,  0.00653529,  0.11014137,  0.21374745,\n         0.31735353,  0.42095961,  0.52456569,  0.62817177,  0.73177785,\n         0.83538393,  0.93899001,  1.04259609,  1.14620217,  1.24980825,\n         1.35341433,  1.45702041,  1.56062649,  1.66423257,  1.76783865]))\n\n\n\nXX.shape\n\n(30, 30)\n\n\n\nfrom einops import rearrange, repeat, reduce\n\n\nXX.shape\n\n(30, 30)\n\n\n\nXX.ravel().shape\n\n(900,)\n\n\n\nrearrange(XX, 'i j -&gt; (i j) 1').shape, rearrange(XX, 'i j -&gt; (i j)').shape\n\n((900, 1), (900,))\n\n\n\nrearrange(YY, 'i j -&gt; (i j) 1').shape\n\n(900, 1)\n\n\n\nXX_flat = rearrange(XX, 'i j -&gt; (i j) 1')\nYY_flat = rearrange(YY, 'i j -&gt; (i j) 1')\n\n\nnp.array([XX_flat, YY_flat]).shape\n\n(2, 900, 1)\n\n\n\nrearrange([XX_flat, YY_flat], 'f n 1 -&gt; n f').shape\n\n(900, 2)\n\n\n\nX_feature = rearrange([XX_flat, YY_flat], 'f n 1 -&gt; n f')\n\n\nX_feature[:32]\n\narray([[-1.67150293, -1.23673767],\n       [-1.52070662, -1.23673767],\n       [-1.36991031, -1.23673767],\n       [-1.219114  , -1.23673767],\n       [-1.06831769, -1.23673767],\n       [-0.91752137, -1.23673767],\n       [-0.76672506, -1.23673767],\n       [-0.61592875, -1.23673767],\n       [-0.46513244, -1.23673767],\n       [-0.31433613, -1.23673767],\n       [-0.16353982, -1.23673767],\n       [-0.01274351, -1.23673767],\n       [ 0.1380528 , -1.23673767],\n       [ 0.28884911, -1.23673767],\n       [ 0.43964542, -1.23673767],\n       [ 0.59044173, -1.23673767],\n       [ 0.74123804, -1.23673767],\n       [ 0.89203435, -1.23673767],\n       [ 1.04283066, -1.23673767],\n       [ 1.19362697, -1.23673767],\n       [ 1.34442328, -1.23673767],\n       [ 1.49521959, -1.23673767],\n       [ 1.6460159 , -1.23673767],\n       [ 1.79681221, -1.23673767],\n       [ 1.94760852, -1.23673767],\n       [ 2.09840483, -1.23673767],\n       [ 2.24920114, -1.23673767],\n       [ 2.39999745, -1.23673767],\n       [ 2.55079376, -1.23673767],\n       [ 2.70159007, -1.23673767],\n       [-1.67150293, -1.13313159],\n       [-1.52070662, -1.13313159]])\n\n\n\nZ = rf.predict_proba(X_feature)[:, 1]\n\n\nZ.shape\n\n(900,)\n\n\n\nplt.scatter(XX_flat, YY_flat, c=Z, cmap=plt.cm.viridis)\n\n\n\n\n\n\n\n\n\nZ[:10]\n\narray([0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.36, 0.73])\n\n\n\n# Divide Z into k levels\nk = 10\nmin_Z = Z.min()\nmax_Z = Z.max()\n\nlevels = np.linspace(min_Z, max_Z, k)\n\nlevels\n\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])\n\n\n\n# Create an image from Z \nimg = rearrange(Z, '(h w) -&gt; h w', h=XX.shape[0])\nplt.imshow(img, cmap=plt.cm.viridis, \n           extent=[XX.min(), XX.max(), YY.min(), YY.max()], \n           origin='lower',\n           interpolation='spline36')\n\n\n\n\n\n\n\n\n\nplt.contourf(XX, YY, Z.reshape(XX.shape), cmap=plt.cm.viridis, levels=10);\nplt.colorbar();\n\n\n\n\n\n\n\n\n\nplt.contourf?\n\nSignature: plt.contourf(*args, data=None, **kwargs)\nDocstring:\nPlot filled contours.\n\nCall signature::\n\n    contourf([X, Y,] Z, [levels], **kwargs)\n\n`.contour` and `.contourf` draw contour lines and filled contours,\nrespectively.  Except as noted, function signatures and return values\nare the same for both versions.\n\nParameters\n----------\nX, Y : array-like, optional\n    The coordinates of the values in *Z*.\n\n    *X* and *Y* must both be 2D with the same shape as *Z* (e.g.\n    created via `numpy.meshgrid`), or they must both be 1-D such\n    that ``len(X) == N`` is the number of columns in *Z* and\n    ``len(Y) == M`` is the number of rows in *Z*.\n\n    *X* and *Y* must both be ordered monotonically.\n\n    If not given, they are assumed to be integer indices, i.e.\n    ``X = range(N)``, ``Y = range(M)``.\n\nZ : (M, N) array-like\n    The height values over which the contour is drawn.\n\nlevels : int or array-like, optional\n    Determines the number and positions of the contour lines / regions.\n\n    If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries\n    to automatically choose no more than *n+1* \"nice\" contour levels\n    between *vmin* and *vmax*.\n\n    If array-like, draw contour lines at the specified levels.\n    The values must be in increasing order.\n\nReturns\n-------\n`~.contour.QuadContourSet`\n\nOther Parameters\n----------------\ncorner_mask : bool, default: :rc:`contour.corner_mask`\n    Enable/disable corner masking, which only has an effect if *Z* is\n    a masked array.  If ``False``, any quad touching a masked point is\n    masked out.  If ``True``, only the triangular corners of quads\n    nearest those points are always masked out, other triangular\n    corners comprising three unmasked points are contoured as usual.\n\ncolors : color string or sequence of colors, optional\n    The colors of the levels, i.e. the lines for `.contour` and the\n    areas for `.contourf`.\n\n    The sequence is cycled for the levels in ascending order. If the\n    sequence is shorter than the number of levels, it's repeated.\n\n    As a shortcut, single color strings may be used in place of\n    one-element lists, i.e. ``'red'`` instead of ``['red']`` to color\n    all levels with the same color. This shortcut does only work for\n    color strings, not for other ways of specifying colors.\n\n    By default (value *None*), the colormap specified by *cmap*\n    will be used.\n\nalpha : float, default: 1\n    The alpha blending value, between 0 (transparent) and 1 (opaque).\n\ncmap : str or `.Colormap`, default: :rc:`image.cmap`\n    A `.Colormap` instance or registered colormap name. The colormap\n    maps the level values to colors.\n\n    If both *colors* and *cmap* are given, an error is raised.\n\nnorm : `~matplotlib.colors.Normalize`, optional\n    If a colormap is used, the `.Normalize` instance scales the level\n    values to the canonical colormap range [0, 1] for mapping to\n    colors. If not given, the default linear scaling is used.\n\nvmin, vmax : float, optional\n    If not *None*, either or both of these values will be supplied to\n    the `.Normalize` instance, overriding the default color scaling\n    based on *levels*.\n\norigin : {*None*, 'upper', 'lower', 'image'}, default: None\n    Determines the orientation and exact position of *Z* by specifying\n    the position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y*\n    are not given.\n\n    - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n    - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n    - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left\n      corner.\n    - 'image': Use the value from :rc:`image.origin`.\n\nextent : (x0, x1, y0, y1), optional\n    If *origin* is not *None*, then *extent* is interpreted as in\n    `.imshow`: it gives the outer pixel boundaries. In this case, the\n    position of Z[0, 0] is the center of the pixel, not a corner. If\n    *origin* is *None*, then (*x0*, *y0*) is the position of Z[0, 0],\n    and (*x1*, *y1*) is the position of Z[-1, -1].\n\n    This argument is ignored if *X* and *Y* are specified in the call\n    to contour.\n\nlocator : ticker.Locator subclass, optional\n    The locator is used to determine the contour levels if they\n    are not given explicitly via *levels*.\n    Defaults to `~.ticker.MaxNLocator`.\n\nextend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n    Determines the ``contourf``-coloring of values that are outside the\n    *levels* range.\n\n    If 'neither', values outside the *levels* range are not colored.\n    If 'min', 'max' or 'both', color the values below, above or below\n    and above the *levels* range.\n\n    Values below ``min(levels)`` and above ``max(levels)`` are mapped\n    to the under/over values of the `.Colormap`. Note that most\n    colormaps do not have dedicated colors for these by default, so\n    that the over and under values are the edge values of the colormap.\n    You may want to set these values explicitly using\n    `.Colormap.set_under` and `.Colormap.set_over`.\n\n    .. note::\n\n        An existing `.QuadContourSet` does not get notified if\n        properties of its colormap are changed. Therefore, an explicit\n        call `.QuadContourSet.changed()` is needed after modifying the\n        colormap. The explicit call can be left out, if a colorbar is\n        assigned to the `.QuadContourSet` because it internally calls\n        `.QuadContourSet.changed()`.\n\n    Example::\n\n        x = np.arange(1, 10)\n        y = x.reshape(-1, 1)\n        h = x * y\n\n        cs = plt.contourf(h, levels=[10, 30, 50],\n            colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n        cs.cmap.set_over('red')\n        cs.cmap.set_under('blue')\n        cs.changed()\n\nxunits, yunits : registered units, optional\n    Override axis units by specifying an instance of a\n    :class:`matplotlib.units.ConversionInterface`.\n\nantialiased : bool, optional\n    Enable antialiasing, overriding the defaults.  For\n    filled contours, the default is *True*.  For line contours,\n    it is taken from :rc:`lines.antialiased`.\n\nnchunk : int &gt;= 0, optional\n    If 0, no subdivision of the domain.  Specify a positive integer to\n    divide the domain into subdomains of *nchunk* by *nchunk* quads.\n    Chunking reduces the maximum length of polygons generated by the\n    contouring algorithm which reduces the rendering workload passed\n    on to the backend and also requires slightly less RAM.  It can\n    however introduce rendering artifacts at chunk boundaries depending\n    on the backend, the *antialiased* flag and value of *alpha*.\n\nlinewidths : float or array-like, default: :rc:`contour.linewidth`\n    *Only applies to* `.contour`.\n\n    The line width of the contour lines.\n\n    If a number, all levels will be plotted with this linewidth.\n\n    If a sequence, the levels in ascending order will be plotted with\n    the linewidths in the order specified.\n\n    If None, this falls back to :rc:`lines.linewidth`.\n\nlinestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n    *Only applies to* `.contour`.\n\n    If *linestyles* is *None*, the default is 'solid' unless the lines\n    are monochrome.  In that case, negative contours will take their\n    linestyle from :rc:`contour.negative_linestyle` setting.\n\n    *linestyles* can also be an iterable of the above strings\n    specifying a set of linestyles to be used. If this\n    iterable is shorter than the number of contour levels\n    it will be repeated as necessary.\n\nhatches : list[str], optional\n    *Only applies to* `.contourf`.\n\n    A list of cross hatch patterns to use on the filled areas.\n    If None, no hatching will be added to the contour.\n    Hatching is supported in the PostScript, PDF, SVG and Agg\n    backends only.\n\ndata : indexable object, optional\n    If given, all parameters also accept a string ``s``, which is\n    interpreted as ``data[s]`` (unless this raises an exception).\n\nNotes\n-----\n1. `.contourf` differs from the MATLAB version in that it does not draw\n   the polygon edges. To draw edges, add line contours with calls to\n   `.contour`.\n\n2. `.contourf` fills intervals that are closed at the top; that is, for\n   boundaries *z1* and *z2*, the filled region is::\n\n      z1 &lt; Z &lt;= z2\n\n   except for the lowest interval, which is closed on both sides (i.e.\n   it includes the lowest value).\n\n3. `.contour` and `.contourf` use a `marching squares\n   &lt;https://en.wikipedia.org/wiki/Marching_squares&gt;`_ algorithm to\n   compute contour locations.  More information can be found in\n   the source ``src/_contour.h``.\nFile:      ~/miniforge3/lib/python3.9/site-packages/matplotlib/pyplot.py\nType:      function\n\n\n\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\n\nX, Y = np.meshgrid(x, y)\n\nZ = X**2 + Y**2\n\nplt.contourf(X, Y, Z, cmap=plt.cm.viridis);\nplt.gca().set_aspect('equal')\nplt.colorbar();\n\n\n\n\n\n\n\n\n\n# Surface plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(5, 5))\nax = fig.gca(projection='3d')\n\nax.plot_surface(X, Y, Z, cmap=plt.cm.viridis)\n\n/tmp/ipykernel_2896512/3580262868.py:5: MatplotlibDeprecationWarning: Calling gca() with keyword arguments was deprecated in Matplotlib 3.4. Starting two minor releases later, gca() will take no keyword arguments. The gca() function should only be used to get the current axes, or if no axes exist, create new axes with default keyword arguments. To create a new axes with non-default arguments, use plt.axes() or plt.subplot().\n  ax = fig.gca(projection='3d')"
  },
  {
    "objectID": "notebooks/rule-based-vs-ml.html",
    "href": "notebooks/rule-based-vs-ml.html",
    "title": "Traditional Programming vs Machine Learning",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom latexify import latexify\nimport seaborn as sns\n%matplotlib inline\n# config retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n# Load MNIST dataset\nmnist_train = torchvision.datasets.MNIST('../datasets', train=True, transform=torchvision.transforms.ToTensor(), download=True)\nmnist_test = torchvision.datasets.MNIST('../datasets', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n\n\n\n# Function to show a digit marking 28x28 grid with arrows pointing to random pixels\ndef show_digit_with_arrows(digit, label=None):\n    fig, ax = plt.subplots()\n    digit = digit.numpy().reshape(28, 28)\n\n    # Display the digit\n    ax.imshow(digit, cmap='gray')\n\n    # Add gridlines corresponding to 28 rows and columns\n    for i in range(1, 28):\n        ax.axhline(i, color='white', linewidth=0.5)\n        ax.axvline(i, color='white', linewidth=0.5)\n\n    # Display label if available\n    if label is not None:\n        ax.set_title(f'Label: {label}')\n    return fig, ax\n\n\nindex = 2\n# Show a random digit with arrows pointing to random 10 pixels\nfig, ax = show_digit_with_arrows(*mnist_train[index])\n# save figure\nfig.savefig(\"../figures/mnist.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Find indices of digit 4 in the training set\ndigit_4_indices_train = torch.where(torch.tensor(mnist_train.targets) == 4)[0]\ndigit_4_indices_test = torch.where(torch.tensor(mnist_test.targets) == 4)[0]\n\nprint(f\"Indices of digit 4 in Train dataset: {digit_4_indices_train}\")\nprint(f\"Number of digit 4 images in training set: {len(digit_4_indices_train)}\\n\")\n\nIndices of digit 4 in Train dataset: tensor([    2,     9,    20,  ..., 59943, 59951, 59975])\nNumber of digit 4 images in training set: 5842\n\n\n\n/tmp/ipykernel_1361527/214778730.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  digit_4_indices_train = torch.where(torch.tensor(mnist_train.targets) == 4)[0]\n/tmp/ipykernel_1361527/214778730.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  digit_4_indices_test = torch.where(torch.tensor(mnist_test.targets) == 4)[0]\n\n\n\nlatexify(fig_width=7, fig_height=5)\n\nfor i in range(15):\n    plt.subplot(3, 5, i+1)\n    plt.imshow(mnist_train.data[digit_4_indices_train[i]], cmap='gray')\n    plt.title(f\"idx: {digit_4_indices_train[i]}\")\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\n# Select a sample from the training set\nsample_idx_1 = 60\nimage, label = mnist_train[sample_idx_1]\nplt.imshow(image.squeeze().numpy(), cmap='gray')\nplt.title(f\"Label: {label}\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Function to extract edges based on intensity threshold\ndef extract_edges(image, threshold=0.1):\n    '''\n    Input:\n        image: torch.tensor of shape (28, 28)\n        threshold: (float) the minimum intensity value to be considered as white pixel\n    '''\n    edges = torch.zeros_like(image)\n\n    # converting all the pixels with intensity greater than threshold to white\n    edges[image &gt; threshold] = 1.0\n    return edges\n\n\n# Creating rules based upon one image\nedges = extract_edges(image)\n\nplt.imshow(edges[0, :, :], cmap='gray')\n\n# finding areas of edges\nleft_edge_train = edges[:, 4:15, 3:12]\nupper_right_edge_train = edges[:, 4:19, 17:24]\nmiddle_edge_train = edges[:, 14:20, 5:25]\nlower_right_edge_train = edges[:, 17:24, 18:24]\n\n\n# R1 (4-15, 3-12)\nr1 = plt.Rectangle((3, 4), 9, 11, linewidth=1, edgecolor='r', facecolor='none')\nr2 = plt.Rectangle((17, 4), 7, 15, linewidth=1, edgecolor='g', facecolor='none')\nr3 = plt.Rectangle((5, 14), 20, 6, linewidth=1, edgecolor='b', facecolor='none')\nr4 = plt.Rectangle((18, 17), 6, 7, linewidth=1, edgecolor='y', facecolor='none')\nfor rect in [r1, r2, r3, r4]:\n    plt.gca().add_patch(rect)\n\n\n\n\n\n\n\n\n\n\n# creat a subplot 2 rows by 2 columns\nfig, axs = plt.subplots(2, 2, figsize=(8, 10))\n\n# plotting the images\naxs[0, 0].imshow(left_edge_train.squeeze().numpy(), cmap='gray')\naxs[0, 1].imshow(upper_right_edge_train.squeeze().numpy(), cmap='gray')\naxs[1, 0].imshow(middle_edge_train.squeeze().numpy(), cmap='gray')\naxs[1, 1].imshow(lower_right_edge_train.squeeze().numpy(), cmap='gray')\n\naxs[0, 0].set_title(f\"Left Edge\\nWhite pixels: {int(left_edge_train.sum())}/{left_edge_train.numel()}\")\naxs[0, 1].set_title(f\"Upper Right Edge\\nWhite pixels: {int(upper_right_edge_train.sum())}/{upper_right_edge_train.numel()}\")\naxs[1, 0].set_title(f\"Middle Edge\\nWhite pixels: {int(middle_edge_train.sum())}/{middle_edge_train.numel()}\")\naxs[1, 1].set_title(f\"Lower Right Edge\\nWhite pixels: {int(lower_right_edge_train.sum())}/{lower_right_edge_train.numel()}\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rule-based digit classifier for digit 4\ndef rule_based_classifier(image):\n    # Extract edges\n    edges = extract_edges(image)\n\n    # Define rules for digit 4 based on the edges of the digit\n    left_edge = edges[:, 4:15, 3:12]\n    upper_right_edge = edges[:, 4:19, 17:24]\n    middle_edge = edges[:, 14:20, 5:25]\n    lower_right_edge = edges[:, 17:24, 18:24]\n\n    # Check if all required edges are present by checking the number of white pixels for each edge.\n    # The number of white pixels for each edge is 'sub' less than the number of pixels in the edge for the above take digit.\n    sub = 10\n    if torch.sum(left_edge) &gt; left_edge_train.sum() - sub and torch.sum(upper_right_edge) &gt; upper_right_edge_train.sum() - sub and torch.sum(middle_edge) &gt; middle_edge_train.sum() - sub and torch.sum(lower_right_edge) &gt; lower_right_edge_train.sum() - sub:\n        return 4\n    else:\n        return -1 # -1 indicates that the digit is not 4\n\n\n# Display some wrongly classified images\n\nindices = [6, 19, 25, 200]\n# define image size\nplt.figure(figsize=(14, 3))\n\nfor i in range(4):\n    plt.subplot(1, 4, i+1)\n    image, label = mnist_test[indices[i]]\n    pred = rule_based_classifier(image)\n    pred = pred if pred != -1 else \"Not 4\"\n    plt.title(f\"Label: {label}, Predicted: {pred}\")\n    plt.imshow(image.squeeze().numpy(), cmap='gray')\n\n\n\n\n\n\n\n\n\n# Evaluating the rule-based classifier\ncount = 0\ncount_4 = 0\nfor i, (image, label) in enumerate(mnist_test):\n    classification = rule_based_classifier(image)\n    if (classification == 4 and label == 4) or (classification == -1 and label != 4):\n        count += 1\n    if (classification == 4 and label == 4):\n        count_4 += 1\n\naccuracy_rule = count * 100/ len(mnist_test)\npercentage_TP_rule = count_4 * 100/ len(digit_4_indices_test)\nprint(f\"Accuracy of the rule-based classifier: {accuracy_rule} %\")\nprint(f\"Percentage of 4s actually classified as 4 (percentage of True Positives): {percentage_TP_rule:.3} %\")\n\nAccuracy of the rule-based classifier: 88.56 %\nPercentage of 4s actually classified as 4 (percentage of True Positives): 4.28 %\n\n\nNote: As per rules, it is predicting most of the digits as non-4 for most of the digits. And since the number of non-4 digits are much more compared to number of instances of the digit 4, the accuracy is high. But this is not a good model as it is not predicting the digit 4 correctly.\n\nML based approach\n\n# Flatten the images and convert the labels to 4 and -1 for binary classification problem\nX_train = mnist_train.data.numpy().reshape((len(mnist_train), -1))\ny_train = np.where(mnist_train.targets.numpy() == 4, 4, -1)\n\nX_test = mnist_test.data.numpy().reshape((len(mnist_test), -1))\ny_test = np.where(mnist_test.targets.numpy() == 4, 4, -1)\n\n\n# Create and train the MLP model\nmlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\nmlp_model.fit(X_train, y_train)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\nMLPClassifier(max_iter=20, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier(max_iter=20, random_state=42)\n\n\n\n# Evaluate the model\ny_pred = mlp_model.predict(X_test)\naccuracy_ML = accuracy_score(y_test,( y_pred))\naccuracy_ML = accuracy_ML * 100\npercentage_TP_ML = np.sum((y_test == 4) & (y_pred == 4)) * 100 / len(digit_4_indices_test)\nprint(f'Test Accuracy: {accuracy_ML:.2f}%')\nprint(f\"Percentage of 4s actually classified as 4 (percentage of True Positives): {percentage_TP_ML:.3} %\")\n\nTest Accuracy: 99.47%\nPercentage of 4s actually classified as 4 (percentage of True Positives): 97.1 %\n\n\n\n\nComparison of Rule-based system and ML based system\n\n# Categories for the bar plot\ncategories = ['Accuracy', 'True Positive Percentage']\n\n# Values for the rule-based classifier\nrule_based_values = [accuracy_rule, percentage_TP_rule]\n\n# Values for the MLP classifier\nmlp_values = [accuracy_ML, percentage_TP_ML]\n\n# Bar width\nbar_width = 0.35\n\n# X-axis positions for the bars\nindex = range(len(categories))\n\n# Plotting the bar plot\nfig, ax = plt.subplots(figsize=(9, 5))\nbar1 = ax.bar(index, rule_based_values, bar_width, label='Rule-Based Classifier')\nbar2 = ax.bar([i + bar_width for i in index], mlp_values, bar_width, label='MLP Classifier')\n\n# Adding labels, title, and legend\nax.set_xlabel('Metrics')\nax.set_ylabel('Percentage / Accuracy')\nax.set_title('Comparison of Classifiers')\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(categories)\nax.legend()\n\n# Display the values on top of the bars\nfor bar in bar1 + bar2:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "notebooks/movie-recommendation-knn-mf.html",
    "href": "notebooks/movie-recommendation-knn-mf.html",
    "title": "Movie Recommendation using Matrix Factorization",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Now working with real data\n\n# Load the data\n\ndf = pd.read_excel(\"Movie Recommendation 2024 (Responses).xlsx\")\ndf.head(10)\n\n\n\n\n\n\n\n\n\nTimestamp\nYour name\nSholay\nSwades (We The People)\nThe Matrix (I)\nInterstellar\nDangal\nTaare Zameen Par\nShawshank Redemption\nThe Dark Knight\nNotting Hill\nUri: The Surgical Strike\n\n\n\n\n0\n2024-02-06 09:42:36.374\nSrijahnavi\n4.0\n3.0\n4.0\n5.0\n5.0\n4.0\n4.0\n5.0\n4.0\n5.0\n\n\n1\n2024-02-06 09:42:50.227\nPriya\n3.0\n3.0\n3.0\n4.0\n4.0\n4.0\n5.0\n3.0\n3.0\n4.0\n\n\n2\n2024-02-06 09:43:05.181\nPulkit Gautam\n4.0\n5.0\n5.0\n5.0\n3.0\n3.0\n5.0\n5.0\n4.0\n3.0\n\n\n3\n2024-02-06 09:43:16.138\nTwinkle Devda\n5.0\n4.0\n5.0\n4.0\n2.0\n4.0\nNaN\nNaN\nNaN\n4.0\n\n\n4\n2024-02-06 09:43:18.300\nIshika Raj\n5.0\n5.0\n4.0\n5.0\n5.0\n5.0\nNaN\nNaN\nNaN\nNaN\n\n\n5\n2024-02-06 09:43:37.950\nKaushik\n1.0\n4.0\n5.0\n5.0\n2.0\n4.0\n5.0\n3.0\nNaN\n4.0\n\n\n6\n2024-02-06 09:43:48.787\nKajal\n3.0\n4.0\nNaN\n4.0\n3.0\n5.0\nNaN\nNaN\nNaN\n4.0\n\n\n7\n2024-02-06 09:43:58.223\nsawan verma\n4.0\n5.0\n5.0\n5.0\n4.0\n4.0\nNaN\n4.0\nNaN\n4.0\n\n\n8\n2024-02-06 09:43:58.698\nDaksh Jain\n5.0\nNaN\n5.0\n5.0\n4.0\n4.0\n5.0\n5.0\nNaN\n4.0\n\n\n9\n2024-02-06 09:44:13.505\nAkshat Pratap Singh\n4.0\nNaN\n5.0\n5.0\n4.0\nNaN\nNaN\n5.0\nNaN\n4.0\n\n\n\n\n\n\n\n\n\n# Discard the timestamp column\n\ndf = df.drop('Timestamp', axis=1)\n\n# Make the \"Your Name\" column the index\n\ndf = df.set_index('Your name')\ndf\n\n\n\n\n\n\n\n\n\nSholay\nSwades (We The People)\nThe Matrix (I)\nInterstellar\nDangal\nTaare Zameen Par\nShawshank Redemption\nThe Dark Knight\nNotting Hill\nUri: The Surgical Strike\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSrijahnavi\n4.0\n3.0\n4.0\n5.0\n5.0\n4.0\n4.0\n5.0\n4.0\n5.0\n\n\nPriya\n3.0\n3.0\n3.0\n4.0\n4.0\n4.0\n5.0\n3.0\n3.0\n4.0\n\n\nPulkit Gautam\n4.0\n5.0\n5.0\n5.0\n3.0\n3.0\n5.0\n5.0\n4.0\n3.0\n\n\nTwinkle Devda\n5.0\n4.0\n5.0\n4.0\n2.0\n4.0\nNaN\nNaN\nNaN\n4.0\n\n\nIshika Raj\n5.0\n5.0\n4.0\n5.0\n5.0\n5.0\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nYash Bothra\n3.0\n4.0\n4.0\n4.0\n3.0\n4.0\n5.0\n5.0\nNaN\n3.0\n\n\nZaqi\n2.0\n3.0\n4.0\n5.0\n2.0\n4.0\n5.0\n5.0\n3.0\n4.0\n\n\nManas Kawal\n3.0\nNaN\n5.0\n5.0\n5.0\n4.0\n5.0\n5.0\n4.0\n4.0\n\n\nSiya Patil\n5.0\nNaN\n4.0\n4.0\n5.0\n4.0\nNaN\nNaN\nNaN\n5.0\n\n\nGaurav Kumar\nNaN\nNaN\nNaN\n4.0\n3.0\n5.0\nNaN\n5.0\nNaN\n3.0\n\n\n\n\n114 rows  10 columns\n\n\n\n\n\ndf.index\n\nIndex(['Srijahnavi ', 'Priya', 'Pulkit Gautam', 'Twinkle Devda ',\n       'Ishika Raj ', 'Kaushik', 'Kajal', 'sawan verma', 'Daksh Jain',\n       'Akshat Pratap Singh ',\n       ...\n       'Shreya Patel', 'Omkar Rajeev Prabhu', 'Nihar Shah', 'Sai Krishna ',\n       'Mithil Pechimuthu', 'Yash Bothra', 'Zaqi', 'Manas Kawal ',\n       'Siya Patil', 'Gaurav Kumar'],\n      dtype='object', name='Your name', length=114)\n\n\n\n# Get index for user and movie\nuser = 'Ayush Shrivastava'\n\nprint(user in df.index)\n\n# Get the movie ratings for user\nuser_ratings = df.loc[user]\nuser_ratings\n\nTrue\n\n\nSholay                      5.0\nSwades (We The People)      4.0\nThe Matrix (I)              5.0\nInterstellar                5.0\nDangal                      3.0\nTaare Zameen Par            4.0\nShawshank Redemption        4.0\nThe Dark Knight             4.0\nNotting Hill                3.0\nUri: The Surgical Strike    4.0\nName: Ayush Shrivastava, dtype: float64\n\n\n\n# Number of missing values\ndf.isnull().sum()\n\nSholay                      18\nSwades (We The People)      45\nThe Matrix (I)              21\nInterstellar                13\nDangal                       1\nTaare Zameen Par             1\nShawshank Redemption        46\nThe Dark Knight             31\nNotting Hill                63\nUri: The Surgical Strike    11\ndtype: int64\n\n\n\n# Generic Matrix Factorization (without missing values)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nn_users, n_movies = 20, 10\n\n# A is a matrix of size (n_users, n_movies) randomly generated values between 1 and 5\nA = torch.randint(1, 6, (n_users, n_movies), dtype=torch.float)\nA\n\ntensor([[1., 4., 5., 3., 4., 4., 2., 5., 2., 2.],\n        [4., 3., 1., 1., 2., 1., 3., 2., 5., 1.],\n        [4., 2., 2., 4., 2., 4., 1., 5., 4., 1.],\n        [3., 1., 3., 2., 1., 3., 1., 1., 1., 2.],\n        [5., 1., 5., 1., 3., 5., 2., 3., 5., 1.],\n        [2., 2., 3., 2., 4., 5., 3., 4., 2., 1.],\n        [5., 5., 1., 5., 4., 2., 5., 5., 2., 4.],\n        [5., 4., 2., 4., 1., 2., 2., 5., 1., 1.],\n        [4., 5., 5., 5., 4., 5., 1., 1., 1., 5.],\n        [4., 2., 5., 1., 5., 4., 1., 4., 1., 1.],\n        [5., 1., 3., 3., 1., 1., 2., 4., 2., 4.],\n        [5., 5., 4., 3., 1., 5., 3., 4., 3., 1.],\n        [4., 4., 3., 5., 3., 2., 4., 5., 4., 2.],\n        [1., 4., 3., 3., 2., 3., 2., 4., 3., 1.],\n        [1., 3., 4., 2., 4., 4., 2., 4., 1., 4.],\n        [4., 1., 3., 3., 3., 4., 2., 2., 2., 1.],\n        [4., 3., 4., 5., 3., 1., 2., 5., 1., 5.],\n        [1., 1., 1., 5., 5., 3., 4., 5., 4., 4.],\n        [1., 4., 4., 1., 5., 4., 3., 3., 4., 5.],\n        [5., 1., 5., 5., 2., 2., 3., 4., 5., 3.]])\n\n\n\nA.shape\n\ntorch.Size([20, 10])\n\n\nLet us decompose A as WH. W is of shape (n, k) and H is of shape (k, n). We can write the above equation as: A = WH\n\n# Randomly initialize A and B\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nr = 3\nW = torch.randn(n_users, r, requires_grad=True, device=device)\nH = torch.randn(r, n_movies, requires_grad=True, device=device)\n\nA = A.to(device)\n\n# Compute the loss\nwith torch.no_grad():\n    loss = torch.norm(torch.mm(W, H) - A)\n    print(loss)\n\ntensor(53.8487, device='cuda:0')\n\n\n\npd.DataFrame(torch.mm(W, H).cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0.789320\n0.273251\n-0.803057\n0.026515\n-0.439304\n0.704782\n-0.403163\n0.184198\n-0.386802\n0.390207\n\n\n1\n-2.582092\n-0.965984\n-4.715790\n0.464595\n-0.800138\n2.220691\n-1.210865\n1.380103\n-2.509600\n-0.448290\n\n\n2\n-6.278386\n-2.250423\n-0.746872\n0.326767\n1.347383\n-1.277587\n0.722436\n0.489918\n-0.613798\n-2.293076\n\n\n3\n-0.894911\n-0.162974\n0.904375\n-0.071838\n-0.077017\n0.688095\n1.019274\n-0.818665\n0.918633\n-0.569915\n\n\n4\n1.630395\n0.457543\n0.028392\n-0.036418\n0.088358\n-0.830817\n-0.725712\n0.438782\n-0.337688\n0.723440\n\n\n\n\n\n\n\n\n\npd.DataFrame(A.cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n1.0\n4.0\n5.0\n3.0\n4.0\n4.0\n2.0\n5.0\n2.0\n2.0\n\n\n1\n4.0\n3.0\n1.0\n1.0\n2.0\n1.0\n3.0\n2.0\n5.0\n1.0\n\n\n2\n4.0\n2.0\n2.0\n4.0\n2.0\n4.0\n1.0\n5.0\n4.0\n1.0\n\n\n3\n3.0\n1.0\n3.0\n2.0\n1.0\n3.0\n1.0\n1.0\n1.0\n2.0\n\n\n4\n5.0\n1.0\n5.0\n1.0\n3.0\n5.0\n2.0\n3.0\n5.0\n1.0\n\n\n\n\n\n\n\n\n\n# Optimizer\noptimizer = optim.Adam([W, H], lr=0.01)\n\n# Train the model\n\nfor i in range(600):\n    # Compute the loss\n    loss = torch.norm(torch.mm(W, H) - A)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 10 == 0:\n        print(i, loss.item())\n\n0 53.84873580932617\n10 51.85832214355469\n20 50.30470275878906\n30 48.986907958984375\n40 47.58940124511719\n50 45.762447357177734\n60 43.2141227722168\n70 39.856666564941406\n80 35.923824310302734\n90 31.9387149810791\n100 28.36842155456543\n110 25.3414249420166\n120 22.89077377319336\n130 21.019773483276367\n140 19.64716911315918\n150 18.663381576538086\n160 17.948944091796875\n170 17.403179168701172\n180 16.958683013916016\n190 16.57893943786621\n200 16.24703598022461\n210 15.956071853637695\n220 15.702412605285645\n230 15.481629371643066\n240 15.287639617919922\n250 15.114280700683594\n260 14.95695686340332\n270 14.812972068786621\n280 14.680870056152344\n290 14.559730529785156\n300 14.448781967163086\n310 14.347304344177246\n320 14.25462818145752\n330 14.170149803161621\n340 14.093340873718262\n350 14.02373218536377\n360 13.960897445678711\n370 13.904433250427246\n380 13.853943824768066\n390 13.809029579162598\n400 13.769281387329102\n410 13.73428726196289\n420 13.703628540039062\n430 13.676892280578613\n440 13.653679847717285\n450 13.63360595703125\n460 13.616312026977539\n470 13.601462364196777\n480 13.588751792907715\n490 13.577902793884277\n500 13.568666458129883\n510 13.560823440551758\n520 13.554177284240723\n530 13.54855728149414\n540 13.543814659118652\n550 13.53981876373291\n560 13.536458015441895\n570 13.533635139465332\n580 13.531268119812012\n590 13.529285430908203\n\n\n\npd.DataFrame(torch.mm(W, H).cpu().detach().numpy()).head(2)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n2.030807\n3.324325\n4.240799\n2.752831\n4.435637\n4.398169\n2.251596\n3.653164\n2.114700\n3.350343\n\n\n1\n4.159636\n1.692163\n2.095336\n2.669384\n0.976548\n1.772289\n1.940019\n2.964438\n2.785541\n0.846138\n\n\n\n\n\n\n\n\n\npd.DataFrame(A.cpu()).head(2)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n1.0\n4.0\n5.0\n3.0\n4.0\n4.0\n2.0\n5.0\n2.0\n2.0\n\n\n1\n4.0\n3.0\n1.0\n1.0\n2.0\n1.0\n3.0\n2.0\n5.0\n1.0\n\n\n\n\n\n\n\n\n\ndef factorize(A, k, device=torch.device(\"cpu\")):\n    \"\"\"Factorize the matrix A into W and H\n    A: input matrix of size (n_users, n_movies)\n    k: number of latent features\n    \n    Returns W and H\n    W: matrix of size (n_users, k)\n    H: matrix of size (k, n_movies)\n    \"\"\"\n    A = A.to(device)\n    # Randomly initialize W and H\n    W = torch.randn(A.shape[0], k, requires_grad=True, device=device)\n    H = torch.randn(k, A.shape[1], requires_grad=True, device=device)\n    \n    # Optimizer\n    optimizer = optim.Adam([W, H], lr=0.01)\n    \n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        loss = torch.norm(torch.mm(W, H) - A)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return W, H, loss\n\n\nfor k in [1, 2, 3, 4, 5, 6, 9]:\n    W, H, loss = factorize(A, k, device=device)\n    print(k, loss.item())\n\n1 18.66737174987793\n2 16.084671020507812\n3 13.519211769104004\n4 11.201322555541992\n5 9.107473373413086\n6 7.247912406921387\n9 2.448281764984131\n\n\n\npd.DataFrame(torch.mm(W,H).cpu().detach().numpy()).head(2)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n1.021576\n4.027153\n4.947227\n2.994748\n4.041862\n3.993290\n1.872400\n5.023528\n2.044280\n2.016569\n\n\n1\n3.923019\n2.903162\n1.188122\n1.018530\n1.850515\n1.024056\n3.455001\n1.916324\n4.842122\n0.941112\n\n\n\n\n\n\n\n\n\npd.DataFrame(A.cpu()).head(2)\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n1.0\n4.0\n5.0\n3.0\n4.0\n4.0\n2.0\n5.0\n2.0\n2.0\n\n\n1\n4.0\n3.0\n1.0\n1.0\n2.0\n1.0\n3.0\n2.0\n5.0\n1.0\n\n\n\n\n\n\n\n\n\n# With missing values\n\n# Randomly replace some entries with NaN\n\nA = torch.randint(1, 6, (n_users, n_movies), dtype=torch.float)\nA[torch.rand(A.shape) &lt; 0.5] = float('nan')\nA\n\ntensor([[2., nan, 5., 4., 2., nan, 1., nan, nan, 3.],\n        [nan, nan, nan, 5., 3., nan, nan, 2., nan, 1.],\n        [4., 5., 4., 1., 5., nan, nan, 1., 4., 5.],\n        [4., 3., nan, 5., nan, nan, nan, nan, nan, 1.],\n        [2., nan, 1., nan, nan, nan, nan, nan, nan, nan],\n        [nan, 3., nan, nan, nan, 1., nan, 3., nan, 4.],\n        [1., 3., 2., nan, nan, nan, nan, nan, nan, 4.],\n        [nan, nan, nan, 3., nan, 2., nan, 1., 5., nan],\n        [nan, nan, nan, nan, nan, 5., 5., nan, nan, 1.],\n        [nan, 1., nan, nan, 5., nan, 2., 1., 3., nan],\n        [4., 5., 4., 1., nan, nan, 3., nan, 3., 5.],\n        [3., nan, 3., nan, 3., 3., nan, 2., nan, nan],\n        [nan, 4., nan, 4., nan, nan, 3., nan, 1., nan],\n        [3., nan, nan, 5., 4., nan, 3., nan, nan, nan],\n        [nan, 2., nan, nan, nan, 2., nan, 2., 2., nan],\n        [nan, 4., 1., 2., nan, nan, nan, nan, 4., nan],\n        [1., nan, 1., 2., nan, 2., 5., nan, 2., nan],\n        [nan, 2., nan, 3., nan, 3., 3., nan, 1., nan],\n        [nan, 4., 5., 1., 2., 5., 3., nan, nan, 3.],\n        [nan, 1., nan, nan, 4., 1., nan, nan, nan, nan]])\n\n\n\nW, H, loss = factorize(A, 2, device=device)\nloss\n\ntensor(nan, device='cuda:0', grad_fn=&lt;LinalgVectorNormBackward0&gt;)\n\n\nAs expected, the above function does not work. Our current loss function does not handle missing values.\n\nA.shape\n\ntorch.Size([20, 10])\n\n\n\nmask = ~torch.isnan(A)\nmask\n\ntensor([[ True, False,  True,  True,  True, False,  True, False, False,  True],\n        [False, False, False,  True,  True, False, False,  True, False,  True],\n        [ True,  True,  True,  True,  True, False, False,  True,  True,  True],\n        [ True,  True, False,  True, False, False, False, False, False,  True],\n        [ True, False,  True, False, False, False, False, False, False, False],\n        [False,  True, False, False, False,  True, False,  True, False,  True],\n        [ True,  True,  True, False, False, False, False, False, False,  True],\n        [False, False, False,  True, False,  True, False,  True,  True, False],\n        [False, False, False, False, False,  True,  True, False, False,  True],\n        [False,  True, False, False,  True, False,  True,  True,  True, False],\n        [ True,  True,  True,  True, False, False,  True, False,  True,  True],\n        [ True, False,  True, False,  True,  True, False,  True, False, False],\n        [False,  True, False,  True, False, False,  True, False,  True, False],\n        [ True, False, False,  True,  True, False,  True, False, False, False],\n        [False,  True, False, False, False,  True, False,  True,  True, False],\n        [False,  True,  True,  True, False, False, False, False,  True, False],\n        [ True, False,  True,  True, False,  True,  True, False,  True, False],\n        [False,  True, False,  True, False,  True,  True, False,  True, False],\n        [False,  True,  True,  True,  True,  True,  True, False, False,  True],\n        [False,  True, False, False,  True,  True, False, False, False, False]])\n\n\n\nmask.sum()\n\ntensor(93)\n\n\n\nW = torch.randn(A.shape[0], k, requires_grad=True, device=device)\nH = torch.randn(k, A.shape[1],  requires_grad=True, device=device)\n\ndiff_matrix = torch.mm(W, H)-A.to(device)\ndiff_matrix.shape\n\ntorch.Size([20, 10])\n\n\n\ndiff_matrix\n\ntensor([[ -3.0289,      nan,  -1.8404,  -6.6440,  -5.9554,      nan,  -4.3658,\n              nan,      nan,  -6.2069],\n        [     nan,      nan,      nan,  -4.4589,  -5.1754,      nan,      nan,\n          -2.6537,      nan,  -4.0168],\n        [ -0.7127, -11.6821,  -3.2689,   1.6448,  -3.8675,      nan,      nan,\n          -3.2507,  -5.3294,  -6.7430],\n        [ -2.1208,   0.4956,      nan,  -5.0059,      nan,      nan,      nan,\n              nan,      nan,   0.1138],\n        [  1.6098,      nan,   3.4681,      nan,      nan,      nan,      nan,\n              nan,      nan,      nan],\n        [     nan,  -4.2573,      nan,      nan,      nan,  -2.3573,      nan,\n          -2.0646,      nan,  -6.0043],\n        [ -0.5000,   3.2015,   1.5488,      nan,      nan,      nan,      nan,\n              nan,      nan,  -1.3461],\n        [     nan,      nan,      nan,  -2.7268,      nan,  -6.2156,      nan,\n          -4.0452,  -8.8992,      nan],\n        [     nan,      nan,      nan,      nan,      nan,  -1.6386,  -3.1412,\n              nan,      nan,   1.7203],\n        [     nan,  -0.5364,      nan,      nan,  -4.6901,      nan,   0.1904,\n           0.5138,  -4.8086,      nan],\n        [ -5.2407,   2.3314,  -2.7369,  -2.7617,      nan,      nan,  -2.8258,\n              nan,  -2.5273,  -1.9235],\n        [ -3.4302,      nan,  -1.8367,      nan,  -3.4663,  -6.3333,      nan,\n          -5.4401,      nan,      nan],\n        [     nan,  -6.2419,      nan,  -7.8502,      nan,      nan,   0.5714,\n              nan,  -3.3440,      nan],\n        [ -5.5879,      nan,      nan,  -8.9398,  -0.4232,      nan,  -4.5401,\n              nan,      nan,      nan],\n        [     nan,  -6.1907,      nan,      nan,      nan,  -1.4435,      nan,\n          -0.6638,  -1.4553,      nan],\n        [     nan,   4.2424,  -2.2983,  -3.4736,      nan,      nan,      nan,\n              nan,  -4.9107,      nan],\n        [ -4.3128,      nan,  -1.7366,  -0.8696,      nan,   1.6266,  -8.9258,\n              nan,  -1.1152,      nan],\n        [     nan,  -6.3231,      nan,   0.3756,      nan,  -0.4233,   0.5720,\n              nan,   1.1861,      nan],\n        [     nan,  -2.2517,  -6.8329,  -1.2483,  -2.0385,  -6.0957,  -4.2059,\n              nan,      nan,  -3.0660],\n        [     nan,  -2.5897,      nan,      nan,  -6.6265,  -3.0056,      nan,\n              nan,      nan,      nan]], device='cuda:0',\n       grad_fn=&lt;SubBackward0&gt;)\n\n\n\n# Mask the matrix\ndiff_matrix[mask].shape\n\ntorch.Size([93])\n\n\n\n# Modify the loss function to ignore NaN values\n\ndef factorize(A, k, device=torch.device(\"cpu\")):\n    \"\"\"Factorize the matrix D into A and B\"\"\"\n    A = A.to(device)\n    # Randomly initialize A and B\n    W = torch.randn(A.shape[0], k, requires_grad=True, device=device)\n    H = torch.randn(k, A.shape[1], requires_grad=True, device=device)\n    # Optimizer\n    optimizer = optim.Adam([W, H], lr=0.01)\n    mask = ~torch.isnan(A)\n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        diff_matrix = torch.mm(W, H) - A\n        diff_vector = diff_matrix[mask]\n        loss = torch.norm(diff_vector)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return W, H, loss\n\n\nW, H, loss = factorize(A, 5, device=device)\nloss\n\ntensor(0.0325, device='cuda:0', grad_fn=&lt;LinalgVectorNormBackward0&gt;)\n\n\n\ntorch.mm(W, H)\n\ntensor([[  2.0060,   4.2384,   5.0177,   3.9902,   1.9983,   1.8141,   0.9947,\n           0.6535,  -5.8801,   3.0064],\n        [ -2.5924,   0.4080,  -1.4594,   4.9931,   2.9982,  -0.8818,  -1.6683,\n           2.0007,  -6.1505,   1.0027],\n        [  4.0070,   5.0059,   4.0123,   0.9978,   5.0000,   2.6739,   4.5491,\n           0.9965,   3.9934,   5.0040],\n        [  4.0063,   3.0025,   5.2751,   5.0058,  -1.0327,   5.7686,  17.8772,\n          -0.5065,   6.9793,   0.9997],\n        [  2.0043,   1.1548,   1.0048,   2.9191,   1.4708,   2.9927,   3.1406,\n           1.6878,   2.3006,   0.9258],\n        [  0.3356,   3.0037,   0.0982,   4.4917,   6.4616,   1.0014,   0.1354,\n           3.0009,  -1.3240,   4.0069],\n        [  1.0009,   3.0039,   2.0116,  -9.5763,   2.8521,  -4.9152, -12.9332,\n          -2.2994,  -2.8702,   4.0032],\n        [  2.0256,   4.3787,   2.3926,   2.9961,   5.6892,   1.9912,   9.4118,\n           0.9957,   4.9925,   4.8245],\n        [  4.6176,   2.5496,   4.5064,   2.6404,  -0.3959,   4.9912,   5.0017,\n           0.8769,   2.5299,   1.0006],\n        [ -1.3725,   0.9985,  -2.2606,   0.4189,   4.9993,  -1.4495,   1.9910,\n           1.0002,   2.9967,   2.7223],\n        [  4.0084,   5.0066,   4.0151,   0.9998,   5.0623,   2.5927,   3.0007,\n           1.1967,   2.9927,   5.0048],\n        [  3.0058,   3.1937,   3.0116,   3.0567,   3.0001,   3.0031,   0.0244,\n           2.0001,  -0.8893,   2.7542],\n        [  2.9764,   3.9994,   2.8232,   3.9924,   4.8724,   3.2035,   2.9892,\n           2.3888,   0.9898,   3.9700],\n        [  3.0070,   0.4287,  -1.4100,   5.0030,   4.0019,   5.3721,   2.9959,\n           4.6116,   6.7204,   1.1466],\n        [  0.5388,   1.9984,   0.2752,   4.6641,   3.9920,   1.9974,   5.9558,\n           1.9981,   1.9922,   2.4148],\n        [  0.3942,   4.0028,   1.0083,   2.0015,   6.5739,  -0.0200,   7.7051,\n           0.6758,   3.9930,   5.1231],\n        [  1.0026,   0.2624,   0.9985,   2.0028,  -0.7153,   1.9998,   5.0004,\n           0.1934,   2.0012,  -0.3363],\n        [  2.3751,   1.9994,   2.1854,   2.9951,   1.5356,   2.9961,   2.9923,\n           1.4122,   0.9959,   1.4741],\n        [  5.9310,   4.0043,   5.0074,   0.9974,   2.0018,   4.9956,   2.9986,\n           1.3067,   4.2646,   3.0030],\n        [  1.7298,   1.0019,  -0.8864,  -0.2205,   4.0021,   1.0070,  -7.6848,\n           2.8271,   0.2264,   2.0611]], device='cuda:0',\n       grad_fn=&lt;MmBackward0&gt;)\n\n\n\ndf.values.shape\n\n(114, 10)\n\n\n\nA = torch.tensor(df.values, dtype=torch.float)\nW, H, loss = factorize(A, 5, device=device)\n\n\n# Dropdown menu for user and predict for all movies\nfrom ipywidgets import interact, widgets\n\n\n\ndef predict_movie_ratings(user, df, W, H):\n    idx = df.index.get_loc(user)\n    user_ratings = df.iloc[idx]\n    user_ratings = user_ratings.dropna()\n    user_ratings = user_ratings.to_frame().T\n    user_ratings = user_ratings.reindex(columns=df.columns, fill_value=float('nan'))\n    user_ratings = user_ratings.to_numpy()\n    \n    predicted_ratings = torch.mm(W, H).cpu().detach().numpy()\n    return pd.DataFrame({\"Observed\": user_ratings.flatten(), \"Predicted\": predicted_ratings[idx].flatten()}, index=df.columns)\n\n\npredict_movie_ratings('Ayush Shrivastava', df, W, H)\n\n\n\n\n\n\n\n\n\nObserved\nPredicted\n\n\n\n\nSholay\n5.0\n4.864046\n\n\nSwades (We The People)\n4.0\n4.143846\n\n\nThe Matrix (I)\n5.0\n4.996091\n\n\nInterstellar\n5.0\n4.691528\n\n\nDangal\n3.0\n3.363646\n\n\nTaare Zameen Par\n4.0\n4.089129\n\n\nShawshank Redemption\n4.0\n4.151148\n\n\nThe Dark Knight\n4.0\n4.241074\n\n\nNotting Hill\n3.0\n2.570189\n\n\nUri: The Surgical Strike\n4.0\n3.842794\n\n\n\n\n\n\n\n\n\ninteract(predict_movie_ratings, user=widgets.Dropdown(options=df.index, value=df.index[0], description='User'), df=widgets.fixed(df), W=widgets.fixed(W), H=widgets.fixed(H))\n\n\n\n\n&lt;function __main__.predict_movie_ratings(user, df, W, H)&gt;\n\n\n\n# Image completion\nimport os\nif os.path.exists('dog.jpg'):\n    print('dog.jpg exists')\nelse:\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg\n\ndog.jpg exists\n\n\n\n# Read in a image from torchvision\nimport torchvision\nimg = torchvision.io.read_image(\"dog.jpg\")\nprint(img.shape)\n\ntorch.Size([3, 1365, 2048])\n\n\n\n# Make grayscale\nimg = torch.tensor(img, dtype=torch.float)\nimg = img.mean(dim=0, keepdim=False)\nprint(img.shape)\n\ntorch.Size([1365, 2048])\n\n\n/tmp/ipykernel_3941315/232810751.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img = torch.tensor(img, dtype=torch.float)\n\n\n\nimg.shape\n\ntorch.Size([1365, 2048])\n\n\n\nplt.imshow(img, cmap='gray')\n\n\n\n\n\n\n\n\n\ncrop = torchvision.transforms.functional.crop(img, 600, 800, 300, 300)\ncrop.shape\n\ntorch.Size([300, 300])\n\n\n\nplt.imshow(crop, cmap='gray')\n\n\n\n\n\n\n\n\n\n# Mask the image with NaN values \ndef mask_image(img, prop):\n    img_copy = img.clone()\n    mask = torch.rand(img.shape) &lt; prop\n    img_copy[mask] = float('nan')\n    return img_copy, mask\n\n\nmasked_img = mask_image(crop, 0.3)\n\n\nmasked_img[1].sum()\n\ntensor(27141)\n\n\n\nplt.imshow(masked_img[0], cmap='gray')\n\n\n\n\n\n\n\n\n\nW, H, loss = factorize(masked_img[0], 50, device=device)\n\n\nloss\n\ntensor(1323.5085, device='cuda:0', grad_fn=&lt;LinalgVectorNormBackward0&gt;)\n\n\n\nplt.imshow(torch.mm(W, H).cpu().detach().numpy(), cmap='gray')\n\n\n\n\n\n\n\n\n\ndef plot_image_completion(prop=0.1, factors=50):\n    masked_img, mask = mask_image(crop, prop)\n    W, H, loss = factorize(masked_img, factors, device=device)\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    reconstructed_img = torch.mm(W, H).cpu().detach().numpy()\n    ax[0].imshow(masked_img, cmap='gray')\n    ax[0].set_title(\"Masked image\")\n    ax[1].imshow(reconstructed_img, cmap='gray')\n    ax[1].set_title(\"Reconstructed image\")\n\n\ninteract(plot_image_completion, prop=widgets.FloatSlider(min=0.01, max=0.9, step=0.01, value=0.3), factors=widgets.IntSlider(min=1, max=150, step=1, value=50))\n\n\n\n\n&lt;function __main__.plot_image_completion(prop=0.1, factors=50)&gt;\n\n\n\n# Now use matrix faactaorization to predict the ratings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Create a class for the model\n\nclass MatrixFactorization(nn.Module):\n    def __init__(self, n_users, n_movies, n_factors=20):\n        super().__init__()\n        self.user_factors = nn.Embedding(n_users, n_factors)\n        self.movie_factors = nn.Embedding(n_movies, n_factors)\n\n    def forward(self, user, movie):\n        return (self.user_factors(user) * self.movie_factors(movie)).sum(1)      \n\n\nmodel = MatrixFactorization(n_users, n_movies, 2)\nmodel\n\nMatrixFactorization(\n  (user_factors): Embedding(100, 2)\n  (movie_factors): Embedding(10, 2)\n)\n\n\n\nmodel(torch.tensor([0]), torch.tensor([2]))\n\ntensor([-0.0271], grad_fn=&lt;SumBackward1&gt;)\n\n\n\nA[0, 2]\n\ntensor(5.)\n\n\n\ntype(A)\n\ntorch.Tensor\n\n\n\nmask = ~torch.isnan(A)\n\n# Get the indices of the non-NaN values\ni, j = torch.where(mask)\n\n# Get the values of the non-NaN values\nv = A[mask]\n\n# Store in PyTorch tensors\nusers = i.to(torch.int64)\nmovies = j.to(torch.int64)\nratings = v.to(torch.float32)\n\n\npd.DataFrame({'user': users, 'movie': movies, 'rating': ratings})\n\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\n\n\n\n\n0\n0\n1\n5.0\n\n\n1\n0\n2\n5.0\n\n\n2\n0\n4\n1.0\n\n\n3\n0\n5\n1.0\n\n\n4\n0\n6\n4.0\n\n\n...\n...\n...\n...\n\n\n512\n98\n8\n2.0\n\n\n513\n98\n9\n4.0\n\n\n514\n99\n0\n1.0\n\n\n515\n99\n4\n2.0\n\n\n516\n99\n6\n4.0\n\n\n\n\n517 rows  3 columns\n\n\n\n\n\n# Fit the Matrix Factorization model\nmodel = MatrixFactorization(n_users, n_movies, 4)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor i in range(1000):\n    # Compute the loss\n    pred = model(users, movies)\n    loss = F.mse_loss(pred, ratings)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 100 == 0:\n        print(loss.item())\n\n14.604362487792969\n4.332712650299072\n1.0960761308670044\n0.6966323852539062\n0.5388827919960022\n0.45243579149246216\n0.4012693464756012\n0.3728969395160675\n0.35568001866340637\n0.34289655089378357\n\n\n\nmodel(users, movies)\n\ntensor([3.5693, 4.5338, 2.6934, 1.8316, 4.8915, 2.0194, 2.7778, 1.8601, 2.1124,\n        1.1378, 2.9079, 5.0470, 0.9911, 1.9791, 1.0050, 3.9618, 2.0085, 2.0034,\n        4.0113, 1.9218, 2.9801, 1.0432, 3.8993, 4.1292, 1.9357, 3.8285, 3.5266,\n        1.3640, 2.3989, 2.4166, 0.8559, 3.3685, 4.4493, 3.4018, 1.4722, 4.8378,\n        4.6684, 4.4473, 4.3097, 2.0022, 5.0147, 5.0113, 1.9599, 2.8305, 1.4493,\n        1.6750, 0.9520, 3.8460, 5.1279, 4.7453, 2.1484, 1.8009, 3.2104, 4.2068,\n        4.5473, 2.9229, 1.1817, 3.1108, 3.2157, 1.2238, 3.6272, 3.9029, 3.2554,\n        0.9945, 3.0062, 5.0030, 4.1144, 1.6314, 3.7945, 3.3091, 4.0727, 3.6212,\n        2.4359, 3.5707, 1.2826, 0.9663, 4.9973, 3.0163, 2.9916, 1.0014, 3.1734,\n        3.8712, 4.3364, 3.7119, 4.5313, 2.3875, 4.0274, 4.7121, 4.3851, 2.8072,\n        3.2066, 3.9684, 0.9307, 1.4160, 2.7484, 2.8771, 1.2753, 2.5825, 4.9857,\n        2.0109, 1.0080, 2.2618, 2.7936, 2.5859, 3.0972, 3.1443, 3.8655, 3.2023,\n        2.0061, 1.9866, 4.0068, 2.7861, 4.2803, 4.5452, 3.8177, 2.9951, 5.6040,\n        1.9013, 2.4498, 1.9155, 4.3239, 3.2993, 1.1275, 4.1460, 4.3619, 4.5913,\n        1.1365, 3.0865, 6.0992, 3.9207, 1.8325, 3.8526, 2.0000, 2.0000, 1.0255,\n        5.0484, 1.9858, 1.9716, 2.0854, 2.7726, 3.5053, 1.7365, 3.2607, 4.7006,\n        1.8830, 0.3378, 2.6222, 0.6491, 3.1144, 3.8404, 2.0632, 4.3311, 4.6138,\n        1.6119, 3.0444, 2.4498, 2.7860, 4.0832, 3.2681, 5.0069, 4.8745, 3.6048,\n        3.5065, 1.8871, 2.9108, 0.9982, 0.6787, 1.4787, 5.6890, 1.5117, 4.0205,\n        4.6624, 4.1051, 4.5437, 3.5410, 1.8761, 4.6653, 5.2577, 5.0311, 4.9531,\n        4.0022, 4.7229, 4.0406, 2.5056, 3.2014, 2.9312, 1.3051, 1.3258, 4.9233,\n        2.0871, 1.8761, 4.1304, 3.9279, 4.5547, 1.6639, 4.3163, 1.8149, 4.7431,\n        3.4398, 3.5483, 3.2541, 2.8282, 1.5230, 2.8214, 4.9214, 3.5371, 4.4522,\n        4.7928, 2.4932, 4.8619, 4.7693, 2.3121, 2.3021, 4.6609, 4.7478, 1.7768,\n        5.3910, 2.8929, 2.4920, 4.1654, 1.8382, 3.4456, 3.7108, 2.3989, 1.0480,\n        4.2870, 5.2730, 3.8976, 3.2956, 3.1010, 0.9666, 3.0227, 2.9491, 3.2218,\n        1.4033, 1.7050, 0.5457, 3.1223, 2.1794, 3.9939, 5.0190, 2.9944, 4.5670,\n        5.4131, 2.4720, 4.0239, 4.5689, 0.9386, 1.1827, 2.1553, 1.4232, 0.4127,\n        1.7502, 1.5601, 2.0953, 2.3530, 2.7085, 2.7157, 2.7900, 4.6793, 1.1711,\n        3.4493, 1.7701, 2.1576, 0.8294, 1.9908, 2.1975, 3.6396, 3.1952, 4.9882,\n        4.0091, 2.9998, 1.9985, 3.0063, 3.7149, 4.2018, 1.1992, 2.2550, 3.5132,\n        1.3794, 2.4434, 2.0220, 0.6691, 1.9927, 3.4666, 2.8342, 1.0393, 4.1834,\n        4.0819, 0.8652, 4.8585, 2.0225, 2.9662, 3.9794, 1.0412, 3.0180, 1.0117,\n        3.1728, 2.6027, 2.5184, 2.5120, 3.3483, 3.0602, 3.2139, 1.9144, 1.4899,\n        2.8253, 2.9160, 1.6410, 3.8484, 2.9637, 2.1272, 2.0490, 4.4915, 1.6853,\n        5.0641, 3.9315, 1.8812, 2.1028, 2.1203, 2.8898, 4.0215, 3.3032, 3.6131,\n        4.8380, 0.7513, 3.4182, 3.0527, 3.6200, 1.3040, 1.1657, 4.2705, 4.8569,\n        2.7037, 0.9142, 1.5261, 3.3970, 0.6980, 3.1131, 3.8348, 3.4478, 4.9488,\n        4.0077, 4.9673, 2.0816, 0.9995, 3.4965, 3.2899, 1.0624, 1.4977, 4.3810,\n        4.2225, 3.1540, 4.0191, 3.4060, 2.0160, 1.2814, 2.9608, 1.1513, 1.9530,\n        0.9872, 3.9914, 4.9865, 3.0477, 1.9701, 2.5473, 4.6173, 0.5252, 3.4813,\n        4.7750, 3.5971, 4.1422, 5.2524, 2.0691, 2.0039, 2.1420, 5.0156, 1.8481,\n        0.7781, 2.2183, 2.6443, 2.2889, 4.3373, 2.2449, 4.5594, 3.8624, 5.2333,\n        2.1537, 4.7373, 5.0298, 2.1322, 2.7047, 4.2308, 2.4907, 2.3259, 4.7678,\n        5.0266, 5.2597, 4.7697, 0.9911, 1.0440, 2.7818, 1.1585, 2.1967, 4.4422,\n        3.4151, 4.8498, 2.8759, 4.0001, 1.8111, 2.3611, 1.9162, 4.3893, 2.4916,\n        2.0815, 3.9944, 5.5013, 4.6856, 4.9904, 4.9865, 4.0133, 1.7178, 2.2895,\n        1.9270, 2.6042, 3.5529, 2.0969, 4.9558, 2.9138, 3.0347, 1.6650, 2.2754,\n        3.7474, 0.9570, 2.3175, 4.1652, 4.4857, 5.1715, 5.4181, 3.7923, 4.0080,\n        3.5960, 2.1456, 2.9088, 3.2796, 1.2589, 4.6664, 2.3447, 3.6577, 2.8478,\n        2.8383, 2.9852, 0.9179, 3.0814, 1.2291, 0.7621, 0.8185, 2.3877, 1.1364,\n        2.8636, 3.2811, 4.5790, 0.7829, 1.2192, 0.8060, 1.2341, 3.9681, 1.1422,\n        4.9521, 5.1383, 2.8294, 3.6377, 3.8734, 3.5638, 2.9749, 1.5051, 1.4258,\n        0.5846, 1.0630, 2.7671, 1.7912, 2.8734, 1.8916, 3.9858, 2.5439, 2.6786,\n        1.6704, 1.3576, 1.7701, 2.0514, 5.1632, 2.6842, 4.8280, 1.1577, 2.9727,\n        3.3256, 2.7693, 2.6430, 4.0140, 2.2755, 2.1973, 4.6792, 5.3522, 2.8058,\n        4.9664, 5.2983, 4.7749, 3.8326, 2.4944, 4.2989, 2.8784, 2.0605, 2.8428,\n        3.1081, 1.0411, 1.9078, 3.9594], grad_fn=&lt;SumBackward1&gt;)\n\n\n\n# Now, let's predict the ratings for our df dataframe\n\nA = torch.from_numpy(df.values)\nA.shape\n\ntorch.Size([45, 10])\n\n\n\nmask = ~torch.isnan(A)\n\n# Get the indices of the non-NaN values\ni, j = torch.where(mask)\n\n# Get the values of the non-NaN values\nv = A[mask]\n\n# Store in PyTorch tensors\nusers = i.to(torch.int64)\nmovies = j.to(torch.int64)\nratings = v.to(torch.float32)\n\n\npd.DataFrame({'user': users, 'movie': movies, 'rating': ratings})\n\n\n\n\n\n\n\n\n\nuser\nmovie\nrating\n\n\n\n\n0\n0\n0\n4.0\n\n\n1\n0\n1\n5.0\n\n\n2\n0\n2\n4.0\n\n\n3\n0\n3\n4.0\n\n\n4\n0\n4\n5.0\n\n\n...\n...\n...\n...\n\n\n371\n44\n3\n4.0\n\n\n372\n44\n4\n4.0\n\n\n373\n44\n5\n4.0\n\n\n374\n44\n6\n4.0\n\n\n375\n44\n7\n5.0\n\n\n\n\n376 rows  3 columns\n\n\n\n\n\n# Fit the Matrix Factorization model\nn_users = A.shape[0]\nn_movies = A.shape[1]\nmodel = MatrixFactorization(n_users, n_movies, 4)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\nfor i in range(1000):\n    # Compute the loss\n    pred = model(users, movies)\n    loss = F.mse_loss(pred, ratings)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 100 == 0:\n        print(loss.item())\n\n19.889324188232422\n3.1148574352264404\n0.6727441549301147\n0.5543633103370667\n0.5081750750541687\n0.4629250764846802\n0.4147825837135315\n0.36878159642219543\n0.32987719774246216\n0.29975879192352295\n\n\n\n# Now, let us predict the ratings for any user and movie from df for which we already have the ratings\n\nusername = 'Dhruv'\nmovie = 'The Dark Knight'\n\n# Get the user and movie indices\nuser_idx = df.index.get_loc(username)\nmovie_idx = df.columns.get_loc(movie)\n\n# Predict the rating\npred = model(torch.tensor([user_idx]), torch.tensor([movie_idx]))\npred.item(), df.loc[username, movie]\n\n(5.259384632110596, 5.0)\n\n\n\ndf.loc[username]\n\nSholay                      NaN\nSwades (We The People)      NaN\nThe Matrix (I)              5.0\nInterstellar                5.0\nDangal                      3.0\nTaare Zameen Par            NaN\nShawshank Redemption        5.0\nThe Dark Knight             5.0\nNotting Hill                4.0\nUri: The Surgical Strike    5.0\nName: Dhruv, dtype: float64\n\n\n\n# Now, let us predict the ratings for any user and movie from df for which we do not have the ratings\n\nusername = 'Dhruv'\nmovie = 'Sholay'\n\n# Get the user and movie indices\nuser_idx = df.index.get_loc(username)\nmovie_idx = df.columns.get_loc(movie)\n\n# Predict the rating\npred = model(torch.tensor([user_idx]), torch.tensor([movie_idx]))\npred, df.loc[username, movie]\n\n(tensor([3.7885], grad_fn=&lt;SumBackward1&gt;), nan)\n\n\n\n# Complete the matrix\nwith torch.no_grad():\n    completed_matrix = pd.DataFrame(model.user_factors.weight @ model.movie_factors.weight.t(), index=df.index, columns=df.columns)\n    # round to nearest integer\n    completed_matrix = completed_matrix.round()\n\n\ncompleted_matrix.head()\n\n\n\n\n\n\n\n\n\nSholay\nSwades (We The People)\nThe Matrix (I)\nInterstellar\nDangal\nTaare Zameen Par\nShawshank Redemption\nThe Dark Knight\nNotting Hill\nUri: The Surgical Strike\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNipun\n5.0\n4.0\n4.0\n5.0\n4.0\n5.0\n5.0\n4.0\n4.0\n5.0\n\n\nGautam Vashishtha\n3.0\n3.0\n4.0\n4.0\n2.0\n3.0\n4.0\n5.0\n4.0\n3.0\n\n\nEshan Gujarathi\n4.0\n4.0\n5.0\n5.0\n4.0\n4.0\n5.0\n5.0\n4.0\n4.0\n\n\nSai Krishna Avula\n4.0\n4.0\n3.0\n4.0\n4.0\n6.0\n4.0\n3.0\n3.0\n4.0\n\n\nAnkit Yadav\n3.0\n2.0\n3.0\n4.0\n3.0\n5.0\n4.0\n3.0\n3.0\n4.0\n\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nSholay\nSwades (We The People)\nThe Matrix (I)\nInterstellar\nDangal\nTaare Zameen Par\nShawshank Redemption\nThe Dark Knight\nNotting Hill\nUri: The Surgical Strike\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNipun\n4.0\n5.0\n4.0\n4.0\n5.0\n5.0\n4.0\n5.0\n4.0\n5.0\n\n\nGautam Vashishtha\n3.0\n4.0\n4.0\n5.0\n3.0\n1.0\n5.0\n5.0\n4.0\n3.0\n\n\nEshan Gujarathi\n4.0\nNaN\n5.0\n5.0\n4.0\n5.0\n5.0\n5.0\nNaN\n4.0\n\n\nSai Krishna Avula\n5.0\n3.0\n3.0\n4.0\n4.0\n5.0\n5.0\n3.0\n3.0\n4.0\n\n\nAnkit Yadav\n3.0\n3.0\n2.0\n5.0\n2.0\n5.0\n5.0\n3.0\n3.0\n4.0"
  },
  {
    "objectID": "Lasso/Lasso_Regression.html",
    "href": "Lasso/Lasso_Regression.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom mpl_toolkits import mplot3d\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n%matplotlib inline\n# Based on: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\n\ny_true = 4*x + 7\nmax_deg = 20\n\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.savefig('true_function.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\ndef cost_lasso(theta_0, theta_1, x, y, lamb):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2 + lamb*(abs(theta_0) + abs(theta_1))\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\n\n\n#lambda  = 1000 cost curve tends to lasso objective. \nfig = plt.figure(figsize=(7,7))\nlamb_list = [10,100,1000]\nfor lamb in lamb_list:\n    lasso_cost_matrix = np.zeros_like(x_grid)\n    for i in range(x_grid.shape[0]):\n        for j in range(x_grid.shape[1]):\n            lasso_cost_matrix[i, j] = cost_lasso(x_grid[i, j], y_grid[i, j], data['x'], data['y'],lamb)\n\n\n    ax = plt.axes(projection='3d')\n    ax.plot_surface(x_grid, y_grid, lasso_cost_matrix,cmap='viridis', edgecolor='none')\n\n    ax.set_title('Least squares objective function');\n    ax.set_xlabel(r\"$\\theta_0$\")\n    ax.set_ylabel(r\"$\\theta_1$\")\n    ax.set_xlim([-4,15])\n    ax.set_ylim([-4,15])\n\n    u = np.linspace(0, np.pi, 30)\n    v = np.linspace(0, 2 * np.pi, 30)\n\n    # x = np.outer(500*np.sin(u), np.sin(v))\n    # y = np.outer(500*np.sin(u), np.cos(v))\n    # z = np.outer(500*np.cos(u), np.ones_like(v))\n    # ax.plot_wireframe(x, y, z)\n\n    ax.view_init(45, 120)\n    plt.savefig('lasso_lamb_{}_surface.pdf'.format(lamb), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef yy(p,soln):\n    xx = np.linspace(-soln,soln,100)\n    xx_final = []\n    yy_final = []\n    for x in xx:\n        if(x&gt;0):\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - x**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n            \n        else:\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - (-x)**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n    return xx_final, yy_final\n\n\nfig,ax = plt.subplots()\nax.fill(xx_final,yy_final)\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\nsoln = 3\np  = 0.5\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nxx = np.linspace(-soln,soln,100)\ny1  = yy1(xx,soln,p)\ny2  = yy2(xx,soln,p)\n\nx_final = np.hstack((xx,xx))\ny_final = np.hstack((y1,y2))\n\nxx_final, yy_final = yy(p,soln)\n\nplt.fill(xx_final,yy_final)\n\n\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n  import sys\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n  \"\"\"\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nplt.scatter(xx, y1, s=0.1,color='k',)\nplt.scatter(xx, y2, s=0.1,color='k',)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nregressor.coef_[0] + regressor.coef_[1]\n\n5.74196012460497\n\n\n\n## Function generator. \niterations = 60\np = 4\nq = 4\nalpha  = 0.1\n\nx = np.linspace(-5,5,1000)\ny1 = x**2\ny2 = abs(x)\n\n\nfor i in range(iterations):\n    fig,ax = plt.subplots(1,2)\n    ax[0].plot(x,y1)\n    ax[1].plot(x,y2)\n    prev = p\n    qrev = q\n    p = p - 2*alpha*p\n    q = q - alpha\n    val = p\n    \n    ax[0].arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    ax[1].arrow(qrev,abs(qrev),q - qrev ,abs(q) - abs(qrev),head_width = 0.5)\n    ax[0].scatter([prev],[prev**2],s=100)\n    ax[1].scatter([qrev],abs(qrev),s=100)\n    ax[0].set_xlabel(\"x\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_ylabel(\"Cost\")\n    ax[0].set_ylabel(\"Cost\")\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(0,5)\n    ax[1].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    ax[0].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    if(i==0):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    if(i%10==9):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1,y1= 5**0.5,5\ny2,x2 = 5,5\nx_shift = 0\ny_shift = -0.5\niterations = 11\nfor i in range(iterations):\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n   \n    ax[0].set_ylim(-1,10)\n    ax[0].set_xlim(-5,5)\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(-1,10)\n    ax[0].plot(x,x**2, color = 'blue')\n    ax[1].plot(x,abs(x),color = 'red')\n    ax[0].scatter(x1,y1,color = 'black')\n    \n    ax[0].annotate(str(round(y1,3)), (x1 + x_shift, y1+y_shift))\n    ax[1].annotate(str(y2), (x2 + x_shift, y2 + y_shift))\n    ax[1].scatter(x2,y2,color = 'black')\n    fig.suptitle('Iteration {}'.format(i))\n    if(iteratio)\n    plt.savefig('GD_Iteration_{}.pdf'.format(i))\n    \n    y1 = y1 - alpha*y1\n    y2 = y2 - 0.5\n    x2 = y2\n    x1 = y1**0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nfrom matplotlib.patches import Rectangle\n\n\nfor alpha in np.linspace(1,2,5):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Lasso(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    total = abs(regressor.coef_[0]) + abs(regressor.coef_[1])\n    p1 = Rectangle((-total, 0), np.sqrt(2)*total, np.sqrt(2)*total, angle = -45, alpha=0.6, color='g', label=r'$|\\theta_0|+|\\theta_1|={:.2f}$'.format(total))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Lasso Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n\n    plt.savefig('lasso_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nfrom sklearn.linear_model import Lasso\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e10]):\n        regressor = Lasso(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('lasso_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 659.2329891662157, tolerance: 2.566256097809531\n  positive)\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5017.444529811921, tolerance: 2.566256097809531\n  positive)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"dataset.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-5,1,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Lasso(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nplt.savefig('lasso_reg.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Question\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation Function (ACF):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating names using MLPs\n\n\n\n\n\nGenerating names\n\n\n\n\n\nMar 31, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Torch\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use logits\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSome practice problems\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSklearn on GPU\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nKalash Kankaria\n\n\n\n\n\n\n\n\n\n\n\n\nMovie Recommendation using Matrix Factorization\n\n\n\n\n\nMovie Recommendation using Matrix Factorization\n\n\n\n\n\nFeb 6, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression with Basis Functions\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nR Yeeshu Dhurandhar, Nipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Variables and Multi-collinearity\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nTaylors Series\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric interpretation of Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansion in Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar, Kalash Kankaria, Inderjeet Singh Bhullar\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansion in Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nContour and Surface Plots\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Discrete Input and Discrete Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSirens v/s Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nMeshgrid\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Feature Importance\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation of Ensemble Models\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparams Tuning Strategies Experimentation\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nWeighted Decision Trees\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nChirag Sarda\n\n\n\n\n\n\n\n\n\n\n\n\nBias Variance Tradeoff\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nNotion of Confusion in ML\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nR Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Sophisticated vs Dummy Baseline ML Algorithms for Imbalanced Datasets\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nTraditional Programming vs Machine Learning\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nR Yeeshu Dhurandhar, Rahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Real Input and Discrete Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Real Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Entropy\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Diffusion to Generate Images from Text\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - I\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Basis\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Iris dataset\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Cost Function\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nObject detection using YOLO and segmentation using Segment Anything\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron learning algorithm\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees [Real I/P Real O/P, Bias vs Variance]\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nNipun Batra, Rahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Tuning\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy Pandas Basics\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nR Yeeshu Dhurandhar, Nipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Classes and Plotting Trees\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nAnscombes Quartet\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nCross Validation Diagrams\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube video to transcript using openAI whisper and summary using OLLama\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nAutodiff\n\n\n\n\n\nAutodiff Helper\n\n\n\n\n\nApr 4, 2023\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "knn/knn/curse_dimensionality.html",
    "href": "knn/knn/curse_dimensionality.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n%matplotlib inline\n\n\nnp.random.seed(0)\n\n\nnum_points = 10\n\n\ndata = {}\nfor num_dimensions in range(1, 20):\n    data[num_dimensions] = np.random.uniform(low=0.0, high=1.0, size=(num_points, num_dimensions))\n\n\nplt.scatter(data[1], np.zeros_like(data[1]))\n\n\n\n\n\n\n\n\n\nplt.scatter(data[2][:, 0], data[2][:, 1])\n\n\n\n\n\n\n\n\n\ndist = {}\nfor num_dimensions in range(1, 20):\n    dist[num_dimensions] = pd.DataFrame(euclidean_distances(data[num_dimensions], data[num_dimensions]))\n\n\nmean_distances = pd.Series({num_dimensions: dist[num_dimensions].mean().mean() for num_dimensions in range(1, 20)})\n\n\nmean_distances.plot(style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Mean distance between two points\")\nplt.savefig('curse_dist.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nratio_max_min = pd.Series({num_dimensions:(dist[num_dimensions].replace({0:np.NAN}).max()/dist[num_dimensions].replace({0:np.NAN}).min()).mean() \n                           for num_dimensions in range(1, 20) })\n\n\nratio_max_min.plot(logy=True, style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Ratio of max to min distances\")\nplt.ylim((-1, 180))\nplt.savefig('curse_spread.pdf', transparent=True, bbox_inches=\"tight\")\n\n/home/prof/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Attempted to set non-positive bottom ylim on a log-scaled axis.\nInvalid limit will be ignored.\n  after removing the cwd from sys.path."
  },
  {
    "objectID": "cnn/tensor-factorisation.html",
    "href": "cnn/tensor-factorisation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import tensorly as tl\n\nUsing numpy backend.\n\n\n\nimport cvxpy as cp\nimport numpy as np\n\n# Ensure repeatably random problem data.\nnp.random.seed(0)\n\n# Generate random data matrix A.\nm = 10\nn = 10\no = 5\nk = 2\nD = 10*np.ones((m, n, o)) + np.random.randn(m, n, o)\n\n# Initialize Y randomly.\nA_init = 10*np.ones((m, k))\nB_init = 10*np.ones((n, k))\nC_init = 10*np.ones((o, k))\n\n\n\n\nPred_A = np.einsum('ir, jr, kr -&gt;ijk', A_init, B_init, C_init)\n\n\n# Ensure same initial random Y, rather than generate new one\n# when executing this cell.\nB = B_init\nC = C_init\n\n# Perform alternating minimization.\nMAX_ITERS = 100\nresidual = np.zeros(MAX_ITERS)\nfor iter_num in range(0, 1+MAX_ITERS):\n\n    if iter_num % 3 == 0:\n        A = cp.Variable(shape=(n, k))\n        constraint = [A &gt;= 0]\n        prediction = A@tl.tenalg.khatri_rao([C, B]).T\n    elif iter_num % 3 == 1:\n        B = cp.Variable(shape=(m, k))\n        constraint = [B &gt;= 0]\n        prediction = B@tl.tenalg.khatri_rao([A, C]).T\n    elif iter_num % 3 == 2:\n        C = cp.Variable(shape=(o, k))\n        constraint = [C &gt;= 0]\n        prediction = C@tl.tenalg.khatri_rao([B, A]).T\n\n    obj = cp.Minimize(cp.norm(D.reshape(prediction.shape) - prediction, 'fro')/D.size)\n    prob = cp.Problem(obj, constraint)\n    prob.solve(solver=cp.SCS, max_iters=10000)\n\n    if prob.status != cp.OPTIMAL:\n        raise Exception(\"Solver did not converge!\")\n\n    print('Iteration {}, residual norm {}'.format(iter_num, prob.value))\n    residual[iter_num-1] = prob.value\n\n    # Convert variable to NumPy array constant for next iteration.\n    if iter_num % 3 == 0:\n        A = A.value\n    elif iter_num%3 == 1:\n        B = B.value\n    else:\n        C = C.value\n\nIteration 0, residual norm 0.044229038768601577\nIteration 1, residual norm 0.04438975125966638\nIteration 2, residual norm 0.04485089174072711\nIteration 3, residual norm 0.0446730384004453\nIteration 4, residual norm 0.044526862069177754\nIteration 5, residual norm 0.04484264445045543\nIteration 6, residual norm 0.044478708695822676\nIteration 7, residual norm 0.04447482085818169\nIteration 8, residual norm 0.045090403949033964\nIteration 9, residual norm 0.04454108258260972\nIteration 10, residual norm 0.04409564845828368\nIteration 11, residual norm 0.04513476609369982\nIteration 12, residual norm 0.04452491993393663\nIteration 13, residual norm 0.04378836637021587\nIteration 14, residual norm 0.045190029913906464\nIteration 15, residual norm 0.04437455947694065\nIteration 16, residual norm 0.04376389953499031\nIteration 17, residual norm 0.04474118548369649\nIteration 18, residual norm 0.04485265629997688\nIteration 19, residual norm 0.04423721109971314\nIteration 20, residual norm 0.04463836313028598\nIteration 21, residual norm 0.04509480034427943\nIteration 22, residual norm 0.04449824189344165\nIteration 23, residual norm 0.0446297404908044\nIteration 24, residual norm 0.044916791718091605\nIteration 25, residual norm 0.04475629136811872\nIteration 26, residual norm 0.044530205009593614\nIteration 27, residual norm 0.04481551774462266\nIteration 28, residual norm 0.04450448414700286\nIteration 29, residual norm 0.04473161074066948\nIteration 30, residual norm 0.044760915844488124\nIteration 31, residual norm 0.04447831568798033\nIteration 32, residual norm 0.04458463344810684\nIteration 33, residual norm 0.04486246216584251\nIteration 34, residual norm 0.04441553594150665\nIteration 35, residual norm 0.04465894246345131\nIteration 36, residual norm 0.04504337082596328\nIteration 37, residual norm 0.044526299849176346\nIteration 38, residual norm 0.04482460772221398\nIteration 39, residual norm 0.04500800584243032\nIteration 40, residual norm 0.04450100433007423\nIteration 41, residual norm 0.04484822280377619\nIteration 42, residual norm 0.04482311584413178\nIteration 43, residual norm 0.044522803791676675\nIteration 44, residual norm 0.044772484551616504\nIteration 45, residual norm 0.04458696318356589\nIteration 46, residual norm 0.044520256516783666\nIteration 47, residual norm 0.04482402095966313\nIteration 48, residual norm 0.04426750617419428\nIteration 49, residual norm 0.044306760469399145\nIteration 50, residual norm 0.04488415609564971\nIteration 51, residual norm 0.04462703244119978\nIteration 52, residual norm 0.04393438972821902\nIteration 53, residual norm 0.04496061133340096\nIteration 54, residual norm 0.04474813896252984\nIteration 55, residual norm 0.04412129577027855\nIteration 56, residual norm 0.04476074674202858\nIteration 57, residual norm 0.04493986609489073\nIteration 58, residual norm 0.04433894515459506\nIteration 59, residual norm 0.04479359793862809\nIteration 60, residual norm 0.04507730499950036\nIteration 61, residual norm 0.04443726027910054\nIteration 62, residual norm 0.04482400811580273\nIteration 63, residual norm 0.044904850352243696\nIteration 64, residual norm 0.04455396547559336\nIteration 65, residual norm 0.04468608722592516\nIteration 66, residual norm 0.04458327810705387\nIteration 67, residual norm 0.0445511237555968\nIteration 68, residual norm 0.044703743776719096\nIteration 69, residual norm 0.04439204969221895\nIteration 70, residual norm 0.04419479288919463\nIteration 71, residual norm 0.04496926843613955\nIteration 72, residual norm 0.04456014579787714\nIteration 73, residual norm 0.04390674892508923\nIteration 74, residual norm 0.04471211816232015\nIteration 75, residual norm 0.04477670162347586\nIteration 76, residual norm 0.044259574198538376\nIteration 77, residual norm 0.044866509022554346\nIteration 78, residual norm 0.04499103987882674\nIteration 79, residual norm 0.04443576055416042\nIteration 80, residual norm 0.04494346910320409\nIteration 81, residual norm 0.04480069447165729\nIteration 82, residual norm 0.044256568121726014\nIteration 83, residual norm 0.044744496252969\nIteration 84, residual norm 0.04448314930970543\nIteration 85, residual norm 0.044155459981671225\nIteration 86, residual norm 0.04521780706935539\nIteration 87, residual norm 0.04446147094598418\nIteration 88, residual norm 0.04404940602099275\nIteration 89, residual norm 0.04516034088743775\nIteration 90, residual norm 0.04457572511910495\nIteration 91, residual norm 0.044197230653923565\nIteration 92, residual norm 0.045118201306481545\nIteration 93, residual norm 0.04421183315228502\nIteration 94, residual norm 0.04375429174860549\nIteration 95, residual norm 0.04441022241202105\nIteration 96, residual norm 0.04401508759046129\nIteration 97, residual norm 0.04393321896924147\nIteration 98, residual norm 0.04435341885307329\nIteration 99, residual norm 0.04411809866902199\nIteration 100, residual norm 0.044273830016112556\n\n\n\nA\n\narray([[ 1.23599056e-01,  4.95799879e-02],\n       [-6.88417340e-12,  4.89807637e-02],\n       [ 1.97771012e-01,  5.00354889e-02],\n       [ 2.97660174e-11,  4.86588823e-02],\n       [ 1.16488475e-01,  4.82973254e-02],\n       [-1.07464945e-11,  4.91216434e-02],\n       [ 2.41321912e-11,  4.81319502e-02],\n       [ 1.23285226e-01,  4.79485790e-02],\n       [ 8.32457971e-12,  4.89433882e-02],\n       [ 1.25287913e-11,  4.91104662e-02]])\n\n\n\nB\n\narray([[ 9.47630862e+00,  1.31432989e+01],\n       [ 2.21280935e+00,  1.29931743e+01],\n       [ 5.84494148e-10,  1.33764365e+01],\n       [ 1.08369019e+01,  1.28286679e+01],\n       [ 5.85242755e+00,  1.28260099e+01],\n       [-2.53650233e-09,  1.30457870e+01],\n       [ 2.49310302e+00,  1.27637902e+01],\n       [-9.13914054e-10,  1.27890647e+01],\n       [ 1.19440017e+00,  1.29894214e+01],\n       [-5.49265493e-10,  1.30485092e+01]])\n\n\n\nC\n\narray([[ 1.52713520e-01,  1.58149369e+01],\n       [ 4.52202894e-01,  1.58153740e+01],\n       [ 5.73957401e-10,  1.56441552e+01],\n       [-1.73226998e-12,  1.54265628e+01],\n       [-1.68743313e-10,  1.57148110e+01]])\n\n\n\nnp.einsum('ir, jr, kr -&gt;ijk', A, B, C)\n\narray([[[10.48458589, 10.83565147, 10.19442928, 10.05263637,\n         10.24047173],\n        [10.22977218, 10.31196443, 10.07798707,  9.93781374,\n         10.12350361],\n        [10.48852243, 10.4888123 , 10.37525943, 10.23095138,\n         10.42211858],\n        [10.26356388, 10.66498719,  9.95038983,  9.81199123,\n          9.99533009],\n        [10.16739655, 10.38431148,  9.94832818,  9.80995826,\n          9.99325913],\n        [10.22925871, 10.22954142, 10.11879543,  9.97805451,\n         10.16449628],\n        [10.05520169, 10.1477645 ,  9.90006825,  9.76236957,\n          9.94478124],\n        [10.02796162, 10.02823877,  9.9196721 ,  9.78170076,\n          9.96447363],\n        [10.20760682, 10.25210095, 10.07507622,  9.93494338,\n         10.12057961],\n        [10.2313932 , 10.23167597, 10.12090687,  9.98013658,\n         10.16661726]],\n\n       [[10.18116321, 10.18144459, 10.07121931,  9.93114011,\n         10.11670529],\n        [10.06487253, 10.0651507 ,  9.95618442,  9.81770523,\n         10.00115085],\n        [10.36175805, 10.36204442, 10.24986394, 10.1073    ,\n         10.29615676],\n        [ 9.93744133,  9.93771598,  9.83012932,  9.69340341,\n          9.87452643],\n        [ 9.93538237,  9.93565696,  9.82809259,  9.69139501,\n          9.87248051],\n        [10.10562779, 10.10590708,  9.99649957,  9.85745964,\n         10.04164808],\n        [ 9.88718524,  9.88745849,  9.78041593,  9.64438148,\n          9.82458851],\n        [ 9.90676358,  9.90703738,  9.79978285,  9.66347903,\n          9.84404291],\n        [10.06196547, 10.06224355,  9.95330875,  9.81486955,\n          9.99826219],\n        [10.10773648, 10.10801583,  9.99858549,  9.85951655,\n         10.04374343]],\n\n       [[10.68660524, 11.24817743, 10.28808747, 10.14499189,\n         10.33455292],\n        [10.34843597, 10.47978553, 10.17057548, 10.02911436,\n         10.2165102 ],\n        [10.58488251, 10.58517505, 10.47057894, 10.32494511,\n         10.5178686 ],\n        [10.47872824, 11.12088193, 10.04180598,  9.90213589,\n         10.08715912],\n        [10.32608231, 10.67300395, 10.0397254 ,  9.90008425,\n         10.08506914],\n        [10.32323689, 10.32352219, 10.21175876, 10.06972482,\n         10.25787948],\n        [10.17538798, 10.3233344 ,  9.99102209,  9.85205835,\n         10.03614586],\n        [10.12009045, 10.12037014, 10.01080605,  9.87156714,\n         10.05601918],\n        [10.314708  , 10.38573678, 10.16763789, 10.02621762,\n         10.21355934],\n        [10.32539099, 10.32567635, 10.2138896 , 10.07182603,\n         10.26001994]],\n\n       [[10.11425681, 10.11453634, 10.00503541,  9.86587676,\n         10.05022247],\n        [ 9.99873034,  9.99900668,  9.89075648,  9.75318732,\n          9.93542741],\n        [10.29366485, 10.29394934, 10.18250606, 10.040879  ,\n         10.22849466],\n        [ 9.87213657,  9.87240941,  9.76552976,  9.62970236,\n          9.80963512],\n        [ 9.87009114,  9.87036392,  9.76350642,  9.62770716,\n          9.80760264],\n        [10.03921777, 10.03949523,  9.9308067 ,  9.79268048,\n          9.97565852],\n        [ 9.82221073,  9.82248219,  9.71614307,  9.58100258,\n          9.76002537],\n        [ 9.84166042,  9.84193242,  9.73538272,  9.59997463,\n          9.77935192],\n        [ 9.99584238,  9.99611864,  9.88789971,  9.75037028,\n          9.93255774],\n        [10.04131261, 10.04159012,  9.93287891,  9.79472388,\n          9.97774009]],\n\n       [[10.20768094, 10.53855895,  9.93069359,  9.79256895,\n          9.97554489],\n        [ 9.96379984, 10.04127254,  9.8172638 ,  9.68071684,\n          9.86160281],\n        [10.21717838, 10.21746076, 10.10684556,  9.96627084,\n         10.15249244],\n        [ 9.99156385, 10.36990232,  9.69296758,  9.55814943,\n          9.73674521],\n        [ 9.90086298, 10.10530773,  9.69095927,  9.55616906,\n          9.73472783],\n        [ 9.96462196,  9.96489736,  9.85701643,  9.71991656,\n          9.90153498],\n        [ 9.7935781 ,  9.88082458,  9.64394785,  9.50981151,\n          9.68750408],\n        [ 9.76853255,  9.76880252,  9.66304454,  9.52864259,\n          9.70668703],\n        [ 9.94281649,  9.98475981,  9.81442826,  9.67792073,\n          9.85875446],\n        [ 9.96670123,  9.96697669,  9.85907325,  9.72194476,\n          9.90360109]],\n\n       [[10.21044653, 10.21072872, 10.1001864 ,  9.95970431,\n         10.14580321],\n        [10.09382137, 10.09410034,  9.98482065,  9.84594316,\n         10.02991642],\n        [10.3915608 , 10.39184799, 10.27934486, 10.13637088,\n         10.32577082],\n        [ 9.96602365,  9.96629909,  9.85840299,  9.72128382,\n          9.9029278 ],\n        [ 9.96395877,  9.96423415,  9.8563604 ,  9.71926965,\n          9.90087599],\n        [10.13469385, 10.13497395, 10.02525176,  9.88581192,\n         10.07053013],\n        [ 9.91562301,  9.91589705,  9.80854661,  9.67212089,\n          9.85284625],\n        [ 9.93525767,  9.93553225,  9.82796924,  9.69127337,\n          9.87235659],\n        [10.09090595, 10.09118483,  9.98193671,  9.84309933,\n         10.02701945],\n        [10.13680861, 10.13708877, 10.02734368,  9.88787475,\n         10.0726315 ]],\n\n       [[10.00472845, 10.00500495,  9.89668982,  9.75903813,\n          9.94138755],\n        [ 9.89045303,  9.89072638,  9.78364843,  9.64756902,\n          9.82783562],\n        [10.18219366, 10.18247507, 10.07223863,  9.93214525,\n         10.11772921],\n        [ 9.76523015,  9.76550004,  9.65977781,  9.5254213 ,\n          9.70340554],\n        [ 9.76320687,  9.7634767 ,  9.65777638,  9.5234477 ,\n          9.70139507],\n        [ 9.93050202,  9.93077647,  9.82326494,  9.68663451,\n          9.86763105],\n        [ 9.71584497,  9.71611349,  9.61092593,  9.47724889,\n          9.65433302],\n        [ 9.73508404,  9.73535309,  9.62995723,  9.49601549,\n          9.67345028],\n        [ 9.88759634,  9.88786961,  9.78082259,  9.64478249,\n          9.82499702],\n        [ 9.93257417,  9.93284868,  9.82531472,  9.68865578,\n          9.86969008]],\n\n       [[10.14502635, 10.4951919 ,  9.85898582,  9.72185855,\n          9.90351326],\n        [ 9.89443406,  9.97640907,  9.74637509,  9.61081411,\n          9.79039394],\n        [10.14340196, 10.1436823 , 10.03386583,  9.89430618,\n         10.0791831 ],\n        [ 9.93205681, 10.33245243,  9.62297639,  9.48913174,\n          9.66643791],\n        [ 9.83619694, 10.05255267,  9.62098258,  9.48716567,\n          9.6644351 ],\n        [ 9.8926692 ,  9.89294261,  9.78584067,  9.64973077,\n          9.83003776],\n        [ 9.7257684 ,  9.81808778,  9.57431062,  9.44114286,\n          9.61755234],\n        [ 9.69799572,  9.69826374,  9.59326942,  9.45983796,\n          9.63659677],\n        [ 9.87241434,  9.91678695,  9.74356002,  9.60803819,\n          9.78756615],\n        [ 9.89473346,  9.89500692,  9.78788264,  9.65174433,\n          9.83208895]],\n\n       [[10.17339432, 10.17367549, 10.06353431,  9.92356201,\n         10.10898558],\n        [10.05719238, 10.05747033,  9.9485872 ,  9.81021368,\n          9.99351932],\n        [10.35385135, 10.3541375 , 10.24204263, 10.09958747,\n         10.28830012],\n        [ 9.92985842,  9.93013285,  9.82262829,  9.68600671,\n          9.86699153],\n        [ 9.92780103,  9.92807541,  9.82059312,  9.68399985,\n          9.86494716],\n        [10.09791654, 10.09819562,  9.98887159,  9.84993776,\n         10.03398565],\n        [ 9.87964067,  9.87991372,  9.77295283,  9.63702219,\n          9.81709171],\n        [ 9.89920408,  9.89947766,  9.79230498,  9.65610517,\n          9.83653126],\n        [10.05428753, 10.0545654 ,  9.94571372,  9.80738017,\n          9.99063287],\n        [10.10002362, 10.10030276,  9.99095592,  9.8519931 ,\n         10.0360794 ]],\n\n       [[10.20812322, 10.20840535, 10.09788818,  9.95743805,\n         10.14349461],\n        [10.0915246 , 10.0918035 ,  9.98254868,  9.84370279,\n         10.02763418],\n        [10.38919627, 10.3894834 , 10.27700587, 10.13406442,\n         10.32342127],\n        [ 9.96375596,  9.96403133,  9.85615978,  9.71907182,\n          9.90067446],\n        [ 9.96169154,  9.96196686,  9.85411766,  9.7170581 ,\n          9.89862311],\n        [10.13238778, 10.13266781, 10.02297059,  9.88356248,\n         10.06823865],\n        [ 9.91336678,  9.91364076,  9.80631475,  9.66992007,\n          9.8506043 ],\n        [ 9.93299697,  9.93327149,  9.82573295,  9.68906819,\n          9.87011021],\n        [10.08860984, 10.08888866,  9.97966539,  9.84085961,\n         10.02473787],\n        [10.13450206, 10.13478215, 10.02506203,  9.88562483,\n         10.07033954]]])"
  },
  {
    "objectID": "cnn/lenet.html",
    "href": "cnn/lenet.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel = keras.Sequential()\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(units=120, activation='relu'))\nmodel.add(layers.Dense(units=84, activation='relu'))\nmodel.add(layers.Dense(units=10, activation = 'softmax'))\n\n\nmodel\n\n&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x7f96fa3cd4e0&gt;\n\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n\n\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n\n\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "cnn/convolution-operation-stride.html",
    "href": "cnn/convolution-operation-stride.html",
    "title": "CIFAR",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_animation(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n    ani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                             frames=zip(i.flat, j.flat), interval=frate)\n    ani.save(fname, writer=\"imagemagick\")\n\n\ncreate_animation(a, kernel, p, s, 'demo.gif', 400)\n\n\n\n\n\n\n\n\n\nfrom keras.datasets import mnist\n\nUsing TensorFlow backend.\n\n\n\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\ncreate_animation(x_train[0], kernel, 0, 1, 'mnist.gif', 2, (20, 4))\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_static(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n     \n    frames=zip(i.flat, j.flat)\n    animate(frames)\n    fig.savefig(fname)\n\n\nfrom keras import backend as K\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel_vertical_edge = keras.Sequential()\nmodel_vertical_edge.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='linear', input_shape=(28, 28, 1)))\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_vertical_edge.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-linear.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[2:3]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-4.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_conv.T.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[0:1]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-5.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[5:6]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-2.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[6:7]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-1.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nfrom keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 91s 1us/step\n\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nmodel_horizontal_edge_relu.layers[0].get_weights()[0].shape\n\n(3, 3, 3, 1)\n\n\n\nfilter_3d_horizontal = np.empty((3, 3, 3))\nfilter_3d_horizontal[:] = filter_conv.T\n\n\nfilter_3d_horizontal\n\narray([[[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]]])\n\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_horizontal.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow(x_train[4])\nplt.title(y_train[4])\nplt.savefig(\"cifar-10-car.pdf\", transparent=True)\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 0], cmap='Reds')\nplt.savefig(\"cifar-10-car-red.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 1], cmap='Greens')\nplt.savefig(\"cifar-10-car-green.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 2], cmap='Blues')\nplt.savefig(\"cifar-10-car-blue.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n\n\n\n\n\n\n\n\nx_train.shape[1:]\n\n(32, 32, 3)\n\n\n\nmodel_horizontal_edge_relu.predict(x_train[4:5]).shape\n\n(1, 30, 30, 1)\n\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nfilter_3d_vertical = np.empty((3, 3, 3))\nfilter_3d_vertical[:] = filter_conv\nfilter_3d_vertical = filter_3d_vertical\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_vertical.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow((filter_3d_horizontal+1)/2)\n\n\n\n\n\n\n\n\n\nplt.imshow(((filter_3d_vertical+1)/2).T)\n\n\n\n\n\n\n\n\n\nfilter_3d_vertical\n\narray([[[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]]])\n\n\n\n(filter_3d_vertical+1)/2\n\narray([[[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]]])\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n\n\n\n\n\n\n\n\nmodel_vertical_edge_relu.layers[0].get_weights()[0][0].shape\n\n(3, 3, 1)\n\n\n\nimport scipy\nimg = x_train[6:7].reshape(32, 32, 3)\nfrom skimage import color\nimg = color.rgb2gray(img)\nsharpen_kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\nimage_sharpen = scipy.signal.convolve2d(img, sharpen_kernel, 'valid')\n\n\nfrom skimage import io\n\n\nbeach = io.imread(\"beach.jpg\")\n\n\nbeach.shape\n\n(1704, 2272, 3)\n\n\n\nbuildings = io.imread(\"buildings.jpg\")\n\n\nbuildings.shape\n\n(1704, 2272, 3)\n\n\n\nplt.imshow(beach)\nplt.axis('OFF')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 0], cmap='Reds')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 1], cmap='Greens')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 2], cmap='Blues')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_red = scipy.signal.convolve2d(beach[:, :, 0], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_red, cmap='Greys')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_green = scipy.signal.convolve2d(beach[:, :, 1], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_green, cmap='Greens')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_blue = scipy.signal.convolve2d(beach[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_blue, cmap='Blues')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nimage_out_buildings_blue = scipy.signal.convolve2d(buildings[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_buildings_blue, cmap='Greys')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nhorizontal_kernel = vertical_kernel.T\nhorizontal_kernel\n\narray([[ 1,  1,  1],\n       [ 0,  0,  0],\n       [-1, -1, -1]])\n\n\n\nimage_out_buildings_blue_horizontal = scipy.signal.convolve2d(buildings[:, :, 2], horizontal_kernel, 'valid')\nplt.imshow(image_out_buildings_blue_horizontal, cmap='Greys')\nplt.axis('off')"
  },
  {
    "objectID": "gradient-descent/Gradient Descent.html",
    "href": "gradient-descent/Gradient Descent.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "#import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nsns.despine()\n\nNameError: name 'sns' is not defined\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\nplt.plot(x,y)\nplt.title(\"Cost Function\")\n\nText(0.5, 1.0, 'Cost Function')\n\n\n\n\n\n\n\n\n\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This webpage contains the course materials for the course Machine Learning that I (Nipun Batra) teaches at Indian Institute of Technology, Gandhinagar. These materials have been developed over several years by me and excellent teaching assistants who have helped me in teaching this course."
  }
]