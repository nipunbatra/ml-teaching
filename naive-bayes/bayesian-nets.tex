\documentclass{beamer}
\usepackage{tcolorbox}

%\beamerdefaultoverlayspecification{<+->}
\newcommand{\data}{\mathcal{D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand\Item[1][]{%
	\ifx\relax#1\relax  \item \else \item[#1] \fi
	\abovedisplayskip=0pt\abovedisplayshortskip=0pt~\vspace*{-\baselineskip}}


\usetheme{metropolis}           % Use metropolis theme


\title{Naive Bayes}
\date{\today}
\author{Nipun Batra}
\institute{IIT Gandhinagar}
\begin{document}
  \maketitle
  
  
  
% \section{Linear Regression}

\begin{frame}{Bayesian Networks}
\begin{itemize}
	
	
	\item Nodes are random variables.
	\item Edges denote direct impact

	
\end{itemize}
\end{frame}

\begin{frame}{Example}
\begin{itemize}

\item Grass can be wet due to multiple reasons:
\begin{itemize}
    \item Rain
    \item Sprinkler
\end{itemize}
\item Also, if it rains, then sprinkler need not be used.
\end{itemize}

    


	

\end{frame}

\begin{frame}{bayesian Nets}
    $P(X_{1},X_{2},X_{3},\dots,X_{N})$ denotes the joint probability, where $X_{i}$ are random variables.
    \begin{equation*}
        P(X_{1},X_{2},X_{3},\dots,X_{N}) = \Pi_{k=1}^{N} P(X_{k} | parents(X_{k}))
    \end{equation*}
    
    
    \begin{equation*}
        P(S,G,R) =  P(G|S,R)P(S|R)P(R)
    \end{equation*}
    
\end{frame}


\begin{frame}{Bayesian Networks}
    % $$
    %     P(Grass = Wet, Rain  = True, Sprinkler = True) =
    %     \\P(G = W| S = T,R = T)P(S = T | R = T)P(R = T)
    % $$
\end{frame}


\begin{frame}{Example}
    Known Random variables
    \begin{itemize}
        \item $P(T)$
        \item $P(E)$
        \item $P(A|T,E)$
        \item $P(R|E)$
    \end{itemize}
\end{frame}


\begin{frame}{Question}
    Given, the above, calculate
    \begin{equation*}
        P(A|T)
    \end{equation*}
\end{frame}

\begin{frame}{Solution}
    \begin{align*}
        \begin{split}
            P(A|T) &= \frac{P(A,T)}{P(A)}\\
            &=\frac{(A,T,E)+P(A,T,\bar{E})}{(A,T,E)+P(A,T,\bar{E}) + (A,\bar{T},E)+P(A,\bar{T},\bar{E})}
        \end{split}
    \end{align*}
\end{frame}

\begin{frame}{Medical Diagnosis}
    You tested positive for a disease.\\
    Well, the test is only 99\% accurate.\\
    \begin{itemize}
        \item $P(Test = +ve | Disease = True) = 0.99$
        \item $P(Test = -ve | Disease = False) = 0.99$
    \end{itemize}
    Also, the disease is a rare one. Only one in 10,000 has it.
\end{frame}

\begin{frame}{Problem}
    \begin{itemize}
        \item $P(T|D)$ = 0.99
        \item $P(\bar{T}|\bar{D})$ = 0.99
        \item $P(T|\bar{D})$ = 0.01
        \item $P(\bar{T}|D)$ = 0.01
        \item $P(D)$ =$ 10^{-4}$
        \item $P(\bar{D})$ =$ 1 - 10^{-4}$
    \end{itemize}
    
    Given the above, calculate $P(D|T)$. Given the result is positive, what is the probability that someone has the disease
\end{frame}

\begin{frame}{Problem}
    \begin{align}
    \label{eqn*:eqlabel}
        \begin{split}
            P(D|T) &= \frac{P(T|D)P(D)}{P(T)}\\
            &=\frac{P(T|D)P(D)}{P(T|D)P(D) + P(T|\bar{D})P(\bar{D})}
        \end{split}
    \end{align}


    % \begin{align}
    % \label{eqn*:eqlabel}
    % \begin{split}
    
            %     P(D|T) &= \frac{P(T|D)P(D)}{P(T)}\\
            % &=\frac{P(T|D)P(D)}{P(T|D)P(D) + P(T|\bar{D})P(\bar{D})}
            
            
            
            
    %   \epsilon &= y - X\theta \\
    % \epsilon^{T} &= (y-X\theta)^{T} = y^{T} - \theta^{T}X^{T}\\
    % \epsilon^{T}\epsilon &= (y^{T} - \theta^{T}X^{T})(y - X\theta)\\
    % &=y^{T}y - \theta^{T}X^{T}y - y^{T}X\theta+\theta^{T}X^{T}X\theta\\
    % &=y^{T}y - 2y^{T}X\theta+\theta^{T}X^{T}X\theta
    % \end{split}
    % \end{align}
    

    % \begin{align}
    %     \begin{split}
            % P(D|T) &= \frac{P(T|D)P(D)}{P(T)}\\
            % &=\frac{P(T|D)P(D)}{P(T|D)P(D) + P(T|\bar{D})P(\bar{D})}
            
            
            
    %     \end{split}
    % \end{align}
\end{frame}

\begin{frame}{SPAM EMAIL CLASSIFICATION}
    From the emails construct a vector $X$. The vector has ones if the word is present, and zeros is the word is absent\\
\end{frame}

\begin{frame}{Naive Bayes}
    \begin{itemize}
        \item Classification model
        \item Scalable
        \item Generative
    \end{itemize}
    
    \begin{equation*}
        P(x_{1},x_{2},x_{3},\dots,x_{N} \vert y) = P(x_{1}|y) P(x_{2}|y) \dots P(x_{N}|y)
    \end{equation*}
\end{frame}

\begin{frame}{Quick Question}
    Why is Naive Bayes model called Naive? 
\end{frame}


\begin{frame}{Frame Title}
    It assumes that the features are independent during modelling, which is generally not the case.
\end{frame}

\begin{frame}{What do we need to predict?}
    \begin{equation*}
        P(y|x_{1},x_{2},\dots,x_{N}) =  \frac{P(x_{1},x_{2},\dots,x_{N}|y)P(y)}{P(x_{1},x_{2},\dots,x_{N})}
    \end{equation*}
\end{frame}


\begin{frame}{Spam Mail Classification}
    Probability of $x_{i}$ being a spam email\\
    $$
    P(x_{i} = 1|y = 1) = \frac{\text{Count} (x_{i} = 1 \text{ and } y = 1)}{\text{Count }(y=1)}
    $$
    
    Similarly,
    
    $$
    P(x_{i} = 0|y = 1) = \frac{\text{Count} (x_{i} = 0 \text{ and } y = 1)}{\text{Count }(y=1)}
    $$
    
    
\end{frame}


\begin{frame}{Spam Mail classification}
    $$
        P(y = 1) = \frac{\text{Count }(y=1) }{\text{Count }(y=1) +\text{Count }(y=0) }
    $$
    
    Similarly,
    
    $$
        P(y = 0) = \frac{\text{Count }(y=0) }{\text{Count }(y=1) +\text{Count }(y=0) }
    $$
    
    
    
\end{frame}

\begin{frame}{Example}
    lets assume that dictionary is $[w_{1},w_{2},w_{3}]$
    
    
    \begin{tabular}{c|c|c|c|c}
    Index&$w_{1}$&$w_{2}$&$w_{3}$&y\\
    \hline
    \hline
         1 & 0 & 0 & 0 & 1  \\
         2 & 0 & 0 & 0 & 0  \\
         3 & 0 & 0 & 0 & 1  \\
         4 & 1 & 0 & 0 & 0  \\
         5 & 1 & 0 & 1 & 1  \\
         6 & 1 & 1 & 1 & 0  \\
         7 & 1 & 1 & 1 & 1  \\
         8 & 1 & 1 & 0 & 0  \\
         9 & 0 & 1 & 1 & 0  \\
         10 & 0 & 1 & 1 & 1  \\
    \end{tabular}
\end{frame}

\begin{frame}{Spam Classification}
    if y=0
    \begin{itemize}
        \item $P(w_{1}=0\vert y=0)$ = $\frac{3}{5}$ = 0.6 \\
        \item $P(w_{2}=0\vert y=0)$ = $\frac{2}{5}$ = 0.4 \\
        \item $P(w_{3}=0\vert y=0)$ = $\frac{3}{5}$ = 0.6 \\
    \end{itemize}
    P(y=0) = 0.5\\
    Similarly, if y=1
    \begin{itemize}
        \item $P(w_{1}=1\vert y=1)$ = $\frac{2}{5}$ = 0.4 \\
        \item $P(w_{2}=1\vert y=1)$ = $\frac{1}{5}$ = 0.2 \\
        \item $P(w_{3}=1\vert y=1)$ = $\frac{3}{5}$ = 0.6 \\
    \end{itemize}
    P(y=1) = 0.5
\end{frame}

\begin{frame}{Spam Classification}
    
    Given, test email {0,0,1}, classify it. Using naive bayes rule, we can do the following, 
    \begin{center}
$$
        P(y=1\vert w_{1}=0,w_{2} = 0,w_{3}=1) 
         \frac{P(w_{1}=0|y=1) P(w_{2}=0|y=1) P(w_{3}=1|y=1) P(y=1)}{P(w_{1}=0, w_{2}=0, w_{3}=1} 
    $$
    
    \end{center}
    
\end{frame}

%\begin{frame}{Spam Classification}
%    
%    Calculate, $P(y=1 \vert w_{1}=0, w_{2}=0, w_{3}=1)$ and $P(y=1 \vert w_{1}=0, w_{2}=0, w_{3}=1)$
%    
%    if $P(y=1 \vert w_{1}=0, w_{2}=0, w_{3}=1)$ $\gt$ $P(y=0 \vert w_{1}=0, w_{2}=0, w_{3}=1)$, then it is a spam mail
%\end{frame}

\begin{frame}{Gaussian naive Bayes}
    We have classes $C_{1}, C_{2}, C_{3},\dots, C_{k}$\\
    There is a continuous attribute x\\
    For Class k 
    \begin{itemize}
        \item $\mu_{k} = Mean(x \vert y(x) = C_{k})$
        \item $\sigma_{k}^{2} = Variance(x \vert y(x)=C_{k})$
    \end{itemize}
    
\end{frame}

\begin{frame}{Guassian Naive Bayes}
    Now for x = some observation 'v'\\
    \begin{equation*}
        P(x=v\vert C_{k}) = \frac{1}{\sqrt{2\pi \sigma_{k}^{2}}} \exp^{\frac{-(v-\mu_{k})^{2}}{2\sigma_{k}^{2}}}
    \end{equation*}
\end{frame}

\begin{frame}{Wikipedia Example}

    \begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Height&Weight&Footsize&Gender\\
    \hline
    \hline
         6 & 180& 12& M \\
         5.92 & 190& 11& M \\
         5.58 & 170& 12& M \\
         5.92 & 165& 10& M \\
         5 & 100& 6& F \\
         5.5 & 100& 6& F \\
         5.42 & 130& 7& F \\
         5.75 & 150& 7& F \\
         \hline
    \end{tabular}
    
    \end{center}
    
\end{frame}

\begin{frame}{Example}
    \begin{center}
        
    
    \begin{tabular}{|c|c|c|}
    \hline
     &Male&Female\\
     \hline
     \hline
     Mean (height) & 5.855 & 5.41  \\
     Variance (height) & 3.5 $\times$ $10^{-2}$ & 9.7 $\times$ $10^{-2}$  \\
     Mean (weight) & 176.25 & 132.5  \\
     Variance (weight) & 1.22 $\times$ $10^{2}$ & 5.5 $\times$ $10^{2}$   \\
     Mean (Foot) & 11.25 & 7.5  \\
     Variance (Foot) & 9.7 $\times$ $10^{-1}$ & 1.67  \\
    \hline
    \hline
    \end{tabular}
    \end{center}
\end{frame}


\begin{frame}{Classify the Person}

    Given height = 6ft, weight = 13.5 lbs, feet = 8 inches, classify if it's male or female.
    
\end{frame}

\begin{frame}{Classify the Person}
    Given height = 6ft, weight = 13.5 lbs, feet = 8 inches, classify if it's male or female.\\
    It is female!
\end{frame}
% \begin{frame}{Matrix representation of the expression}
% \begin{center}
% \begin{tcolorbox}
% $weight_{i}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{i}$
% \end{tcolorbox}
% \end{center}

% \begin{itemize}
% \item $weight_{1}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{1}$

% \item $weight_{2}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{2}$


% \item $weight_{N}$ $\approx$
% $\theta_{0}$+$\theta_{1}$*$height_{N}$

% \end{itemize}
% \end{frame}

% \begin{frame}{Matrix representation of the expression}



% \[\begin{bmatrix}
%     weight_{1}   \\
%     weight_{2}   \\
%     \dots \\
%     weight_{N}
% \end{bmatrix}
% = \begin{bmatrix}
%     1& height_{1}   \\
%     1& height_{2}   \\
%     \dots \\
%     1& height_{N}   \\
% \end{bmatrix}
% \begin{bmatrix}
%     \theta_{0} \\
%     \theta_{1}
% \end{bmatrix}\]


% \begin{itemize}
%     \item $\theta_{0}$ - Bias Term
%     \item $\theta_{1}$ - Intercept Term
% \end{itemize}
% \end{frame}


% \begin{frame}{Extension to multiple dimensions}

% In the previous example y = F(x), where x is one-dimensional.\\
% Examples in multiple dimensions.\\
% One example is to predict the water demand of the IITGN campus

% \small{
% \begin{center}
%     \begin{tcolorbox}
%         Demand = F(\# occupants, Temperature)
%     \end{tcolorbox}
% \end{center}

% \begin{center}
%     \begin{tcolorbox}
%         Demand = Base Demand + $K_{1}$ * \# occupants + $K_{2}$ * Temperature
%     \end{tcolorbox}
% \end{center}
% }

% \end{frame}

% \begin{frame}{We can expect the following}
%     \begin{itemize}
%         \item Demand increases, if \# occupants increases, then $K_{1}$ is likely to be positive
        
%         \item Demand increases, if temperature increases, then $K_{2}$ is likely to be positive
        
%         \item Base demand is independent of the temperature and the \# occupants.
        
%     \end{itemize}
% \end{frame}


% \begin{frame}{Generalized Linear Regression Format}


%     \[\begin{bmatrix}
%         y_{1}\\
%         y_{2} \\
%         \vdots \\
%         y_{N}
%     \end{bmatrix}
%     =     \begin{bmatrix}
%         1 & x_{11} & x_{12} & \dots & x_{1m}\\
%         1 & x_{21} & x_{22} & \dots & x_{2m}\\
%         \vdots & \vdots & \vdots & \dots & \vdots\\
%         1 & x_{N1} & x_{N2} & \dots & x_{Nm}\\
%         % y_{2} \\
%         % \vdots \\
%         % y_{N}
%     \end{bmatrix}
%     [\begin{bmatrix}
%         theta_{0}\\
%         theta_{1}\\
%         \vdots{5} \\
%         theta_{m}\\
%     \end{bmatrix}
%   \]
   
   
%   \begin{tcolorbox}
%   \begin{center}
       
   
       
   
%   $ Y \approx X\theta$
%   \end{center}
%   \end{tcolorbox}
   
%   \begin{itemize}
%       \item We have N knows, (or N equations)
%       \item M unknowns (or M parameters/ variables)
%   \end{itemize}
   
% \end{frame}


% \begin{frame}{Relationships between feature and target variables}
    
%     There are differenct $\theta_{0}$ $\&$ $\theta_{1}$. Each of them can represents a relationship.\\
%     \vspace{0.5em}
%     Given multiples values of $\theta_{0}$ $\&$ $\theta_{1}$ how to choose which is the  best?
    
% \end{frame}
% \begin{frame}{Error terms}
% \begin{itemize}
%     \item $\theta_{0}, \theta_{1}$: The parameters of the linear regressions
%     \item $\epsilon_{i}$: Variable residual per example 
% \end{itemize}


% \begin{equation*}
%     \epsilon_{i} = y_{i} - \hat{y}_{i}
% \end{equation*}
% \begin{equation*}
%     \epsilon_{i} = y_{i} - (\theta_{0} + x_{i}\times\theta_{1})
% \end{equation*}
% \end{frame}



% \begin{frame}{Good fit}

% \begin{itemize}
%     \item $|\epsilon_{1}|$, $|\epsilon_{2}|$, $|\epsilon_{3}|$, ... should be small.
%     \item 
% ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$ - $L_{2}$ Norm
%     \item 
% ${\text{minimize }} |\epsilon_{1}| + |\epsilon_{1}| + \dots + |\epsilon_{1}|$ - $L_{1}$ Norm
% \end{itemize}
% \end{frame}



% \begin{frame}{Normal Equation}
    
    
%     \begin{tcolorbox}
%       $ Y = X\theta + \epsilon$
%     \end{tcolorbox}
    
%     To Learn: $\theta$ \\
%     Objective: ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$  
% \end{frame}

% \begin{frame}{Normal Equation}
    
% \begin{equation*}
%  \epsilon = 
% \begin{bmatrix}
%     \epsilon_{1} \\
%     \epsilon_{1} \\
%     \vdots \\
%     \epsilon_{N} \\
% \end{bmatrix}
% \end{equation*}
% \\
% \begin{center}
%     Minimize $\epsilon^{T}\epsilon$    
% \end{center}
% \end{frame}

% \begin{frame}{Derivation of Normal Equation}
% \begin{align}
% \label{eqn*:eqlabel}
% \begin{split}
%   \epsilon &= y - X\theta \\
% \epsilon^{T} &= (y-X\theta)^{T} = y^{T} - \theta^{T}X^{T}\\
% \epsilon^{T}\epsilon &= (y^{T} - \theta^{T}X^{T})(y - X\theta)\\
% &=y^{T}y - \theta^{T}X^{T}y - y^{T}X\theta+\theta^{T}X^{T}X\theta\\
% &=y^{T}y - 2y^{T}X\theta+\theta^{T}X^{T}X\theta
% \end{split}
% \end{align}

% This is what we wish to minimize
% \end{frame}

% \begin{frame}{Minimizing the objective function}
    
    
%     \begin{equation}
%         \frac{\partial \epsilon^{T} \epsilon}{\partial \theta} = 0    
%     \end{equation}
    
    
    
%     \begin{itemize}
%         \item $
%     \frac{\partial }{\partial \theta} y^{T}y= 0
%     $
%     \item $
%     \frac{\partial }{\partial \theta}(-2y^{T}X\theta ) = (-2y^{T}X)^{T} = -2X^{T}y
%     $
%     \item $ \frac{\partial}{\partial\theta} (\theta^{T}X^{T}X\theta) = 2X^{T}X\theta$
%     \end{itemize}
    
%     Substitute the values in the top equation
    
% \end{frame}

% \begin{frame}{Normal Equation derivation}
% $$
%     0 = -2X^{T}y + 2X^{T}X\theta
% $$

% $$
%     X^{T}y  = X^{T}X\theta
% $$

% \begin{tcolorbox}
% \begin{center}
    

%         $\theta = (X^{T}X)^{-1}X^{T}$
% \end{center}
% \end{tcolorbox}

% \end{frame}

% \begin{frame}{Worked out example}
%     \begin{center}
%  \begin{tabular}{||c c||} 
%  \hline
%  x  & y \\ [0.5ex] 
%  \hline\hline
%  0 & 0 \\
%  1 & 1 \\
%  2 & 2 \\
%  3 & 3 \\
%  \hline
% \end{tabular}
% \end{center}

% Given the data above, find $\theta_{0}$ and $\theta_{1}$.

% \end{frame}



% \begin{frame}{Worked out example}
% \begin{align}
%     \begin{split}
%         X &= \begin{bmatrix}
%             1 & 0\\
%             1 & 1\\
%             1 & 2\\
%             1 & 3
%         \end{bmatrix}\\
%         X^{T} &= \begin{bmatrix}
%             1&1&1&1\\
%             1&2&3&4
%         \end{bmatrix}\\
%         X^{T}X &= \begin{bmatrix}
%             4 &6\\6&14
%         \end{bmatrix}
%     \end{split}
% \end{align}
% Given the data above, find $\theta_{0}$ and $\theta_{1}$.
% \end{frame}


% \begin{frame}{Worked out example}
%     \begin{align}
%         \begin{split}
%             (X^{T}X)^{-1} &= \frac{1}{20} \begin{bmatrix}
%                 14 & -6\\
%                 -6& 4
%             \end{bmatrix}\\
%             X^{T}y &= \begin{bmatrix}
%                 6\\
%                 14
%             \end{bmatrix}\\
%             \theta &= (X^{T}X)^{-1}(X^{T}y)
%         \end{split}
%     \end{align}
% \end{frame}


% % \begin{frame}{Frame Title}
    
% %     \begin{align}
        
% %         \begin{split}
% %             \theta &= \frac{1}{20}\begin{bmatrix}
% %                 14 & -6\\
% %                 -6 & 4
% %             \end{bmatrix}
% %             \begin{bmatrix}
% %                 6\\
% %                 14
% %             \end{bmatrix}\\
% %             &= \frac{1}{20} \begin{bmatrix}
% %                 0 \\
% %                 20
% %             \end{bmatrix}\\
            
% %             % \begin{bmatrix}
% %             %     \theta_{0}\\
% %             %     \theta_{1}
% %             % \end{bmatrix}
% %             %  = 
% %             %  \begin{bmatrix}
% %             %      0 \\
% %             %      1
% %             %  \end{bmatrix}
% %         \end{split}
% %     \end{align}
% %     $$
% % \end{frame}

% \begin{frame}{Quick Question}
    
%     $$
%     y = X\theta;
%     $$
    
%     Can we do
%     $$theta = X^{-1}y$$
    
%     Why? or why not?
    
% \end{frame}


% \begin{frame}{Quick Question}

%     X may not be a square matrix. So, $X^{-1}$ may not exist.
    
%     Rectangular matrices have a left inverse and a right inverse.
% \end{frame}



% \begin{frame}{Relation between \#instances and \# Variables}
    
%     If N$<$ M, then it is an under-determined system\\
%     Example: N=2; M=3\\
 
%   $$ \begin{bmatrix}
%         30 \\
%         40 
%     \end{bmatrix}
%      = \begin{bmatrix}
%          1 & 6& 30\\
%          1 & 5& 20
%      \end{bmatrix}    \begin{bmatrix}
%         \theta_{0}\\
%         \theta_{1}\\
%         \theta_{2}\\
%     \end{bmatrix} $$
 
    
    
%     \begin{align}
% \label{eqn*:eqlabel}
% \begin{split}
%   30 &= \theta_{0} + 6\theta_{1} + 30\theta_{2} \\
% 40 &= \theta_{0} + 5\theta_{1} + 20\theta_{2}\\
% \hline
% -10 &=  -1 \theta_{1} -10\theta_{2}
% \end{split}
% \end{align}

    
        
%     %     \begin{split}
%     %     %     30 &= \theta_{0} + 6\theta_{1} + 30\theta_{2} \\
%     %     %     40 &= \theta_{0} + 5\theta_{1} + 20\theta_{2} \\
        
%     %     % -10 &=  -1 \theta_{1} -10\theta_{2} \\
            
%     %     \end{split}
    
    
%     % \end{align}
%     % \\
    
%     The above equation can have infinitely many solutions. \\
%     Under-determined system: $\epsilon_{i} = 0$ for all $i$

% \end{frame}

% \begin{frame}{Relation between \#instances and \# Variables}
%     What if N $>$ M\\
%     Then it is an over determined system. So, the sum of squared residuals > 0.
% \end{frame}
% \begin{frame}{Special Cases}
% There can be situations where $X^{T}X$ is not computable. This condition arises when the $|X^{T}X|$ = 0.

% \begin{equation}
%     X = \begin{bmatrix}
%         1 & 1& 2\\
%         1 & 2& 4\\
%         1 & 3& 6\\
%     \end{bmatrix}
% \end{equation}

% The matrix X is not full rank. 
% \end{frame}


% \begin{frame}{Things to know about Linear Regression}
%     What to do When relationship is non-linear?
% \end{frame}


% \begin{frame}{Things to know about Linear Regression}
%     Transform the data, by including the higher power terms in the feature space. 
    
       
%     \begin{center}
%  \begin{tabular}{||c c||} 
%  \hline
%  t  & s \\ [0.5ex] 
%  \hline\hline
%  0 & 0 \\
%  1 & 6 \\
%  3 & 24 \\
%  4 & 36 \\
%  \hline
% \end{tabular}
% \end{center}

% The above table represents the data before transformation
% \end{frame}


% \begin{frame}{Things to know about Linear Regression}
% Add the higher degree features to the previous table
    
       
%     \begin{center}
%  \begin{tabular}{||c c c||} 
%  \hline
%  t  & $t^{2}$ & s \\ [0.5ex] 
%  \hline\hline
%  0 & 0&0 \\
%  1 & 1&6 \\
%  3 & 9&24 \\
%  4 & 16&36 \\
%  \hline
% \end{tabular}
% \end{center}

% The above table represents the data after transformation
% \end{frame}






% \begin{frame}{Multi - Collinearity}

% It arises when one or more predictor varibale/feature in X can be expressed as a linear combinations of others\\
% \vspace{5mm}



% How to tackle it?
% \begin{itemize}
%     \item Regularize
%     \item Drop variables
%     \item use different subsets of data
%     \item Avoid dummy variable trap
% \end{itemize}
% \end{frame}

% \begin{frame}{Modelling Interaction}
    
%     $$
%     y = \theta_{0} + x_{1}\theta_{1} +
%     x_{1}\theta_{1} +
%     \dots + x_{m}\theta_{m} 
%     $$
    
%     If x_{1} increases by one unit, then y increases by \theta_{1} units, irrespective of the interactive with x_{2},x_{3},\dots,d_{m}.
    
%     $$
%     y = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + \theta_{3}x_{1}x_{2} + \dots
%     $$

%     This way we can model the interactions. 
% \end{frame}
% \begin{frame}{Alternative parameter estimation}
%     $$
%     y_{i} = \theta_{0} + \theta_{1}x_{i}
%     $$
    
%     $$
%     \epsilon_{i} = y_{i} - \hat{y}_{i}
%     $$
    
%     $$
%     \sum \epsilon_{i}^{2} = \sum (y_{i} - \theta_{0} - \theta_{1}x_{i})^{2}
%     $$
    
%     Now, we compute the derivative of it with all the  $\theta_{j}$
    
    
% \end{frame}

% \begin{frame}{Alternative approach}

% \begin{align}
%     \begin{split}
%         \frac{\partial}{\partial \theta_{0}}\sum \epsilon_{i}^{2} &= 2\sum(y_{i} -  \theta_{0} - \theta_{1}x_{i})(-1) = 0 \\
%         0 &= \sum y_{i} -  N\theta_{0} - \sum \theta_{1}x_{i}\\
%         \theta_{0} &= \frac{\sum y_{i} - \theta_{1}\sum x_{i}}{N}
%     \end{split}
% \end{align}


% \begin{tcolorbox}
% \begin{center}
%     $ \theta_{0} = \bar{y} - \theta_{1} \bar{x}$
% \end{center}
% \end{tcolorbox}
        


% \end{frame}
% \begin{frame}{Alternative approach}

% $$
% \frac{\partial}{\partial \theta_{1}}\sum \epsilon_{i}^{2} = 0
% $$


% $$
% \implies 2 \sum_{i=1}^{N} (y_{i} - \theta_{0} \ \theta_{1}x_{i})(-x_{i}) = 0
% $$

% $$
% \implies \sum_{i=1}^{N} (x_{i}y_{i} - \theta_{0}x_{i} \ \theta_{1}x_{i}^{2}) = 0
% $$

% $$
% \implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \sum \theta_{0}x_{i}
% $$

% $$
% \implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \sum (\bar{y} - \theta_{1}\bar{x})
% $$


% \end{frame}

% \begin{frame}{alternative approach}


% $$
% \implies \sum  \theta_{1}x_{i}^{2} = \sum x_{i}y_{i} - \bar{y}\sum x_{i} + \theta_{1}\bar{x}\sum x_{i} 
% $$

% $$
% \implies \sum  
% x_{i}y_{i} - \sum x_{i}y = \theta_{1} (-\bar{x}\sum x_{i} + \sum x_{i}^{2})
% $$

% $$
% \theta_{1} = \frac{x_{i}y_{i} - \sum x_{i}y}{\sum x_{i}^{2} -\bar{x}\sum x_{i}}
% $$

    
% \end{frame}

% \begin{frame}{Alternative approach}
    
%     $$
%     \theta_{i} = \frac{ \frac{1}{N} \sum_{i=1}^{N}(X_{i} - \bar{X})(Y_{i} - \bar{y})}{\frac{1}{N}(X_{i} - \bar{x})^{2}}
%     $$
    
%     $$
%     \theta_{1} = \frac{Cov(x,y)}{variance(x)}
%     $$
    
% \end{frame}
% % \begin{frame}{Derivation of Normal Rule}
% %     \begin{equation*}
% %          = \\
        
% %     \end{equation*}
% % \end{frame}

% % ${\text{minimize }} \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2$


% % \begin{equation*}

% %     \displaystyle{\minimize \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2 }
% % \end{equation*}
    
% % $\displaystyle{\minimize \epsilon_{1}^2 + \epsilon_{2}^2 + \dots + \epsilon_{N}^2 }$

    




% % \begin{frame}{There can be multiple equa}
    
% % \end{frame}

% % =
% % \begin{bmatrix}
% %     x_{11} & x_{12} & x_{13} & \dots  & x_{1n} \\
% %     x_{21} & x_{22} & x_{23} & \dots  & x_{2n} \\
% %     \vdots & \vdots & \vdots & \ddots & \vdots \\
% %     x_{d1} & x_{d2} & x_{d3} & \dots  & x_{dn}
% % \end{bmatrix}





% % \begin{frame}{Another example on Bayes rule}
% % \end{frame}


% % \begin{frame}{Bayes Rule for Machine Learning}
% % \begin{itemize}


% %     \item $P(A|B)P(B) = P(B|A)P(A)$
% %     \item Let us consider for a machine learning problem:
% %     \begin{itemize}
% %     	\item A = Parameters ($\theta$)
% %     	\item B = Data ($\mathcal{D}$)
% %     \end{itemize}
% % \item We can rewrite the Bayes rule as:
% % \begin{itemize}
% % 	\item $P(\theta|\mathcal{D}) = \frac{P(\mathcal{D}|\theta)P(\theta)}{P(\mathcal{D})}$
% % 	\item Posterior: 
% % 	\item Prior:
% % 	\item Likelihood
% % 	\item 
% % \end{itemize}
% % \end{itemize}
% % \end{frame}

% % \begin{frame}{Likelihood}
% % \begin{itemize}
% % 	\item Likelihood is a function of $\theta$
% % 	\item Given a coin flip and 5 H and 1 T, what is more likely: P(H) = 0.5 or P(H) = 1
% % \end{itemize}
% % \end{frame}

% % \begin{frame}{Bayesian Learning is well suited for online settings}
% % content...
% % \end{frame}

% % \begin{frame}{Coin flipping}
% % \begin{itemize}
% % 	\item Assume we do a coin flip multiple times and we get the following observation: \{H, H, H, H, H, H, T, T, T, T\}: 6 Heads and 4 Tails
% % 	\item  What is $P(Head)$?
% % 	\item Is your answer: 6/10. Why?
% % \end{itemize}

% % \end{frame}

% % \begin{frame}{Coin flipping: Maximum Likelihood Estimate (MLE)}
% % \begin{itemize}
% % 	\item We have $\mathcal{D} = \{\data_1, \data_2, ...\data_{N}\}$ for $N$ observations where each $\mathcal{D}_i \in \{H, T\}$
% % 	\item Assume we have $n_H$ heads and $n_T$ tails, $n_H + n_T = N$
% % 	\item Let us have $P(H) = \theta, P(T) = 1-\theta$
% % 	\item We have Likelihood, $L(\theta) = P(\mathcal{D}|\theta) = P(\data_1, \data_2, ..., \data_N|\theta)$
% % 	\item Since observations are i.i.d., $L(\theta) = P(\data_1|\theta).P(\data_2|\theta) ... P(\data_N|\theta)$
% % \end{itemize}

% % \end{frame}


% % \begin{frame}{Coin flipping: Maximum Likelihood Estimate (MLE)}
% % \begin{itemize}
% % 	\item  
% % \begin{align*}  
% % P(\data_i|\theta) =  \left
% % \{\begin{array}{lr} \theta, & \text{for~} \data_i =H \\
% % 1-\theta, & \text{for~} \data_i = T
% % \end{array}\right.\
% % \end{align*}  
% % \item Thus, $L(\theta) = \theta^{n_H}\times (1-\theta)^{n_T}$
% % \item Log-Likelihood, $LL(\theta) = n_Hlog\theta + (n_T)(log(1-\theta))$
% % \item $\frac{\partial LL(\theta)}{\partial \theta} = \frac{n_H}{\theta} - \frac{n_T}{1-\theta}$
% % \item  For maxima, set derivative of LL to zero

% % \item 	$\frac{n_H}{\theta} - \frac{n_T}{1-\theta} = 0 $
% % \end{itemize}
% % \begin{tcolorbox}
% % 	\begin{align*}
% % 	 \theta = \frac{n_H}{n_H + n_T}
% % 	\end{align*}

% % \end{tcolorbox}
% % Question: Is this maxima or minima?

% % \end{frame}

% % \begin{frame}{}
% % \begin{align*}
% % \frac{\partial^2 LL(\theta)}{\partial \theta^2} = \frac{-n_H}{\theta^2} + \frac{-n_T}{(1-\theta)^2} \in \mathbb R_-
% % \end{align*}
% % Thus, the solution is a maxima
% % \end{frame}

% % \begin{frame}{Maximum A Posteriori estimate (MAP)}
% % \begin{itemize}


% % \item \textbf{MLE does not handle prior knowledge}: What if we know that our coin is biased towards head?
% % \item \textbf{MLE can overfit}: What is the probability of heads when we have observed 6 heads and 0 tails?
% % \end{itemize}

% % \end{frame}


% % \begin{frame}{Maximum A Posteriori estimate (MAP)}
% % Goal: Maximize the Posterior
% % \begin{tcolorbox}
% % 	\begin{align}
% % \hat{\theta}_{MAP} = \argmin_\theta P(\theta|\data)\\
% % \hat{\theta}_{MAP}= \argmin_\theta P(\data|\theta)P(\theta)
% % \end{align}
% % \end{tcolorbox}

% % \end{frame}

% % \begin{frame}{Prior distributions}
% % \end{frame}

% % \begin{frame}{Beta Distribution}
% % \end{frame}

% % \begin{frame}{Beta Distribution}
% % \end{frame}

% % \begin{frame}{Coin toss: MAP estimate}
% % \end{frame}

% % \begin{frame}{Linear Regression: MLE}
% % We previously saw 
% % \begin{tcolorbox}
% % \begin{align*}
% % \hat{\theta}_{least-square} = \argmin_\theta \epsilon^T\epsilon = \argmin_\theta (y-X\theta)^T(y-X\theta)
% % \end{align*}
% % \end{tcolorbox}


% % \end{frame}

\end{document}
